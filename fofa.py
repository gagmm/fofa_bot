import os
import sys
import json
import logging
import base64
import time
import re
import requests
import signal
import socket
import hashlib
import shutil
import random
import csv
import asyncio
import pandas as pd
import threading
from functools import wraps
from datetime import datetime, timedelta
from dateutil import tz
from urllib.parse import urlparse
import uuid # ç¡®ä¿æ–‡ä»¶é¡¶éƒ¨æœ‰è¿™è¡Œ
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, BotCommand, ParseMode, ReplyKeyboardMarkup, KeyboardButton, InlineQueryResultArticle, InputTextMessageContent
from telegram.ext import (
    Updater,
    CommandHandler,
    CallbackContext,
    ConversationHandler,
    MessageHandler,
    CallbackQueryHandler,
    InlineQueryHandler, # <--- ç¡®ä¿æœ‰è¿™ä¸ª
    Filters,
)

from telegram.error import BadRequest, RetryAfter, TimedOut, NetworkError, InvalidToken

# --- å…¨å±€å˜é‡å’Œå¸¸é‡ ---
CONFIG_FILE = 'config.json'
HISTORY_FILE = 'history.json'
LOG_FILE = 'fofa_bot.log'
FOFA_CACHE_DIR = 'fofa_file'
ANONYMOUS_KEYS_FILE = 'fofa_anonymous.json'
SCAN_TASKS_FILE = 'scan_tasks.json'
MONITOR_TASKS_FILE = 'monitor_tasks.json' # æ–°å¢ç›‘æ§é…ç½®
MONITOR_DATA_DIR = 'monitor_data' # æ–°å¢ç›‘æ§æ•°æ®ç›®å½•
MAX_HISTORY_SIZE = 50
MAX_SCAN_TASKS = 50
CACHE_EXPIRATION_SECONDS = 24 * 60 * 60
MAX_BATCH_TARGETS = 10000
FOFA_SEARCH_URL = "https://fofa.info/api/v1/search/all"
FOFA_NEXT_URL = "https://fofa.info/api/v1/search/next"
FOFA_INFO_URL = "https://fofa.info/api/v1/info/my"
FOFA_STATS_URL = "https://fofa.info/api/v1/search/stats"
FOFA_HOST_BASE_URL = "https://fofa.info/api/v1/host/"

# --- å¤§æ´²å›½å®¶ä»£ç  ---
CONTINENT_COUNTRIES = {
    'Asia': ['AF', 'AM', 'AZ', 'BH', 'BD', 'BT', 'BN', 'KH', 'CN', 'CY', 'GE', 'IN', 'ID', 'IR', 'IQ', 'IL', 'JP', 'JO', 'KZ', 'KW', 'KG', 'LA', 'LB', 'MY', 'MV', 'MN', 'MM', 'NP', 'KP', 'OM', 'PK', 'PS', 'PH', 'QA', 'SA', 'SG', 'KR', 'LK', 'SY', 'TW', 'TJ', 'TH', 'TL', 'TR', 'TM', 'AE', 'UZ', 'VN', 'YE'],
    'Europe': ['AL', 'AD', 'AM', 'AT', 'BY', 'BE', 'BA', 'BG', 'HR', 'CY', 'CZ', 'DK', 'EE', 'FO', 'FI', 'FR', 'GE', 'DE', 'GI', 'GR', 'HU', 'IS', 'IE', 'IT', 'KZ', 'LV', 'LI', 'LT', 'LU', 'MK', 'MT', 'MD', 'MC', 'ME', 'NL', 'NO', 'PL', 'PT', 'RO', 'RU', 'SM', 'RS', 'SK', 'SI', 'ES', 'SE', 'CH', 'TR', 'UA', 'GB', 'VA'],
    'NorthAmerica': ['AG', 'BS', 'BB', 'BZ', 'CA', 'CR', 'CU', 'DM', 'DO', 'SV', 'GD', 'GT', 'HT', 'HN', 'JM', 'MX', 'NI', 'PA', 'KN', 'LC', 'VC', 'TT', 'US'],
    'SouthAmerica': ['AR', 'BO', 'BR', 'CL', 'CO', 'EC', 'GY', 'PY', 'PE', 'SR', 'UY', 'VE'],
    'Africa': ['DZ', 'AO', 'BJ', 'BW', 'BF', 'BI', 'CV', 'CM', 'CF', 'TD', 'KM', 'CD', 'CG', 'CI', 'DJ', 'EG', 'GQ', 'ER', 'SZ', 'ET', 'GA', 'GM', 'GH', 'GN', 'GW', 'KE', 'LS', 'LR', 'LY', 'MG', 'MW', 'ML', 'MR', 'MU', 'YT', 'MA', 'MZ', 'NA', 'NE', 'NG', 'RW', 'ST', 'SN', 'SC', 'SL', 'SO', 'ZA', 'SS', 'SD', 'TZ', 'TG', 'TN', 'UG', 'EH', 'ZM', 'ZW'],
    'Oceania': ['AS', 'AU', 'CK', 'FJ', 'PF', 'GU', 'KI', 'MH', 'FM', 'NR', 'NC', 'NZ', 'NU', 'NF', 'MP', 'PW', 'PG', 'PN', 'WS', 'SB', 'TK', 'TO', 'TV', 'VU', 'WF']
}
ALL_COUNTRY_CODES = sorted(list(set(code for countries in CONTINENT_COUNTRIES.values() for code in countries)))

# --- FOFA å­—æ®µå®šä¹‰ ---
FOFA_STATS_FIELDS = "protocol,domain,port,title,os,server,country,asn,org,asset_type,fid,icp"
FREE_FIELDS = ["ip", "port", "protocol", "country", "country_name", "region", "city", "longitude", "latitude", "asn", "org", "host", "domain", "os", "server", "icp", "title", "jarm", "header", "banner", "cert", "base_protocol", "link", "cert.issuer.org", "cert.issuer.cn", "cert.subject.org", "cert.subject.cn", "tls.ja3s", "tls.version", "cert.sn", "cert.not_before", "cert.not_after", "cert.domain"]
PERSONAL_FIELDS = FREE_FIELDS + ["header_hash", "banner_hash", "banner_fid"]
BUSINESS_FIELDS = PERSONAL_FIELDS + ["cname", "lastupdatetime", "product", "product_category", "version", "icon_hash", "cert.is_valid", "cname_domain", "body", "cert.is_match", "cert.is_equal"]
ENTERPRISE_FIELDS = BUSINESS_FIELDS + ["icon", "fid", "structinfo"]
FIELD_CATEGORIES = {
    "å…è´¹å­—æ®µ": FREE_FIELDS,
    "ä¸ªäººä¼šå‘˜å­—æ®µ": list(set(PERSONAL_FIELDS) - set(FREE_FIELDS)),
    "å•†ä¸šç‰ˆæœ¬å­—æ®µ": list(set(BUSINESS_FIELDS) - set(PERSONAL_FIELDS)),
    "ä¼ä¸šç‰ˆæœ¬å­—æ®µ": list(set(ENTERPRISE_FIELDS) - set(BUSINESS_FIELDS)),
}
KEY_LEVELS = {}

# --- æ—¥å¿—é…ç½® ---
if os.path.exists(LOG_FILE) and os.path.getsize(LOG_FILE) > (5 * 1024 * 1024):
    try: os.rename(LOG_FILE, LOG_FILE + '.old')
    except OSError as e: print(f"æ— æ³•è½®æ¢æ—¥å¿—æ–‡ä»¶: {e}")
logging.basicConfig(
    level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler(LOG_FILE, encoding='utf-8'), logging.StreamHandler()]
)
logging.getLogger("requests").setLevel(logging.WARNING); logging.getLogger("urllib3").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# --- ä¼šè¯çŠ¶æ€å®šä¹‰ (v10.9.6 é‡æ„) ---
# ä¸ºæ¯ä¸ªç‹¬ç«‹çš„ ConversationHandler åˆ†é…å”¯ä¸€çš„çŠ¶æ€èŒƒå›´ï¼Œé˜²æ­¢å†²çª

# ä¸»èœå•äº¤äº’ (ReplyKeyboardMarkup)
STATE_AWAITING_QUERY, STATE_AWAITING_HOST = range(1, 3)

# /kkfofa å’Œ /allfofa æŸ¥è¯¢æµç¨‹
(
    QUERY_STATE_GET_GUEST_KEY,
    QUERY_STATE_ASK_CONTINENT,
    QUERY_STATE_CONTINENT_CHOICE,
    QUERY_STATE_CACHE_CHOICE,
    QUERY_STATE_KKFOFA_MODE,
    QUERY_STATE_GET_TRACEBACK_LIMIT,
    QUERY_STATE_ALLFOFA_GET_LIMIT,
) = range(10, 17)

# /settings è®¾ç½®æµç¨‹
(
    SETTINGS_STATE_MAIN, SETTINGS_STATE_ACTION,
    SETTINGS_STATE_GET_KEY, SETTINGS_STATE_REMOVE_API,
    SETTINGS_STATE_PRESET_MENU, SETTINGS_STATE_GET_PRESET_NAME,
    SETTINGS_STATE_GET_PRESET_QUERY, SETTINGS_STATE_REMOVE_PRESET,
    SETTINGS_STATE_GET_UPDATE_URL, SETTINGS_STATE_PROXYPOOL_MENU,
    SETTINGS_STATE_GET_PROXY_ADD, SETTINGS_STATE_GET_PROXY_REMOVE,
    SETTINGS_STATE_UPLOAD_API_MENU, SETTINGS_STATE_GET_UPLOAD_URL,
    SETTINGS_STATE_GET_UPLOAD_TOKEN, SETTINGS_STATE_ADMIN_MENU,
    SETTINGS_STATE_GET_ADMIN_ID_TO_ADD, SETTINGS_STATE_GET_ADMIN_ID_TO_REMOVE,
) = range(20, 38)

# /batch æ‰¹é‡å¯¼å‡ºæµç¨‹
(
    BATCH_STATE_SELECT_FIELDS,
    BATCH_STATE_MODE_CHOICE,
    BATCH_STATE_GET_LIMIT,
) = range(50, 53)

# /stats, /import, /batchfind, /restore, /batchcheckapi ç­‰ç‹¬ç«‹æµç¨‹
(
    STATS_STATE_GET_QUERY,
    IMPORT_STATE_GET_FILE,
    BATCHFIND_STATE_GET_FILE, BATCHFIND_STATE_SELECT_FEATURES,
    RESTORE_STATE_GET_FILE,
    BATCHCHECKAPI_STATE_GET_FILE,
) = range(80, 86)

# /scan æ‰«ææµç¨‹ (CallbackQueryHandler)
(
    SCAN_STATE_GET_CONCURRENCY,
    SCAN_STATE_GET_TIMEOUT,
) = range(100, 102)

# --- é…ç½®ç®¡ç† & ç¼“å­˜ ---
def load_json_file(filename, default_content):
    if not os.path.exists(filename):
        with open(filename, 'w', encoding='utf-8') as f: json.dump(default_content, f, indent=4); return default_content
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            config = json.load(f)
            if isinstance(default_content, dict):
                for key, value in default_content.items(): config.setdefault(key, value)
            return config
    except (json.JSONDecodeError, IOError):
        logger.error(f"{filename} æŸåï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®é‡å»ºã€‚");
        with open(filename, 'w', encoding='utf-8') as f: json.dump(default_content, f, indent=4); return default_content
def save_json_file(filename, data):
    with open(filename, 'w', encoding='utf-8') as f: json.dump(data, f, indent=4, ensure_ascii=False)
DEFAULT_CONFIG = { "bot_token": "YOUR_BOT_TOKEN_HERE", "apis": [], "admins": [], "proxy": "", "proxies": [], "full_mode": False, "public_mode": False, "presets": [], "update_url": "", "upload_api_url": "", "upload_api_token": "" }
CONFIG = load_json_file(CONFIG_FILE, DEFAULT_CONFIG)
HISTORY = load_json_file(HISTORY_FILE, {"queries": []})
ANONYMOUS_KEYS = load_json_file(ANONYMOUS_KEYS_FILE, {})
SCAN_TASKS = load_json_file(SCAN_TASKS_FILE, {})
MONITOR_TASKS = load_json_file(MONITOR_TASKS_FILE, {}) # åŠ è½½ç›‘æ§ä»»åŠ¡
def save_config(): save_json_file(CONFIG_FILE, CONFIG)
def save_anonymous_keys(): save_json_file(ANONYMOUS_KEYS_FILE, ANONYMOUS_KEYS)
def save_scan_tasks():
    logger.info(f"Saving {len(SCAN_TASKS)} scan tasks to {SCAN_TASKS_FILE}")
    save_json_file(SCAN_TASKS_FILE, SCAN_TASKS)
def save_monitor_tasks():
    save_json_file(MONITOR_TASKS_FILE, MONITOR_TASKS)
def add_or_update_query(query_text, cache_data=None):
    existing_query = next((q for q in HISTORY['queries'] if q['query_text'] == query_text), None)
    if existing_query:
        HISTORY['queries'].remove(existing_query); existing_query['timestamp'] = datetime.now(tz.tzutc()).isoformat()
        if cache_data: existing_query['cache'] = cache_data
        HISTORY['queries'].insert(0, existing_query)
    else:
        new_query = {"query_text": query_text, "timestamp": datetime.now(tz.tzutc()).isoformat(), "cache": cache_data}
        HISTORY['queries'].insert(0, new_query)
    while len(HISTORY['queries']) > MAX_HISTORY_SIZE: HISTORY['queries'].pop()
    save_json_file(HISTORY_FILE, HISTORY)
def find_cached_query(query_text):
    query = next((q for q in HISTORY['queries'] if q['query_text'] == query_text), None)
    if query and query.get('cache'):
        if 'file_path' in query['cache'] and os.path.exists(query['cache']['file_path']):
            return query
    return None

# --- è¾…åŠ©å‡½æ•°ä¸è£…é¥°å™¨ ---
def generate_filename_from_query(query_text: str, prefix: str = "fofa", ext: str = ".txt") -> str:
    sanitized_query = re.sub(r'[^a-z0-9\-_]+', '_', query_text.lower()).strip('_')
    max_len = 100
    if len(sanitized_query) > max_len: sanitized_query = sanitized_query[:max_len].rsplit('_', 1)[0]
    timestamp = int(time.time()); return f"{prefix}_{sanitized_query}_{timestamp}{ext}"
def get_proxies(proxy_to_use=None):
    """
    è¿”å›ä¸€ä¸ªä»£ç†é…ç½®å­—å…¸ã€‚
    å¦‚æœæä¾›äº† proxy_to_useï¼Œåˆ™ä¸“é—¨ä½¿ç”¨å®ƒã€‚
    å¦åˆ™ï¼Œä»ä»£ç†æ± ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªã€‚
    """
    proxy_str = proxy_to_use
    if proxy_str is None:
        proxies_list = CONFIG.get("proxies", [])
        if proxies_list:
            proxy_str = random.choice(proxies_list)
        else:
            proxy_str = CONFIG.get("proxy")
    
    if proxy_str:
        return {"http": proxy_str, "https": proxy_str}
    return None
def is_admin(user_id: int) -> bool: return user_id in CONFIG.get('admins', [])
def is_super_admin(user_id: int) -> bool:
    admins = CONFIG.get('admins', [])
    return admins and user_id == admins[0]

def admin_only(func):
    @wraps(func)
    def wrapped(update: Update, context: CallbackContext, *args, **kwargs):
        if not is_admin(update.effective_user.id):
            message_text = "â›”ï¸ æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™æ‰§è¡Œæ­¤ç®¡ç†æ“ä½œã€‚"
            if update.callback_query: update.callback_query.answer(message_text, show_alert=True)
            elif update.message: update.message.reply_text(message_text)
            return None
        return func(update, context, *args, **kwargs)
    return wrapped
def escape_markdown_v2(text: str) -> str:
    if not isinstance(text, str): text = str(text)
    escape_chars = r'_*[]()~`>#+-=|{}.!'
    return re.sub(f'([{re.escape(escape_chars)}])', r'\\\1', text)
def create_progress_bar(percentage: float, length: int = 10) -> str:
    if percentage < 0: percentage = 0
    if percentage > 100: percentage = 100
    filled_length = int(length * percentage // 100)
    bar = 'â–ˆ' * filled_length + 'â–‘' * (length - filled_length)
    return f"[{bar}] {percentage:.1f}%"

# --- æ–‡ä»¶ä¸Šä¼ è¾…åŠ©å‡½æ•° ---
def send_file_safely(context: CallbackContext, chat_id: int, file_path: str, caption: str = "", parse_mode: str = None, filename: str = None):
    """å®‰å…¨åœ°å‘é€æ–‡ä»¶ï¼Œå¤„ç†Telegram APIçš„å¤§å°é™åˆ¶ã€‚"""
    try:
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        TELEGRAM_MAX_FILE_SIZE_MB = 48

        if file_size_mb < TELEGRAM_MAX_FILE_SIZE_MB:
            with open(file_path, 'rb') as doc:
                context.bot.send_document(
                    chat_id, 
                    document=doc, 
                    filename=filename or os.path.basename(file_path), 
                    caption=caption, 
                    parse_mode=parse_mode, 
                    timeout=120 
                )
        else:
            message = (
                f"âš ï¸ *æ–‡ä»¶è¿‡å¤§*\n\n"
                f"æ–‡ä»¶ `{escape_markdown_v2(filename or os.path.basename(file_path))}` \\({file_size_mb:.2f} MB\\) "
                f"è¶…è¿‡äº†Telegramçš„å‘é€é™åˆ¶ \\({TELEGRAM_MAX_FILE_SIZE_MB} MB\\)\\."
            )
            context.bot.send_message(chat_id, message, parse_mode=ParseMode.MARKDOWN_V2)
    except FileNotFoundError:
        logger.error(f"å°è¯•å‘é€æ–‡ä»¶å¤±è´¥: æ–‡ä»¶æœªæ‰¾åˆ° at path {file_path}")
        context.bot.send_message(chat_id, f"âŒ å†…éƒ¨é”™è¯¯: å°è¯•å‘é€ç»“æœæ–‡ä»¶æ—¶æ‰¾ä¸åˆ°å®ƒã€‚")
    except (TimedOut, NetworkError) as e:
        logger.error(f"å‘é€æ–‡ä»¶ '{file_path}' æ—¶å‡ºç°ç½‘ç»œé”™è¯¯æˆ–è¶…æ—¶: {e}")
        context.bot.send_message(chat_id, f"âš ï¸ å‘é€æ–‡ä»¶æ—¶ç½‘ç»œè¶…æ—¶æˆ–å‡ºé”™ã€‚å¦‚æœé…ç½®äº†å¤–éƒ¨ä¸Šä¼ ï¼Œè¯·æ£€æŸ¥é‚£é‡Œçš„é“¾æ¥ã€‚")
    except Exception as e:
        logger.error(f"å‘é€æ–‡ä»¶ '{file_path}' æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
        context.bot.send_message(chat_id, f"âš ï¸ å‘é€æ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: `{escape_markdown_v2(str(e))}`", parse_mode=ParseMode.MARKDOWN_V2)

def upload_and_send_links(context: CallbackContext, chat_id: int, file_path: str):
    api_url = CONFIG.get("upload_api_url")
    api_token = CONFIG.get("upload_api_token")
    if not api_url or not api_token:
        logger.info("æœªé…ç½®ä¸Šä¼ APIçš„URLæˆ–Tokenï¼Œè·³è¿‡æ–‡ä»¶ä¸Šä¼ ã€‚")
        return
    try:
        with open(file_path, 'rb') as f:
            files = {'file': (os.path.basename(file_path), f)}
            headers = {'Authorization': api_token}
            response = requests.post(api_url, headers=headers, files=files, timeout=60, proxies=get_proxies())
            response.raise_for_status()
            result = response.json()
        if result and isinstance(result, list) and 'src' in result[0]:
            file_url_path = result[0]['src']
            parsed_main_url = urlparse(api_url)
            base_url = f"{parsed_main_url.scheme}://{parsed_main_url.netloc}"
            full_url = base_url + file_url_path
            file_name = os.path.basename(file_url_path)
            download_commands = (
                f"ğŸ“¥ *æ–‡ä»¶ä¸‹è½½å‘½ä»¤*\n\n"
                f"*cURL:*\n`curl -o \"{escape_markdown_v2(file_name)}\" \"{escape_markdown_v2(full_url)}\"`\n\n"
                f"*Wget:*\n`wget --content-disposition \"{escape_markdown_v2(full_url)}\"`"
            )
            context.bot.send_message(chat_id, download_commands, parse_mode=ParseMode.MARKDOWN_V2)
        else:
            raise ValueError(f"å“åº”æ ¼å¼ä¸æ­£ç¡®: {result}")
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        context.bot.send_message(chat_id, f"âš ï¸ æ–‡ä»¶ä¸Šä¼ åˆ°å¤–éƒ¨æœåŠ¡å™¨å¤±è´¥: `{escape_markdown_v2(str(e))}`", parse_mode=ParseMode.MARKDOWN_V2)

# --- FOFA API æ ¸å¿ƒé€»è¾‘ ---
def _make_api_request(url, params, timeout=60, use_b64=True, retries=10, proxy_session=None):
    if use_b64 and 'q' in params:
        params['qbase64'] = base64.b64encode(params.pop('q').encode('utf-8')).decode('utf-8')
    
    last_error = None
    # v10.9.4 FIX: ä¸ºæ•´ä¸ªé‡è¯•å¾ªç¯ç¡®å®šä»£ç†ã€‚
    # å¦‚æœä¼ é€’äº†ç‰¹å®šçš„ä¼šè¯ï¼Œåˆ™ä½¿ç”¨å®ƒã€‚å¦åˆ™ï¼Œä¸ºæ­¤å°è¯•è·å–ä¸€ä¸ªéšæœºçš„ã€‚
    request_proxies = get_proxies(proxy_to_use=proxy_session)

    for attempt in range(retries):
        try:
            response = requests.get(url, params=params, timeout=timeout, proxies=request_proxies, verify=False)
            if response.status_code == 429:
                wait_time = 5 * (attempt + 1)
                logger.warning(f"FOFA API rate limit hit (429). Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{retries})")
                time.sleep(wait_time)
                last_error = f"APIè¯·æ±‚å› é€Ÿç‡é™åˆ¶(429)å¤±è´¥"
                continue
            if response.status_code == 502: # Bad Gateway
                wait_time = 5 * (attempt + 1)
                logger.warning(f"FOFA API returned 502 Bad Gateway. Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{retries})")
                time.sleep(wait_time)
                last_error = "APIè¯·æ±‚å¤±è´¥ (502 Bad Gateway)"
                continue
            response.raise_for_status()
            data = response.json()
            if data.get("error"):
                return None, data.get("errmsg", "æœªçŸ¥çš„FOFAé”™è¯¯")
            return data, None
        except requests.exceptions.RequestException as e:
            last_error = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}"
            logger.error(f"RequestException on attempt {attempt + 1}: {e}")
            time.sleep(5)
        except json.JSONDecodeError as e:
            last_error = f"è§£æJSONå“åº”å¤±è´¥: {e}"
            break
    logger.error(f"API request failed after {retries} retries. Last error: {last_error}")
    return None, last_error if last_error else "APIè¯·æ±‚æœªçŸ¥é”™è¯¯"
def verify_fofa_api(key): return _make_api_request(FOFA_INFO_URL, {'key': key}, timeout=15, use_b64=False, retries=3)
def fetch_fofa_data(key, query, page=1, page_size=10000, fields="host", proxy_session=None):
    query_lower = query.lower()
    if 'body=' in query_lower: page_size = min(page_size, 500)
    elif 'cert=' in query_lower: page_size = min(page_size, 2000)
    params = {'key': key, 'q': query, 'size': page_size, 'page': page, 'fields': fields, 'full': CONFIG.get("full_mode", False)}
    return _make_api_request(FOFA_SEARCH_URL, params, proxy_session=proxy_session)
def fetch_fofa_stats(key, query, proxy_session=None):
    params = {'key': key, 'q': query, 'fields': FOFA_STATS_FIELDS}
    return _make_api_request(FOFA_STATS_URL, params, proxy_session=proxy_session)
def fetch_fofa_host_info(key, host, detail=False, proxy_session=None):
    url = FOFA_HOST_BASE_URL + host
    params = {'key': key, 'detail': str(detail).lower()}
    return _make_api_request(url, params, use_b64=False, proxy_session=proxy_session)
def fetch_fofa_next_data(key, query, next_id=None, page_size=10000, fields="host", proxy_session=None):
    params = {'key': key, 'q': query, 'size': page_size, 'fields': fields, 'full': CONFIG.get("full_mode", False)}
    # FIX: Ensure 'next' parameter is always present, and empty on the first call, to comply with API spec.
    params['next'] = next_id if next_id is not None else ""
    return _make_api_request(FOFA_NEXT_URL, params, proxy_session=proxy_session)

# --- æ™ºèƒ½ä¸‹è½½æ ¸å¿ƒå·¥å…· ---
def iter_fofa_traceback(key, query, limit=None, proxy_session=None, page_size=10000):
    """
    é€šè¿‡ before/after æ—¶é—´å›æº¯æœºåˆ¶è¿­ä»£è·å–æ•°æ®çš„ç”Ÿæˆå™¨ã€‚
    Yields: ç»“æœåˆ—è¡¨
    """
    current_query = query
    last_page_date = None
    collected_count = 0
    
    # ç®€å•çš„å“ˆå¸Œå»é‡ï¼ˆç”¨äºå¤„ç†åŒä¸€å¤©çš„åˆ†é¡µé‡å ï¼‰
    page_hashes = set() 
    
    while True:
        # è·å–ç¬¬ä¸€é¡µ
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è¯·æ±‚ lastupdatetime ä»¥ä¾¿ç¡®å®šä¸‹ä¸€é¡µçš„ before æ—¶é—´é”šç‚¹
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œå¦‚æœæ²¡æœ‰ VIP æƒé™ï¼Œè¿™ä¸ª fields è¯·æ±‚å¯èƒ½ä¼šè¢«å¿½ç•¥æˆ–è€…éœ€è¦å¤–éƒ¨ç¡®ä¿ Key æƒé™
        # è¿™é‡Œå‡è®¾è°ƒç”¨æ­¤å‡½æ•°æ—¶å·²ä½¿ç”¨äº†å…·å¤‡æƒé™çš„ Key
        fields = "host,lastupdatetime"
        
        # ä½¿ç”¨ execute_query_with_fallback çš„ç­‰ä»·å•æ¬¡è°ƒç”¨ï¼Œæˆ–è€…ç›´æ¥è°ƒ fetchã€‚
        # è¿™é‡Œæ˜¯è¿­ä»£å™¨å†…éƒ¨ï¼Œå‡å®š key æ˜¯ç¡®å®šçš„ã€‚
        # å¦‚æœ Key ç­‰çº§ < 1 (æ— æ³•æŸ¥è¯¢ lastupdatetime)ï¼Œåˆ™åªèƒ½æŸ¥æ™®é€šç¿»é¡µï¼Œè¿™ä¼šå¯¼è‡´å¤§é‡æ•°æ®ä¸‹çš„æ­»å¾ªç¯ï¼Œ
        # æ‰€ä»¥å¤–éƒ¨å¿…é¡»ç¡®ä¿ key level >= 1
        
        data, error = fetch_fofa_data(key, current_query, page=1, page_size=page_size, fields=fields, proxy_session=proxy_session)
        
        if error or not data or not data.get('results'):
            break

        results = data.get('results', [])
        if not results:
            break

        # Yield current batch
        # æˆ‘ä»¬è¿”å›å®Œæ•´ç»“æœä»¥ä¾¿å¤–éƒ¨å¤„ç†
        yield results
        collected_count += len(results)
        if limit and collected_count >= limit:
            break

        # åˆ†ææœ€åä¸€æ¡çš„æ—¶é—´ï¼Œè®¾ç½®æ–°çš„ Time Anchor
        # FOFA ç»“æœæ˜¯å€’åºçš„ï¼Œæœ€åä¸€æ¡æ˜¯æœ€æ—§çš„
        # å–æœ€åä¸€æ¡çš„æ—¶é—´ï¼Œä½œä¸ºä¸‹ä¸€è½®çš„ before
        valid_anchor_found = False
        
        # å€’åºå¯»æ‰¾æœ‰æ•ˆæ—¶é—´æˆ³
        for i in range(len(results) - 1, -1, -1):
            if not results[i] or len(results[i]) < 2: continue
            
            # æ ¼å¼å¯èƒ½æ˜¯ "2023-01-01 12:00:00"
            ts_str = results[i][-1] # lastupdatetime
            try:
                current_date_obj = datetime.strptime(ts_str.split(' ')[0], '%Y-%m-%d').date()
                
                # é˜²æ­¢æ­»å¾ªç¯ï¼šå¦‚æœè¿™é¡µæ‰¾åˆ°çš„æ—¥æœŸ >= ä¸Šä¸€é¡µæ‰¾åˆ°çš„é”šç‚¹æ—¥æœŸï¼Œè¯´æ˜åœ¨è¿™ä¸€å¤©å†…å¡ä½äº†
                # æˆ‘ä»¬éœ€è¦å¼ºåˆ¶å°†æ—¥æœŸ -1 å¤©æ¥è·³è¿‡è¿™ä¸€å¤©ï¼ˆä¼šæœ‰æ•°æ®æŸå¤±ï¼Œä½†å¥½è¿‡æ­»å¾ªç¯ï¼‰
                # æˆ–è€…ï¼ŒFOFA api æ”¯æŒ page ç¿»é¡µï¼Œå¦‚æœæ˜¯åœ¨åŒä¸€å¤©ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ç¿» page 2?
                # ç®€åŒ–èµ·è§ï¼šTime Slicing ç­–ç•¥æ˜¯â€œå¤©â€çº§çš„ã€‚å¦‚æœä¸€å¤© > 10000 æ¡ï¼Œè¿™é‡Œçš„é€»è¾‘ä¼šè·³è¿‡å½“å¤©å‰©ä½™æ•°æ®ã€‚
                # ä½†æ ¹æ® Smart Slicing å‡è®¾ï¼Œå›½å®¶è¢«å‰¥ç¦»åï¼Œå•æ—¥å•å›½æ•°æ®å¾ˆéš¾ > 10000ã€‚
                
                next_page_date_obj = current_date_obj
                
                if last_page_date and current_date_obj >= last_page_date:
                    # å¦‚æœæ—¶é—´æ²¡æœ‰å‰æ¨ï¼Œå¼ºåˆ¶ -1 å¤©
                    next_page_date_obj -= timedelta(days=1)
                
                last_page_date = next_page_date_obj
                
                # æ›´æ–°æŸ¥è¯¢ï¼šè¿½åŠ  before å‚æ•°
                # æ³¨æ„å¤„ç† query ä¸­ç°æœ‰çš„æ‹¬å·
                current_query = f'({query}) && before="{next_page_date_obj.strftime("%Y-%m-%d")}"'
                valid_anchor_found = True
                break
            except (ValueError, TypeError, IndexError):
                continue
        
        if not valid_anchor_found:
            break

def check_and_classify_keys():
    logger.info("--- å¼€å§‹æ£€æŸ¥å¹¶åˆ†ç±»API Keys ---")
    global KEY_LEVELS
    KEY_LEVELS.clear()
    for key in CONFIG.get('apis', []):
        data, error = verify_fofa_api(key)
        if error:
            logger.warning(f"Key '...{key[-4:]}' æ— æ•ˆ: {error}")
            KEY_LEVELS[key] = -1
            continue
        is_vip = data.get('isvip', False)
        api_level = data.get('vip_level', 0)
        level = 0
        if not is_vip:
            level = 0
        else:
            if api_level == 2: level = 1
            elif api_level == 3: level = 2
            elif api_level >= 4: level = 3
            else: level = 1 
        KEY_LEVELS[key] = level
        level_name = {0: "å…è´¹ä¼šå‘˜", 1: "ä¸ªäººä¼šå‘˜", 2: "å•†ä¸šä¼šå‘˜", 3: "ä¼ä¸šä¼šå‘˜"}.get(level, "æœªçŸ¥ç­‰çº§")
        logger.info(f"Key '...{key[-4:]}' ({data.get('username', 'N/A')}) - ç­‰çº§: {level} ({level_name})")
    logger.info("--- API Keys åˆ†ç±»å®Œæˆ ---")

def get_fields_by_level(level):
    if level >= 3: return ENTERPRISE_FIELDS
    if level == 2: return BUSINESS_FIELDS
    if level == 1: return PERSONAL_FIELDS
    return FREE_FIELDS

def execute_query_with_fallback(query_func, preferred_key_index=None, proxy_session=None, min_level=0):
    if not CONFIG['apis']: return None, None, None, None, None, "æ²¡æœ‰é…ç½®ä»»ä½•API Keyã€‚"
    
    keys_to_try = [k for k in CONFIG['apis'] if KEY_LEVELS.get(k, -1) >= min_level]
    
    if not keys_to_try:
        if min_level > 0:
            return None, None, None, None, None, f"æ²¡æœ‰æ‰¾åˆ°ç­‰çº§ä¸ä½äºâ€œä¸ªäººä¼šå‘˜â€çš„æœ‰æ•ˆAPI Keyä»¥æ‰§è¡Œæ­¤æ“ä½œã€‚"
        return None, None, None, None, None, "æ‰€æœ‰é…ç½®çš„API Keyéƒ½æ— æ•ˆã€‚"
    
    start_index = 0
    if preferred_key_index is not None and 1 <= preferred_key_index <= len(CONFIG['apis']):
        preferred_key = CONFIG['apis'][preferred_key_index - 1]
        if preferred_key in keys_to_try:
            start_index = keys_to_try.index(preferred_key)

    # v10.9.4 FIX: å¦‚æœæœªé”å®šä»£ç†ä¼šè¯ï¼Œåˆ™åœ¨æ­¤å›é€€åºåˆ—çš„æŒç»­æ—¶é—´å†…é€‰æ‹©ä¸€ä¸ªã€‚
    current_proxy_session_str = proxy_session
    if current_proxy_session_str is None:
        proxies_list = CONFIG.get("proxies", [])
        if proxies_list:
            current_proxy_session_str = random.choice(proxies_list)
        else:
            current_proxy_session_str = CONFIG.get("proxy")

    for i in range(len(keys_to_try)):
        idx = (start_index + i) % len(keys_to_try)
        key = keys_to_try[idx]
        key_num = CONFIG['apis'].index(key) + 1
        key_level = KEY_LEVELS.get(key, 0)
        
        # v10.9.4 FIX: å°†keyã€key_levelå’Œä¸€è‡´çš„proxy_sessionä¼ é€’ç»™æŸ¥è¯¢å‡½æ•°ã€‚
        data, error = query_func(key, key_level, current_proxy_session_str)
        
        if not error:
            # è¿”å›æˆåŠŸä½¿ç”¨çš„ä»£ç†ã€‚
            return data, key, key_num, key_level, current_proxy_session_str, None
        if "[820031]" in str(error):
            logger.warning(f"Key [#{key_num}] Fç‚¹ä½™é¢ä¸è¶³...");
            continue
        # å¯¹äºå…¶ä»–é”™è¯¯ï¼Œå¿«é€Ÿå¤±è´¥å¹¶è¿”å›é—®é¢˜keyçš„ä¿¡æ¯
        return None, key, key_num, key_level, current_proxy_session_str, error
        
    return None, None, None, None, None, "æ‰€æœ‰Keyå‡å°è¯•å¤±è´¥ (å¯èƒ½Fç‚¹å‡ä¸è¶³)ã€‚"

# --- å¼‚æ­¥æ‰«æé€»è¾‘ ---
async def async_check_port(host, port, timeout):
    try:
        fut = asyncio.open_connection(host, port)
        _, writer = await asyncio.wait_for(fut, timeout=timeout)
        writer.close(); await writer.wait_closed()
        return f"{host}:{port}"
    except (asyncio.TimeoutError, ConnectionRefusedError, OSError, socket.gaierror): return None
    except Exception: return None

async def async_scanner_orchestrator(targets, concurrency, timeout, mode='tcping', progress_callback=None):
    semaphore = asyncio.Semaphore(concurrency)
    scan_targets = []
    if mode == 'tcping':
        for t in targets:
            try:
                host, port_str = t.split(':', 1)
                scan_targets.append((host, int(port_str)))
            except (ValueError, IndexError): continue
    elif mode == 'subnet':
        subnets_to_ports = {}
        for line in targets:
            try:
                ip_str, port_str = line.strip().split(':'); port = int(port_str)
                subnet = ".".join(ip_str.split('.')[:3])
                if subnet not in subnets_to_ports: subnets_to_ports[subnet] = set()
                subnets_to_ports[subnet].add(port)
            except ValueError: continue
        for subnet, ports in subnets_to_ports.items():
            for i in range(1, 255):
                for port in ports:
                    scan_targets.append((f"{subnet}.{i}", port))

    total_tasks = len(scan_targets)
    completed_tasks = 0
    
    async def worker(host, port):
        nonlocal completed_tasks
        async with semaphore:
            result = await async_check_port(host, port, timeout)
            completed_tasks += 1
            if progress_callback:
                await progress_callback(completed_tasks, total_tasks)
            return result

    tasks = [worker(host, port) for host, port in scan_targets]
    results = await asyncio.gather(*tasks)
    return [res for res in results if res is not None]

def run_async_scan_job(context: CallbackContext):
    job_context = context.job.context
    chat_id, msg, original_query, mode = job_context['chat_id'], job_context['msg'], job_context['original_query'], job_context['mode']
    concurrency, timeout = job_context['concurrency'], job_context['timeout']
    
    cached_item = find_cached_query(original_query)
    if not cached_item:
        try: msg.edit_text("âŒ æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶çš„æœ¬åœ°ç¼“å­˜è®°å½•ã€‚")
        except (BadRequest, RetryAfter, TimedOut): pass
        return

    try: msg.edit_text("1/3: æ­£åœ¨è¯»å–æœ¬åœ°ç¼“å­˜æ–‡ä»¶...")
    except (BadRequest, RetryAfter, TimedOut): pass
    
    try:
        with open(cached_item['cache']['file_path'], 'r', encoding='utf-8') as f:
            targets = [line.strip() for line in f if ':' in line.strip()]
    except Exception as e:
        try: msg.edit_text(f"âŒ è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        except (BadRequest, RetryAfter, TimedOut): pass
        return

    scan_type_text = "TCPå­˜æ´»æ‰«æ" if mode == 'tcping' else "å­ç½‘æ‰«æ"
    
    async def main_scan_logic():
        last_update_time = 0
        
        async def progress_callback(completed, total):
            nonlocal last_update_time
            current_time = time.time()
            if total > 0 and current_time - last_update_time > 2:
                percentage = (completed / total) * 100
                progress_bar = create_progress_bar(percentage)
                try:
                    msg.edit_text(
                        f"2/3: æ­£åœ¨è¿›è¡Œå¼‚æ­¥{scan_type_text}...\n"
                        f"{progress_bar} ({completed}/{total})"
                    )
                    last_update_time = current_time
                except (BadRequest, RetryAfter, TimedOut):
                    pass # Ignore if editing fails, continue scanning

        initial_message = f"2/3: å·²åŠ è½½ {len(targets)} ä¸ªç›®æ ‡ï¼Œå¼€å§‹å¼‚æ­¥{scan_type_text} (å¹¶å‘: {concurrency}, è¶…æ—¶: {timeout}s)..."
        try:
            msg.edit_text(initial_message)
        except (BadRequest, RetryAfter, TimedOut):
            pass

        return await async_scanner_orchestrator(targets, concurrency, timeout, mode, progress_callback)

    live_results = asyncio.run(main_scan_logic())
    
    if not live_results:
        try: msg.edit_text("ğŸ¤·â€â™€ï¸ æ‰«æå®Œæˆï¼Œä½†æœªå‘ç°ä»»ä½•å­˜æ´»çš„ç›®æ ‡ã€‚")
        except (BadRequest, RetryAfter, TimedOut): pass
        return

    try: msg.edit_text("3/3: æ­£åœ¨æ‰“åŒ…å¹¶å‘é€æ–°ç»“æœ...")
    except (BadRequest, RetryAfter, TimedOut): pass
    
    output_filename = generate_filename_from_query(original_query, prefix=f"{mode}_scan")
    with open(output_filename, 'w', encoding='utf-8') as f: f.write("\n".join(sorted(list(live_results))))
    
    final_caption = f"âœ… *å¼‚æ­¥{escape_markdown_v2(scan_type_text)}å®Œæˆ\!*\n\nå…±å‘ç° *{len(live_results)}* ä¸ªå­˜æ´»ç›®æ ‡\\."
    send_file_safely(context, chat_id, output_filename, caption=final_caption, parse_mode=ParseMode.MARKDOWN_V2)
    upload_and_send_links(context, chat_id, output_filename)
    os.remove(output_filename)
    try: msg.delete()
    except (BadRequest, RetryAfter, TimedOut): pass

# --- æ‰«ææµç¨‹å…¥å£ ---
def offer_post_download_actions(context: CallbackContext, chat_id, query_text):
    query_hash = hashlib.md5(query_text.encode()).hexdigest()
    SCAN_TASKS[query_hash] = query_text
    while len(SCAN_TASKS) > MAX_SCAN_TASKS:
        SCAN_TASKS.pop(next(iter(SCAN_TASKS)))
    save_scan_tasks()

    keyboard = [[
        InlineKeyboardButton("âš¡ï¸ å¼‚æ­¥TCPå­˜æ´»æ‰«æ", callback_data=f'start_scan_tcping_{query_hash}'),
        InlineKeyboardButton("ğŸŒ å¼‚æ­¥å­ç½‘æ‰«æ(/24)", callback_data=f'start_scan_subnet_{query_hash}')
    ]]
    context.bot.send_message(chat_id, "ä¸‹è½½å®Œæˆï¼Œéœ€è¦å¯¹ç»“æœè¿›è¡ŒäºŒæ¬¡æ‰«æå—ï¼Ÿ", reply_markup=InlineKeyboardMarkup(keyboard))
def start_scan_callback(update: Update, context: CallbackContext) -> int:
    query = update.callback_query; query.answer()
    # v10.9.1 FIX: Correctly parse callback data to get mode and query_hash
    try:
        _, _, mode, query_hash = query.data.split('_', 3)
    except ValueError:
        logger.error(f"æ— æ³•ä»å›è°ƒæ•°æ®è§£ææ‰«æä»»åŠ¡: {query.data}")
        query.message.edit_text("âŒ å†…éƒ¨é”™è¯¯ï¼šæ— æ³•è§£ææ‰«æä»»åŠ¡ã€‚")
        return ConversationHandler.END

    original_query = SCAN_TASKS.get(query_hash)
    if not original_query:
        query.message.edit_text("âŒ æ‰«æä»»åŠ¡å·²è¿‡æœŸæˆ–æœºå™¨äººåˆšåˆšé‡å¯ã€‚è¯·é‡æ–°å‘èµ·æŸ¥è¯¢ä»¥å¯ç”¨æ‰«æã€‚")
        return ConversationHandler.END

    context.user_data['scan_original_query'] = original_query
    context.user_data['scan_mode'] = mode
    query.message.edit_text("è¯·è¾“å…¥æ‰«æå¹¶å‘æ•° (å»ºè®® 100-1000):")
    return SCAN_STATE_GET_CONCURRENCY
def get_concurrency_callback(update: Update, context: CallbackContext) -> int:
    try:
        concurrency = int(update.message.text)
        if not 1 <= concurrency <= 5000: raise ValueError
        context.user_data['scan_concurrency'] = concurrency
        update.message.reply_text("è¯·è¾“å…¥è¿æ¥è¶…æ—¶æ—¶é—´ (ç§’, å»ºè®® 1-3):")
        return SCAN_STATE_GET_TIMEOUT
    except ValueError:
        update.message.reply_text("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1-5000 ä¹‹é—´çš„æ•´æ•°ã€‚")
        return SCAN_STATE_GET_CONCURRENCY
def get_timeout_callback(update: Update, context: CallbackContext) -> int:
    try:
        timeout = float(update.message.text)
        if not 0.1 <= timeout <= 10: raise ValueError
        msg = update.message.reply_text("âœ… å‚æ•°è®¾ç½®å®Œæ¯•ï¼Œä»»åŠ¡å·²æäº¤åˆ°åå°ã€‚")
        job_context = {
            'chat_id': update.effective_chat.id, 'msg': msg,
            'original_query': context.user_data['scan_original_query'],
            'mode': context.user_data['scan_mode'],
            'concurrency': context.user_data['scan_concurrency'],
            'timeout': timeout
        }
        context.job_queue.run_once(run_async_scan_job, 1, context=job_context, name=f"scan_{update.effective_chat.id}")
        context.user_data.clear()
        return ConversationHandler.END
    except ValueError:
        update.message.reply_text("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 0.1-10 ä¹‹é—´çš„æ•°å­—ã€‚")
        return SCAN_STATE_GET_TIMEOUT

# --- åå°ä¸‹è½½ä»»åŠ¡ ---
def start_download_job(context: CallbackContext, callback_func, job_data):
    chat_id = job_data['chat_id']; job_name = f"download_job_{chat_id}"
    for job in context.job_queue.get_jobs_by_name(job_name): job.schedule_removal()
    context.bot_data.pop(f'stop_job_{chat_id}', None)
    context.job_queue.run_once(callback_func, 1, context=job_data, name=job_name)
def run_full_download_query(context: CallbackContext):
    job_data = context.job.context; bot, chat_id, query_text, total_size = context.bot, job_data['chat_id'], job_data['query'], job_data['total_size']
    output_filename = generate_filename_from_query(query_text); unique_results, stop_flag = set(), f'stop_job_{chat_id}'
    msg = bot.send_message(chat_id, "â³ å¼€å§‹å…¨é‡ä¸‹è½½ä»»åŠ¡..."); pages_to_fetch = (total_size + 9999) // 10000
    for page in range(1, pages_to_fetch + 1):
        if context.bot_data.get(stop_flag): msg.edit_text("ğŸŒ€ ä¸‹è½½ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢."); break
        try: msg.edit_text(f"ä¸‹è½½è¿›åº¦: {len(unique_results)}/{total_size} (Page {page}/{pages_to_fetch})...")
        except (BadRequest, RetryAfter, TimedOut): pass
        guest_key = job_data.get('guest_key')
        if guest_key:
            data, error = fetch_fofa_data(guest_key, query_text, page, 10000, "host")
        else:
            data, _, _, _, _, error = execute_query_with_fallback(
                lambda key, key_level, proxy_session: fetch_fofa_data(key, query_text, page, 10000, "host", proxy_session=proxy_session)
            )
        if error: msg.edit_text(f"âŒ ç¬¬ {page} é¡µä¸‹è½½å‡ºé”™: {error}"); break
        results = data.get('results', []);
        if not results: break
        unique_results.update(res for res in results if ':' in res)
    if unique_results:
        with open(output_filename, 'w', encoding='utf-8') as f: f.write("\n".join(unique_results))
        msg.edit_text(f"âœ… ä¸‹è½½å®Œæˆï¼å…± {len(unique_results)} æ¡ã€‚æ­£åœ¨å‘é€...")
        cache_path = os.path.join(FOFA_CACHE_DIR, output_filename)
        shutil.move(output_filename, cache_path)
        send_file_safely(context, chat_id, cache_path, filename=output_filename)
        upload_and_send_links(context, chat_id, cache_path)
        cache_data = {'file_path': cache_path, 'result_count': len(unique_results)}
        add_or_update_query(query_text, cache_data); offer_post_download_actions(context, chat_id, query_text)
    elif not context.bot_data.get(stop_flag): msg.edit_text("ğŸ¤·â€â™€ï¸ ä»»åŠ¡å®Œæˆï¼Œä½†æœªèƒ½ä¸‹è½½åˆ°ä»»ä½•æ•°æ®ã€‚")
    context.bot_data.pop(stop_flag, None)

def run_sharded_download_job(context: CallbackContext):
    """
    æ™ºèƒ½åˆ†ç‰‡ä¸‹è½½ä»»åŠ¡ï¼šæŒ‰å›½å®¶ä»£ç å°†æŸ¥è¯¢æ‹†åˆ†ï¼Œç»•è¿‡å•æ¬¡æŸ¥è¯¢10000æ¡çš„é™åˆ¶ã€‚
    """
    job_data = context.job.context
    bot, chat_id, base_query = context.bot, job_data['chat_id'], job_data['query']
    
    output_filename = generate_filename_from_query(base_query, prefix="sharded")
    unique_results = set()
    stop_flag = f'stop_job_{chat_id}'
    
    msg = bot.send_message(chat_id, f"â³ *å¯åŠ¨æ™ºèƒ½åˆ†ç‰‡ä¸‹è½½*\nç›®æ ‡ï¼šå°†æŸ¥è¯¢æŒ‰ {len(ALL_COUNTRY_CODES)} ä¸ªå›½å®¶åŒºåŸŸæ‹†åˆ†...\næ³¨æ„ï¼šæ­¤æ¨¡å¼å°†æ¶ˆè€—è¾ƒå¤šçš„ API è¯·æ±‚æ¬¡æ•°ã€‚", parse_mode=ParseMode.MARKDOWN_V2)
    
    start_time = time.time()
    last_ui_update_time = 0
    total_codes = len(ALL_COUNTRY_CODES)
    
    # éå†æ‰€æœ‰å›½å®¶
    for i, country_code in enumerate(ALL_COUNTRY_CODES):
        if context.bot_data.get(stop_flag):
            try: msg.edit_text("ğŸ›‘ ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚")
            except (BadRequest, RetryAfter, TimedOut): pass
            break
            
        current_time = time.time()
        # æ›´æ–°è¿›åº¦UI (æ¯2ç§’æœ€å¤šæ›´æ–°ä¸€æ¬¡)
        if current_time - last_ui_update_time > 2 or i == 0:
            elapsed = current_time - start_time
            speed = len(unique_results) / elapsed if elapsed > 0 else 0
            progress_bar = create_progress_bar((i / total_codes) * 100)
            try:
                msg.edit_text(
                    f"ğŸŒ *æ­£åœ¨åˆ†ç‰‡æ‰«æ...* `{country_code}`\n"
                    f"{progress_bar} {i}/{total_codes}\n"
                    f"å·²æ”¶é›†æ•°æ®: *{len(unique_results)}* æ¡\n"
                    f"å½“å‰å¹³å‡é€Ÿåº¦: *{int(speed)}* æ¡/ç§’",
                    parse_mode=ParseMode.MARKDOWN_V2
                )
                last_ui_update_time = current_time
            except (BadRequest, RetryAfter, TimedOut):
                pass

        # æ„é€ åˆ†ç‰‡æŸ¥è¯¢
        sharded_query = f'({base_query}) && country="{country_code}"'
        
        # å†…éƒ¨æŸ¥è¯¢å‡½æ•°
        def query_logic(key, key_level, proxy_session):
            # ä¸ºäº†èŠ‚çœæµé‡å’Œé€Ÿåº¦ï¼Œé»˜è®¤åªè¯·æ±‚ç¬¬ä¸€é¡µ (max 10000 per country is usually enough for most cases)
            return fetch_fofa_data(key, sharded_query, page=1, page_size=10000, fields="host", proxy_session=proxy_session)

        # å°è¯•æŸ¥è¯¢
        guest_key = job_data.get('guest_key')
        if guest_key:
            data, error = fetch_fofa_data(guest_key, sharded_query, page=1, page_size=10000, fields="host")
        else:
            data, _, _, _, _, error = execute_query_with_fallback(query_logic)
        
        # å¤„ç†ç»“æœ
        if not error and data and data.get('results'):
            new_data = data['results']
            # å¤„ç†ç®€å•å­—ç¬¦ä¸²ç»“æœæˆ–åˆ—è¡¨ç»“æœ
            extracted_hosts = []
            if new_data and isinstance(new_data[0], list):
                 extracted_hosts = [r[0] for r in new_data if r and r[0] and ':' in r[0]]
            else:
                 extracted_hosts = [r for r in new_data if isinstance(r, str) and ':' in r]
            
            unique_results.update(extracted_hosts)
            
            # (å¯é€‰ä¼˜åŒ–) å¦‚æœå•ä¸ªå›½å®¶ç»“æœä¹Ÿæ˜¯æ»¡çš„ 10000ï¼Œç†æƒ³æƒ…å†µåº”è¯¥å†å¯¹è¯¥å›½å®¶æŒ‰ region åˆ†ç‰‡
            # ä½†è¿™é‡Œä¸ºäº†é¿å…æ— é™é€’å½’ï¼Œæš‚æ—¶æ¥å—å•ä¸ªåˆ†ç‰‡ 10000 çš„ä¸Šé™ã€‚å¯¹äºç»å¤§å¤šæ•°å›½å®¶å·²è¶³å¤Ÿã€‚

    # å¾ªç¯ç»“æŸåçš„æ”¶å°¾
    context.bot_data.pop(stop_flag, None)
    
    if unique_results:
        final_count = len(unique_results)
        msg.edit_text(f"âœ… åˆ†ç‰‡æ‰«æå®Œæˆï¼\næ€»è®¡å‘ç° *{final_count}* æ¡å”¯ä¸€æ•°æ®ã€‚\næ­£åœ¨ç”Ÿæˆå¹¶å‘é€æ–‡ä»¶...", parse_mode=ParseMode.MARKDOWN_V2)
        
        with open(output_filename, 'w', encoding='utf-8') as f:
            f.write("\n".join(sorted(list(unique_results))))
            
        cache_path = os.path.join(FOFA_CACHE_DIR, output_filename)
        shutil.move(output_filename, cache_path)
        send_file_safely(context, chat_id, cache_path, filename=output_filename)
        upload_and_send_links(context, chat_id, cache_path)
        
        cache_data = {'file_path': cache_path, 'result_count': final_count}
        add_or_update_query(base_query, cache_data)
        offer_post_download_actions(context, chat_id, base_query)
    else:
        msg.edit_text("ğŸ¤·â€â™€ï¸ ä»»åŠ¡å®Œæˆï¼Œä½†åœ¨ä»»ä½•å›½å®¶åˆ†ç‰‡ä¸­éƒ½æœªæ‰¾åˆ°æ•°æ®ã€‚")

def run_traceback_download_query(context: CallbackContext):
    job_data = context.job.context; bot, chat_id, base_query = context.bot, job_data['chat_id'], job_data['query']; limit = job_data.get('limit')
    output_filename = generate_filename_from_query(base_query); unique_results, page_count, last_page_date, termination_reason, stop_flag, last_update_time = set(), 0, None, "", f'stop_job_{chat_id}', 0
    msg = bot.send_message(chat_id, "â³ å¼€å§‹æ·±åº¦è¿½æº¯ä¸‹è½½...")
    current_query = base_query
    guest_key = job_data.get('guest_key')
    
    # v10.9.4 FIX: ä¸ºæ•´ä¸ªè¿½æº¯è¿‡ç¨‹é”å®šä¸€ä¸ªä»£ç†ä¼šè¯
    locked_proxy_session = None

    while True:
        page_count += 1
        if context.bot_data.get(stop_flag): termination_reason = "\n\nğŸŒ€ ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢."; break

        fields_were_extended = False
        if guest_key:
            # Guest keys are assumed to be low-level, don't request lastupdatetime
            data, error = fetch_fofa_data(guest_key, current_query, 1, 10000, fields="host")
        else:
            def query_logic(key, key_level, proxy_session):
                nonlocal fields_were_extended
                # Personal members and above can search this field.
                if key_level >= 1:
                    fields_were_extended = True
                    return fetch_fofa_data(key, current_query, 1, 10000, fields="host,lastupdatetime", proxy_session=proxy_session)
                else:
                    fields_were_extended = False
                    return fetch_fofa_data(key, current_query, 1, 10000, fields="host", proxy_session=proxy_session)
            
            # ä»…åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶é€‰æ‹©å¹¶é”å®šä»£ç†
            if locked_proxy_session is None:
                data, _, _, _, locked_proxy_session, error = execute_query_with_fallback(query_logic)
            else:
                data, _, _, _, _, error = execute_query_with_fallback(query_logic, proxy_session=locked_proxy_session)

        if error: termination_reason = f"\n\nâŒ ç¬¬ {page_count} è½®å‡ºé”™: {error}"; break
        results = data.get('results', [])
        if not results: termination_reason = "\n\nâ„¹ï¸ å·²è·å–æ‰€æœ‰æŸ¥è¯¢ç»“æœ."; break

        if fields_were_extended:
            newly_added = [r[0] for r in results if r and r[0] and ':' in r[0]]
        else:
            newly_added = [r for r in results if r and ':' in r]
        
        original_count = len(unique_results)
        unique_results.update(newly_added)
        newly_added_count = len(unique_results) - original_count

        if limit and len(unique_results) >= limit: unique_results = set(list(unique_results)[:limit]); termination_reason = f"\n\nâ„¹ï¸ å·²è¾¾åˆ°æ‚¨è®¾ç½®çš„ {limit} æ¡ç»“æœä¸Šé™ã€‚"; break
        current_time = time.time()
        if current_time - last_update_time > 2:
            try: msg.edit_text(f"â³ å·²æ‰¾åˆ° {len(unique_results)} æ¡... (ç¬¬ {page_count} è½®, æ–°å¢ {newly_added_count})")
            except (BadRequest, RetryAfter, TimedOut): pass
            last_update_time = current_time

        if not fields_were_extended:
             termination_reason = "\n\nâš ï¸ å½“å‰Keyç­‰çº§ä¸æ”¯æŒæ—¶é—´è¿½æº¯ï¼Œå·²è·å–ç¬¬ä¸€é¡µç»“æœã€‚"
             break
        
        valid_anchor_found = False
        for i in range(len(results) - 1, -1, -1):
            if not results[i] or len(results[i]) < 2 or not results[i][1]: continue
            try:
                timestamp_str = results[i][1]; current_date_obj = datetime.strptime(timestamp_str.split(' ')[0], '%Y-%m-%d').date()
                if last_page_date and current_date_obj >= last_page_date: continue
                next_page_date_obj = current_date_obj
                if last_page_date and current_date_obj == last_page_date: next_page_date_obj -= timedelta(days=1)
                last_page_date = current_date_obj; current_query = f'({base_query}) && before="{next_page_date_obj.strftime("%Y-%m-%d")}"'; valid_anchor_found = True
                break
            except (ValueError, TypeError): continue
        if not valid_anchor_found: termination_reason = "\n\nâš ï¸ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´é”šç‚¹ä»¥ç»§ç»­ï¼Œå¯èƒ½å·²è¾¾æŸ¥è¯¢è¾¹ç•Œ."; break
    if unique_results:
        with open(output_filename, 'w', encoding='utf-8') as f: f.write("\n".join(sorted(list(unique_results))))
        msg.edit_text(f"âœ… æ·±åº¦è¿½æº¯å®Œæˆï¼å…± {len(unique_results)} æ¡ã€‚{termination_reason}\næ­£åœ¨å‘é€æ–‡ä»¶...")
        cache_path = os.path.join(FOFA_CACHE_DIR, output_filename)
        shutil.move(output_filename, cache_path)
        send_file_safely(context, chat_id, cache_path, filename=output_filename)
        upload_and_send_links(context, chat_id, cache_path)
        cache_data = {'file_path': cache_path, 'result_count': len(unique_results)}
        add_or_update_query(base_query, cache_data); offer_post_download_actions(context, chat_id, base_query)
    else: msg.edit_text(f"ğŸ¤·â€â™€ï¸ ä»»åŠ¡å®Œæˆï¼Œä½†æœªèƒ½ä¸‹è½½åˆ°ä»»ä½•æ•°æ®ã€‚{termination_reason}")
    context.bot_data.pop(stop_flag, None)
def run_incremental_update_query(context: CallbackContext):
    job_data = context.job.context; bot, chat_id, base_query = context.bot, job_data['chat_id'], job_data['query']; msg = bot.send_message(chat_id, "--- å¢é‡æ›´æ–°å¯åŠ¨ ---")
    try: msg.edit_text("1/5: æ­£åœ¨è·å–æ—§ç¼“å­˜...")
    except (BadRequest, RetryAfter, TimedOut): pass
    cached_item = find_cached_query(base_query)
    if not cached_item: msg.edit_text("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æœ¬åœ°ç¼“å­˜é¡¹ã€‚"); return
    old_file_path = cached_item['cache']['file_path']; old_results = set()
    try:
        with open(old_file_path, 'r', encoding='utf-8') as f: old_results = set(line.strip() for line in f if line.strip() and ':' in line)
    except Exception as e: msg.edit_text(f"âŒ è¯»å–æœ¬åœ°ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}"); return
    try: msg.edit_text("2/5: æ­£åœ¨ç¡®å®šæ›´æ–°èµ·å§‹ç‚¹...")
    except (BadRequest, RetryAfter, TimedOut): pass
    data, _, _, _, _, error = execute_query_with_fallback(
        lambda key, key_level, proxy_session: fetch_fofa_data(key, base_query, fields="lastupdatetime", proxy_session=proxy_session)
    )
    if error or not data.get('results'): msg.edit_text(f"âŒ æ— æ³•è·å–æœ€æ–°è®°å½•æ—¶é—´æˆ³: {error or 'æ— ç»“æœ'}"); return
    ts_str = data['results'][0][0] if isinstance(data['results'][0], list) else data['results'][0]; cutoff_date = ts_str.split(' ')[0]
    incremental_query = f'({base_query}) && after="{cutoff_date}"'
    try: msg.edit_text(f"3/5: æ­£åœ¨ä¾¦å¯Ÿè‡ª {cutoff_date} ä»¥æ¥çš„æ–°æ•°æ®...")
    except (BadRequest, RetryAfter, TimedOut): pass
    data, _, _, _, _, error = execute_query_with_fallback(
        lambda key, key_level, proxy_session: fetch_fofa_data(key, incremental_query, page_size=1, proxy_session=proxy_session)
    )
    if error: msg.edit_text(f"âŒ ä¾¦å¯ŸæŸ¥è¯¢å¤±è´¥: {error}"); return
    total_new_size = data.get('size', 0)
    if total_new_size == 0: msg.edit_text("âœ… æœªå‘ç°æ–°æ•°æ®ã€‚ç¼“å­˜å·²æ˜¯æœ€æ–°ã€‚"); return
    new_results, stop_flag = set(), f'stop_job_{chat_id}'; pages_to_fetch = (total_new_size + 9999) // 10000
    for page in range(1, pages_to_fetch + 1):
        if context.bot_data.get(stop_flag):
            try: msg.edit_text("ğŸŒ€ å¢é‡æ›´æ–°å·²æ‰‹åŠ¨åœæ­¢ã€‚")
            except (BadRequest, RetryAfter, TimedOut): pass
            return
        try: msg.edit_text(f"3/5: æ­£åœ¨ä¸‹è½½æ–°æ•°æ®... ( Page {page}/{pages_to_fetch} )")
        except (BadRequest, RetryAfter, TimedOut): pass
        data, _, _, _, _, error = execute_query_with_fallback(
            lambda key, key_level, proxy_session: fetch_fofa_data(key, incremental_query, page=page, page_size=10000, proxy_session=proxy_session)
        )
        if error: msg.edit_text(f"âŒ ä¸‹è½½æ–°æ•°æ®å¤±è´¥: {error}"); return
        if data.get('results'): new_results.update(res for res in data.get('results', []) if ':' in res)
    try: msg.edit_text(f"4/5: æ­£åœ¨åˆå¹¶æ•°æ®... (å‘ç° {len(new_results)} æ¡æ–°æ•°æ®)")
    except (BadRequest, RetryAfter, TimedOut): pass
    combined_results = sorted(list(new_results.union(old_results)))
    with open(old_file_path, 'w', encoding='utf-8') as f: f.write("\n".join(combined_results))
    try: msg.edit_text(f"5/5: å‘é€æ›´æ–°åçš„æ–‡ä»¶... (å…± {len(combined_results)} æ¡)")
    except (BadRequest, RetryAfter, TimedOut): pass
    send_file_safely(context, chat_id, old_file_path)
    upload_and_send_links(context, chat_id, old_file_path)
    cache_data = {'file_path': old_file_path, 'result_count': len(combined_results)}
    add_or_update_query(base_query, cache_data)
    msg.delete(); bot.send_message(chat_id, f"âœ… å¢é‡æ›´æ–°å®Œæˆï¼"); offer_post_download_actions(context, chat_id, base_query)
def run_batch_download_query(context: CallbackContext):
    job_data = context.job.context; bot, chat_id, query_text, total_size, fields = context.bot, job_data['chat_id'], job_data['query'], job_data['total_size'], job_data['fields']
    output_filename = generate_filename_from_query(query_text, prefix="batch_export", ext=".csv"); results_list, stop_flag = [], f'stop_job_{chat_id}'
    msg = bot.send_message(chat_id, "â³ å¼€å§‹è‡ªå®šä¹‰å­—æ®µæ‰¹é‡å¯¼å‡ºä»»åŠ¡..."); pages_to_fetch = (total_size + 9999) // 10000
    for page in range(1, pages_to_fetch + 1):
        if context.bot_data.get(stop_flag): msg.edit_text("ğŸŒ€ ä¸‹è½½ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢."); break
        try: msg.edit_text(f"ä¸‹è½½è¿›åº¦: {len(results_list)}/{total_size} (Page {page}/{pages_to_fetch})...")
        except (BadRequest, RetryAfter, TimedOut): pass
        data, _, _, _, _, error = execute_query_with_fallback(
            lambda key, key_level, proxy_session: fetch_fofa_data(key, query_text, page, 10000, fields, proxy_session=proxy_session)
        )
        if error: msg.edit_text(f"âŒ ç¬¬ {page} é¡µä¸‹è½½å‡ºé”™: {error}"); break
        page_results = data.get('results', [])
        if not page_results: break
        results_list.extend(page_results)
    if results_list:
        msg.edit_text(f"âœ… ä¸‹è½½å®Œæˆï¼å…± {len(results_list)} æ¡ã€‚æ­£åœ¨ç”ŸæˆCSVæ–‡ä»¶...")
        try:
            with open(output_filename, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f); writer.writerow(fields.split(',')); writer.writerows(results_list)
            send_file_safely(context, chat_id, output_filename, caption=f"âœ… è‡ªå®šä¹‰å¯¼å‡ºå®Œæˆ\næŸ¥è¯¢: `{escape_markdown_v2(query_text)}`", parse_mode=ParseMode.MARKDOWN_V2)
            upload_and_send_links(context, chat_id, output_filename)
        except Exception as e:
            msg.edit_text(f"âŒ ç”Ÿæˆæˆ–å‘é€CSVæ–‡ä»¶å¤±è´¥: {e}"); logger.error(f"Failed to generate/send CSV for batch command: {e}")
        finally:
            if os.path.exists(output_filename): os.remove(output_filename)
            msg.delete()
    elif not context.bot_data.get(stop_flag): msg.edit_text("ğŸ¤·â€â™€ï¸ ä»»åŠ¡å®Œæˆï¼Œä½†æœªèƒ½ä¸‹è½½åˆ°ä»»ä½•æ•°æ®ã€‚")
    context.bot_data.pop(stop_flag, None)
def run_batch_traceback_query(context: CallbackContext):
    job_data = context.job.context; bot, chat_id, base_query, fields, limit = context.bot, job_data['chat_id'], job_data['query'], job_data['fields'], job_data.get('limit')
    output_filename = generate_filename_from_query(base_query, prefix="batch_traceback", ext=".csv")
    unique_results, page_count, last_page_date, termination_reason, stop_flag, last_update_time = [], 0, None, "", f'stop_job_{chat_id}', 0
    msg = bot.send_message(chat_id, "â³ å¼€å§‹è‡ªå®šä¹‰å­—æ®µæ·±åº¦è¿½æº¯ä¸‹è½½...")
    current_query = base_query; seen_hashes = set()
    
    # v10.9.4 FIX: ä¸ºæ•´ä¸ªè¿½æº¯è¿‡ç¨‹é”å®šä¸€ä¸ªä»£ç†ä¼šè¯
    locked_proxy_session = None

    while True:
        page_count += 1
        if context.bot_data.get(stop_flag): termination_reason = "\n\nğŸŒ€ ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢."; break
        
        fields_were_extended = False
        def query_logic(key, key_level, proxy_session):
            nonlocal fields_were_extended
            if key_level >= 1:
                fields_were_extended = True
                return fetch_fofa_data(key, current_query, 1, 10000, fields=fields + ",lastupdatetime", proxy_session=proxy_session)
            else:
                fields_were_extended = False
                return fetch_fofa_data(key, current_query, 1, 10000, fields=fields, proxy_session=proxy_session)

        # ä»…åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶é€‰æ‹©å¹¶é”å®šä»£ç†
        if locked_proxy_session is None:
            data, _, _, _, locked_proxy_session, error = execute_query_with_fallback(query_logic)
        else:
            data, _, _, _, _, error = execute_query_with_fallback(query_logic, proxy_session=locked_proxy_session)

        if error: termination_reason = f"\n\nâŒ ç¬¬ {page_count} è½®å‡ºé”™: {error}"; break
        results = data.get('results', [])
        if not results: termination_reason = "\n\nâ„¹ï¸ å·²è·å–æ‰€æœ‰æŸ¥è¯¢ç»“æœ."; break

        newly_added_count = 0
        for r in results:
            r_hash = hashlib.md5(str(r).encode()).hexdigest()
            if r_hash not in seen_hashes:
                seen_hashes.add(r_hash)
                unique_results.append(r[:-1] if fields_were_extended else r)
                newly_added_count += 1
        if limit and len(unique_results) >= limit: unique_results = unique_results[:limit]; termination_reason = f"\n\nâ„¹ï¸ å·²è¾¾åˆ°æ‚¨è®¾ç½®çš„ {limit} æ¡ç»“æœä¸Šé™ã€‚"; break
        current_time = time.time()
        if current_time - last_update_time > 2:
            try: msg.edit_text(f"â³ å·²æ‰¾åˆ° {len(unique_results)} æ¡... (ç¬¬ {page_count} è½®, æ–°å¢ {newly_added_count})")
            except (BadRequest, RetryAfter, TimedOut): pass
            last_update_time = current_time

        if not fields_were_extended:
             termination_reason = "\n\nâš ï¸ å½“å‰Keyç­‰çº§ä¸æ”¯æŒæ—¶é—´è¿½æº¯ï¼Œå·²è·å–ç¬¬ä¸€é¡µç»“æœã€‚"
             break
        
        valid_anchor_found = False
        for i in range(len(results) - 1, -1, -1):
            if not results[i] or len(results[i]) < 2 or not results[i][-1]: continue
            try:
                timestamp_str = results[i][-1]; current_date_obj = datetime.strptime(timestamp_str.split(' ')[0], '%Y-%m-%d').date()
                if last_page_date and current_date_obj >= last_page_date: continue
                next_page_date_obj = current_date_obj
                if last_page_date and current_date_obj == last_page_date: next_page_date_obj -= timedelta(days=1)
                last_page_date = current_date_obj; current_query = f'({base_query}) && before="{next_page_date_obj.strftime("%Y-%m-%d")}"'; valid_anchor_found = True
                break
            except (ValueError, TypeError): continue
        if not valid_anchor_found: termination_reason = "\n\nâš ï¸ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´é”šç‚¹ä»¥ç»§ç»­ï¼Œå¯èƒ½å·²è¾¾æŸ¥è¯¢è¾¹ç•Œ."; break
    if unique_results:
        msg.edit_text(f"âœ… è¿½æº¯å®Œæˆï¼å…± {len(unique_results)} æ¡ã€‚{termination_reason}\næ­£åœ¨ç”ŸæˆCSV...")
        try:
            with open(output_filename, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f); writer.writerow(fields.split(',')); writer.writerows(unique_results)
            send_file_safely(context, chat_id, output_filename)
            upload_and_send_links(context, chat_id, output_filename)
        except Exception as e:
            msg.edit_text(f"âŒ ç”Ÿæˆæˆ–å‘é€CSVæ–‡ä»¶å¤±è´¥: {e}"); logger.error(f"Failed to generate/send CSV for batch traceback: {e}")
        finally:
            if os.path.exists(output_filename): os.remove(output_filename)
            msg.delete()
    else: msg.edit_text(f"ğŸ¤·â€â™€ï¸ ä»»åŠ¡å®Œæˆï¼Œä½†æœªèƒ½ä¸‹è½½åˆ°ä»»ä½•æ•°æ®ã€‚{termination_reason}")
    context.bot_data.pop(stop_flag, None)

# --- ç›‘æ§ç³»ç»Ÿ (Data Reservoir + Radar Mode) ---
def monitor_command(update: Update, context: CallbackContext):
    args = context.args
    if not args:
        help_txt = (
            "ğŸ“¡ *ç›‘æ§é›·è¾¾æŒ‡ä»¤æ‰‹å†Œ*\n\n"
            "`/monitor add <query>` \\- æ·»åŠ æ–°çš„ç›‘æ§ä»»åŠ¡\n"
            "`/monitor list` \\- æŸ¥çœ‹å½“å‰è¿è¡Œçš„ä»»åŠ¡\n"
            "`/monitor get <id>` \\- æ‰“åŒ…æå–ä»»åŠ¡æ•°æ®\n"
            "`/monitor del <id>` \\- åˆ é™¤ç›‘æ§ä»»åŠ¡\n\n"
            "_ç›‘æ§ä»»åŠ¡ä¼šå°†æ–°æ•°æ®è‡ªåŠ¨æ²‰æ·€åˆ°æœ¬åœ°æ•°æ®åº“ï¼Œæ‚¨éšæ—¶å¯ä»¥æå–ã€‚_"
        )
        update.message.reply_text(help_txt, parse_mode=ParseMode.MARKDOWN_V2)
        return

    sub_cmd = args[0].lower()
    
    if sub_cmd == 'add':
        if len(args) < 2:
            update.message.reply_text("ç”¨æ³•: `/monitor add <query>`")
            return
        query_text = " ".join(args[1:])
        # ç”Ÿæˆç®€çŸ­ID
        task_id = hashlib.md5(query_text.encode()).hexdigest()[:8]
        
        if task_id in MONITOR_TASKS:
            update.message.reply_text(f"âš ï¸ ä»»åŠ¡å·²å­˜åœ¨ (ID: `{task_id}`)", parse_mode=ParseMode.MARKDOWN_V2)
            return
            
        MONITOR_TASKS[task_id] = {
            "query": query_text,
            "chat_id": update.effective_chat.id,
            "added_at": int(time.time()),
            "last_run": 0,
            "interval": 3600, # åˆå§‹1å°æ—¶
            "status": "active"
        }
        save_monitor_tasks()
        
        # ç«‹å³å¯åŠ¨ç¬¬ä¸€æ¬¡è°ƒåº¦ (Use Jitter 0 for first run)
        context.job_queue.run_once(run_monitor_execution_job, 1, context={"task_id": task_id}, name=f"monitor_{task_id}")
        update.message.reply_text(f"âœ… ç›‘æ§é›·è¾¾å·²å¯åŠ¨\nID: `{task_id}`\næŸ¥è¯¢: `{escape_markdown_v2(query_text)}`\n\næ•°æ®å°†è‡ªåŠ¨æ²‰æ·€ï¼Œä½¿ç”¨ `/monitor get {task_id}` æå–ã€‚", parse_mode=ParseMode.MARKDOWN_V2)

    elif sub_cmd == 'list':
        if not MONITOR_TASKS:
            update.message.reply_text("ğŸ“­ å½“å‰æ²¡æœ‰æ´»è·ƒçš„ç›‘æ§ä»»åŠ¡ã€‚")
            return
        msg = ["*ğŸ“¡ æ´»è·ƒç›‘æ§ä»»åŠ¡*"]
        for tid, task in MONITOR_TASKS.items():
            if task.get('status') != 'active': continue
            
            # ç»Ÿè®¡æœ¬åœ°æ•°æ®
            data_file = os.path.join(MONITOR_DATA_DIR, f"{tid}.txt")
            count = 0
            if os.path.exists(data_file):
                try: 
                    with open(data_file, 'r', encoding='utf-8') as f: count = sum(1 for _ in f)
                except: pass
                
            last_run_str = "ç­‰å¾…ä¸­"
            if task.get('last_run'):
                dt = datetime.fromtimestamp(task['last_run']).replace(tzinfo=tz.tzlocal())
                last_run_str = dt.strftime('%H:%M')
            
            # å°†intervalè½¬æ¢ä¸ºåˆ†é’Ÿæˆ–å°æ—¶æ˜¾ç¤º
            interval = task.get('interval', 3600)
            if interval < 3600: dur = f"{interval//60}åˆ†"
            else: dur = f"{interval/3600:.1f}å°æ—¶"

            msg.append(f"ğŸ“¡ `{tid}`: *{escape_markdown_v2(task['query'][:25])}...*")
            msg.append(f"   ğŸ“¦ åº“å­˜: *{count}* \| â± ä¸Šæ¬¡: {last_run_str} \| â³ é¢‘ç‡: {dur}")
            msg.append("") # Spacer
            
        update.message.reply_text("\n".join(msg), parse_mode=ParseMode.MARKDOWN_V2)

    elif sub_cmd == 'del':
        if len(args) < 2: 
            update.message.reply_text("ç”¨æ³•: `/monitor del <task_id>`")
            return
        tid = args[1]
        if tid in MONITOR_TASKS:
            # å–æ¶ˆç°æœ‰ Job
            for job in context.job_queue.get_jobs_by_name(f"monitor_{tid}"):
                job.schedule_removal()
                
            del MONITOR_TASKS[tid]
            save_monitor_tasks()
            
            # åˆ é™¤æ•°æ®æ–‡ä»¶? (ä¿ç•™æ•°æ®æ›´å®‰å…¨ï¼Œåªåˆ ä»»åŠ¡)
            update.message.reply_text(f"ğŸ—‘ï¸ ä»»åŠ¡ `{tid}` å·²åœæ­¢å¹¶ç§»é™¤é…ç½®ã€‚", parse_mode=ParseMode.MARKDOWN_V2)
        else:
            update.message.reply_text("âŒ ä»»åŠ¡IDä¸å­˜åœ¨ã€‚")

    elif sub_cmd == 'get':
        if len(args) < 2:
            update.message.reply_text("ç”¨æ³•: `/monitor get <task_id>`") 
            return
        tid = args[1]
        
        # å³ä½¿ä»»åŠ¡ä¸åœ¨ config ä¸­ï¼Œåªè¦æœ‰æ–‡ä»¶ä¹Ÿå¯ä»¥å–ï¼ˆé˜²æ„å¤–åˆ é™¤ï¼‰
        data_file = os.path.join(MONITOR_DATA_DIR, f"{tid}.txt")
        if not os.path.exists(data_file):
            if tid not in MONITOR_TASKS:
                update.message.reply_text("âŒ æ‰¾ä¸åˆ°è¯¥IDçš„ä»»åŠ¡è®°å½•æˆ–æ•°æ®æ–‡ä»¶ã€‚")
            else:
                update.message.reply_text("ğŸ¤·â€â™€ï¸ è¯¥ä»»åŠ¡æš‚æ— ä»»ä½•æ•°æ®æ²‰æ·€ã€‚")
            return
            
        task_info = MONITOR_TASKS.get(tid, {})
        q_info = task_info.get('query', 'æœªçŸ¥æŸ¥è¯¢')
        
        send_file_safely(context, update.effective_chat.id, data_file, caption=f"ğŸ“¦ ç›‘æ§æ•°æ®å¯¼å‡º\nID: `{tid}`\nQuery: `{escape_markdown_v2(q_info)}`", parse_mode=ParseMode.MARKDOWN_V2)
        upload_and_send_links(context, update.effective_chat.id, data_file)
        
    else:
        update.message.reply_text("âŒ æœªçŸ¥å‘½ä»¤ã€‚è¯·ä½¿ç”¨ `/monitor` æŸ¥çœ‹å¸®åŠ©ã€‚")

def run_monitor_execution_job(context: CallbackContext):
    """è‡ªé€‚åº”ç›‘æ§é›·è¾¾æ ¸å¿ƒé€»è¾‘"""
    job_context = context.job.context
    task_id = job_context.get('task_id')
    
    if task_id not in MONITOR_TASKS: return
    task = MONITOR_TASKS[task_id]
    
    query_text = task['query']
    os.makedirs(MONITOR_DATA_DIR, exist_ok=True)
    db_file = os.path.join(MONITOR_DATA_DIR, f"{task_id}.txt")
    
    # 1. è½½å…¥å¸ƒéš†è¿‡æ»¤å™¨ï¼ˆæˆ–ç®€æ˜“Setï¼‰
    known_hashes = set()
    if os.path.exists(db_file):
        try:
            with open(db_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line: known_hashes.add(hashlib.md5(line.encode()).hexdigest())
        except Exception as e:
            logger.error(f"Error reading monitor DB: {e}")

    # 2. æ‰§è¡Œæ£€æµ‹ (Radar Ping)
    # ç­–ç•¥ï¼šé»˜è®¤æŠ“ç¬¬ä¸€é¡µï¼ˆæœ€æ–°æ•°æ®ï¼‰ã€‚å¦‚æœæ˜¯é«˜é€Ÿæ¨¡å¼ï¼ŒæŠ“å‰3é¡µã€‚
    # æˆ‘ä»¬åªå…³å¿ƒæ˜¯å¦æœ‰ NEW æ•°æ®æ¥å†³å®šé¢‘ç‡ã€‚
    
    # è¿™é‡Œæˆ‘ä»¬åªå– fields="host,ip,port"ï¼Œä¸ºäº†çœæµé‡ä¸”é€šç”¨
    fetch_func = lambda k, kl, ps: fetch_fofa_data(k, query_text, page=1, page_size=100, fields="host", proxy_session=ps)
    data, _, _, _, _, error = execute_query_with_fallback(fetch_func)
    
    new_data_lines = []
    
    if not error and data and data.get('results'):
        results = data.get('results')
        for item in results:
            line_str = item[0] if isinstance(item, list) else str(item)
            h = hashlib.md5(line_str.strip().encode()).hexdigest()
            if h not in known_hashes:
                new_data_lines.append(line_str.strip())
                
    # 3. è‡ªé€‚åº”è°ƒé¢‘ç®—æ³• (Adaptive Frequency)
    # åŸºç¡€é¢‘ç‡
    base_interval = 3600 
    current_interval = task.get('interval', base_interval)
    
    if len(new_data_lines) > 0:
        # å‘½ä¸­æ–°ç›®æ ‡ï¼--> å†™å…¥åº“
        with open(db_file, 'a', encoding='utf-8') as f:
            f.write("\n".join(new_data_lines) + "\n")
            
        # æ¿€è¿›ç­–ç•¥ï¼šå¦‚æœæœ‰æ–°æ•°æ®ï¼Œç«‹åˆ»ç¼©çŸ­æ£€æŸ¥é—´éš”ï¼Œä»¥æ­¤è¿½è¸ªçˆ†å‘æœŸ
        # æœ€å° 10 åˆ†é’Ÿ (600s)
        new_interval = max(600, int(current_interval * 0.5))
    else:
        # æ— æ–°æ•°æ® --> é€æ­¥å†·å´ï¼ŒèŠ‚çœèµ„æº
        # æœ€å¤§ 12 å°æ—¶ (43200s)
        new_interval = min(43200, int(current_interval * 1.5))

    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
    task['last_run'] = int(time.time())
    task['interval'] = new_interval
    save_monitor_tasks()
    
    # 4. å®‰æ’ä¸‹ä¸€æ¬¡è¿è¡Œ (Add Jitter: +/- 10%)
    jitter = random.randint(int(-new_interval * 0.1), int(new_interval * 0.1))
    next_run_delay = new_interval + jitter
    
    context.job_queue.run_once(run_monitor_execution_job, next_run_delay, context={"task_id": task_id}, name=f"monitor_{task_id}")

# --- æ ¸å¿ƒå‘½ä»¤å¤„ç† ---
def start_command(update: Update, context: CallbackContext):
    user = update.effective_user
    welcome_text = f'ğŸ‘‹ æ¬¢è¿, {user.first_name}ï¼\nè¯·é€‰æ‹©ä¸€ä¸ªæ“ä½œ:'
    
    # v10.9.6 FIX: æ‰©å±•ä¸»èœå•å¹¶é‡åšå·¥ä½œæµç¨‹
    keyboard = [
        [KeyboardButton("å¸¸è§„æœç´¢"), KeyboardButton("æµ·é‡æœç´¢")],
        [KeyboardButton("ä¸»æœºè¯¦æŸ¥"), KeyboardButton("æ‰¹é‡å¯¼å‡º")],
        [KeyboardButton("è®¾ç½®"), KeyboardButton("å¸®åŠ©æ‰‹å†Œ")]
    ]
    reply_markup = ReplyKeyboardMarkup(keyboard, resize_keyboard=True)
    
    update.message.reply_text(welcome_text, reply_markup=reply_markup)

    if not CONFIG['admins']:
        first_admin_id = update.effective_user.id
        CONFIG.setdefault('admins', []).append(first_admin_id)
        save_config()
        update.message.reply_text(f"â„¹ï¸ å·²è‡ªåŠ¨å°†æ‚¨ (ID: `{first_admin_id}`) æ·»åŠ ä¸ºç¬¬ä¸€ä¸ªç®¡ç†å‘˜ã€‚")

def help_command(update: Update, context: CallbackContext):
    help_text = ( "ğŸ“– *Fofa æœºå™¨äººæŒ‡ä»¤æ‰‹å†Œ v10\\.9*\n\n"
                  "*ğŸ” èµ„äº§æœç´¢ \\(å¸¸è§„\\)*\n`/kkfofa [key] <query>`\n_FOFAæœç´¢, é€‚ç”¨äº1ä¸‡æ¡ä»¥å†…æ•°æ®_\n\n"
                  "*ğŸšš èµ„äº§æœç´¢ \\(æµ·é‡\\)*\n`/allfofa <query>`\n_ä½¿ç”¨nextæ¥å£ç¨³å®šè·å–æµ·é‡æ•°æ® \\(ç®¡ç†å‘˜\\)_\n\n"
                  "*ğŸ“¦ ä¸»æœºè¯¦æŸ¥ \\(æ™ºèƒ½\\)*\n`/host <ip|domain>`\n_è‡ªé€‚åº”è·å–æœ€å…¨ä¸»æœºä¿¡æ¯ \\(ç®¡ç†å‘˜\\)_\n\n"
                  "*ğŸ”¬ ä¸»æœºé€ŸæŸ¥ \\(èšåˆ\\)*\n`/lowhost <ip|domain> [detail]`\n_å¿«é€Ÿè·å–ä¸»æœºèšåˆä¿¡æ¯ \\(æ‰€æœ‰ç”¨æˆ·\\)_\n\n"
                  "*ğŸ“Š èšåˆç»Ÿè®¡*\n`/stats <query>`\n_è·å–å…¨å±€èšåˆç»Ÿè®¡ \\(ç®¡ç†å‘˜\\)_\n\n"
                  "*ğŸ“‚ æ‰¹é‡æ™ºèƒ½åˆ†æ*\n`/batchfind`\n_ä¸Šä¼ IPåˆ—è¡¨, åˆ†æç‰¹å¾å¹¶ç”ŸæˆExcel \\(ç®¡ç†å‘˜\\)_\n\n"
                  "*ğŸ“¤ æ‰¹é‡è‡ªå®šä¹‰å¯¼å‡º \\(äº¤äº’å¼\\)*\n`/batch <query>`\n_è¿›å…¥äº¤äº’å¼èœå•é€‰æ‹©å­—æ®µå¯¼å‡º \\(ç®¡ç†å‘˜\\)_\n\n"
                  "*âš™ï¸ ç®¡ç†ä¸è®¾ç½®*\n`/settings`\n_è¿›å…¥äº¤äº’å¼è®¾ç½®èœå• \\(ç®¡ç†å‘˜\\)_\n\n"
                  "*ğŸ”‘ Keyç®¡ç†*\n`/batchcheckapi`\n_ä¸Šä¼ æ–‡ä»¶æ‰¹é‡éªŒè¯API Key \\(ç®¡ç†å‘˜\\)_\n\n"
                  "*ğŸ’» ç³»ç»Ÿç®¡ç†*\n"
                  "`/check` \\- ç³»ç»Ÿè‡ªæ£€\n"
                  "`/update` \\- åœ¨çº¿æ›´æ–°è„šæœ¬\n"
                  "`/shutdown` \\- å®‰å…¨å…³é—­/é‡å¯\n\n"
                  "*ğŸ›‘ ä»»åŠ¡æ§åˆ¶*\n`/stop` \\- ç´§æ€¥åœæ­¢ä¸‹è½½ä»»åŠ¡\n`/cancel` \\- å–æ¶ˆå½“å‰æ“ä½œ" )
    update.message.reply_text(help_text, parse_mode=ParseMode.MARKDOWN_V2)
def cancel(update: Update, context: CallbackContext) -> int:
    message = "æ“ä½œå·²å–æ¶ˆã€‚"
    if update.message: update.message.reply_text(message)
    elif update.callback_query: update.callback_query.edit_message_text(message)
    context.user_data.clear()
    return ConversationHandler.END

# --- /kkfofa, /allfofa & è®¿å®¢é€»è¾‘ ---
def query_entry_point(update: Update, context: CallbackContext):
    user_id = update.effective_user.id
    query_obj = update.callback_query
    message_obj = update.message

    if query_obj:
        query_obj.answer()
        context.user_data['command'] = '/kkfofa'
        
        if not is_admin(user_id):
            guest_key = ANONYMOUS_KEYS.get(str(user_id))
            if not guest_key:
                query_obj.message.edit_text("ğŸ‘‹ æ¬¢è¿ï¼ä½œä¸ºé¦–æ¬¡ä½¿ç”¨çš„è®¿å®¢ï¼Œè¯·å…ˆå‘é€æ‚¨çš„FOFA API Keyã€‚")
                return ConversationHandler.END
            context.user_data['guest_key'] = guest_key

        try:
            preset_index = int(query_obj.data.replace("run_preset_", ""))
            preset = CONFIG["presets"][preset_index]
            context.user_data['original_query'] = preset['query']
            context.user_data['key_index'] = None
            keyboard = [[InlineKeyboardButton("ğŸŒ æ˜¯çš„, é™å®šå¤§æ´²", callback_data="continent_select"), InlineKeyboardButton("â© ä¸, ç›´æ¥æœç´¢", callback_data="continent_skip")]]
            query_obj.message.edit_text(f"é¢„è®¾æŸ¥è¯¢: `{escape_markdown_v2(preset['query'])}`\n\næ˜¯å¦è¦å°†æ­¤æŸ¥è¯¢é™å®šåœ¨ç‰¹å®šå¤§æ´²èŒƒå›´å†…ï¼Ÿ", reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
            return QUERY_STATE_ASK_CONTINENT
        except (ValueError, IndexError):
            query_obj.message.edit_text("âŒ é¢„è®¾æŸ¥è¯¢å¤±è´¥ã€‚")
            return ConversationHandler.END

    elif message_obj:
        command = message_obj.text.split()[0].lower()

        if command == '/allfofa' and not is_admin(user_id):
            message_obj.reply_text("â›”ï¸ æŠ±æ­‰ï¼Œ`/allfofa` å‘½ä»¤ä»…é™ç®¡ç†å‘˜ä½¿ç”¨ã€‚")
            return ConversationHandler.END

        if not is_admin(user_id):
            guest_key = ANONYMOUS_KEYS.get(str(user_id))
            if not guest_key:
                message_obj.reply_text("ğŸ‘‹ æ¬¢è¿ï¼ä½œä¸ºé¦–æ¬¡ä½¿ç”¨çš„è®¿å®¢ï¼Œè¯·è¾“å…¥æ‚¨çš„FOFA API Keyä»¥ç»§ç»­ã€‚æ‚¨çš„Keyåªä¼šè¢«æ‚¨è‡ªå·±ä½¿ç”¨ã€‚")
                if context.args:
                    context.user_data['pending_query'] = " ".join(context.args)
                return QUERY_STATE_GET_GUEST_KEY
            context.user_data['guest_key'] = guest_key

        if not context.args:
            if command == '/kkfofa':
                presets = CONFIG.get("presets", [])
                if not presets:
                    message_obj.reply_text(f"æ¬¢è¿ä½¿ç”¨FOFAæŸ¥è¯¢æœºå™¨äººã€‚\n\nâ¡ï¸ ç›´æ¥è¾“å…¥æŸ¥è¯¢è¯­æ³•: `/kkfofa domain=\"example.com\"`\nâ„¹ï¸ å½“å‰æ²¡æœ‰å¯ç”¨çš„é¢„è®¾æŸ¥è¯¢ã€‚ç®¡ç†å‘˜å¯é€šè¿‡ /settings æ·»åŠ ã€‚")
                    return ConversationHandler.END
                keyboard = []
                for i, p in enumerate(presets):
                    query_preview = p['query'][:25] + '...' if len(p['query']) > 25 else p['query']
                    keyboard.append([InlineKeyboardButton(f"{p['name']} (`{query_preview}`)", callback_data=f"run_preset_{i}")])
                message_obj.reply_text("ğŸ‘‡ è¯·é€‰æ‹©ä¸€ä¸ªé¢„è®¾æŸ¥è¯¢:", reply_markup=InlineKeyboardMarkup(keyboard))
            else:
                 message_obj.reply_text(f"ç”¨æ³•: `{command} <fofa_query>`")
            return ConversationHandler.END

        key_index, query_text = None, " ".join(context.args)
        if context.args[0].isdigit() and is_admin(user_id):
            try:
                num = int(context.args[0])
                if 1 <= num <= len(CONFIG['apis']):
                    key_index = num
                    query_text = " ".join(context.args[1:])
            except ValueError:
                pass
        
        context.user_data['original_query'] = query_text
        context.user_data['key_index'] = key_index
        context.user_data['command'] = command

        keyboard = [[InlineKeyboardButton("ğŸŒ æ˜¯çš„, é™å®šå¤§æ´²", callback_data="continent_select"), InlineKeyboardButton("â© ä¸, ç›´æ¥æœç´¢", callback_data="continent_skip")]]
        message_obj.reply_text(f"æŸ¥è¯¢: `{escape_markdown_v2(query_text)}`\n\næ˜¯å¦è¦å°†æ­¤æŸ¥è¯¢é™å®šåœ¨ç‰¹å®šå¤§æ´²èŒƒå›´å†…ï¼Ÿ", reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
        return QUERY_STATE_ASK_CONTINENT

    
    else:
        logger.error("query_entry_point called with an unsupported update type.")
        return ConversationHandler.END

def get_guest_key(update: Update, context: CallbackContext):
    user_id = update.effective_user.id
    guest_key = update.message.text.strip()
    msg = update.message.reply_text("â³ æ­£åœ¨éªŒè¯æ‚¨çš„API Key...")
    data, error = verify_fofa_api(guest_key)
    if error:
        msg.edit_text(f"âŒ KeyéªŒè¯å¤±è´¥: {error}\nè¯·é‡æ–°è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„Keyï¼Œæˆ–ä½¿ç”¨ /cancel å–æ¶ˆã€‚")
        return QUERY_STATE_GET_GUEST_KEY
    ANONYMOUS_KEYS[str(user_id)] = guest_key
    save_anonymous_keys()
    msg.edit_text(f"âœ… KeyéªŒè¯æˆåŠŸ ({data.get('username', 'N/A')})ï¼æ‚¨çš„Keyå·²ä¿å­˜ï¼Œç°åœ¨å¯ä»¥å¼€å§‹æŸ¥è¯¢äº†ã€‚")
    if 'pending_query' in context.user_data:
        context.args = context.user_data.pop('pending_query').split()
        return query_entry_point(update, context)
    return ConversationHandler.END

def ask_continent_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); choice = query.data.split('_')[1]
    command = context.user_data['command']

    if choice == 'skip':
        context.user_data['query'] = context.user_data['original_query']
        query.message.edit_text(f"å¥½çš„ï¼Œå°†ç›´æ¥æœç´¢: `{escape_markdown_v2(context.user_data['query'])}`", parse_mode=ParseMode.MARKDOWN_V2)
        if command == '/kkfofa':
            return proceed_with_kkfofa_query(update, context, message_to_edit=query.message)
        elif command == '/allfofa':
            return start_allfofa_search(update, context, message_to_edit=query.message)
    elif choice == 'select':
        keyboard = [
            [InlineKeyboardButton("ğŸŒ äºšæ´²", callback_data="continent_Asia"), InlineKeyboardButton("ğŸŒ æ¬§æ´²", callback_data="continent_Europe")],
            [InlineKeyboardButton("ğŸŒ åŒ—ç¾æ´²", callback_data="continent_NorthAmerica"), InlineKeyboardButton("ğŸŒ å—ç¾æ´²", callback_data="continent_SouthAmerica")],
            [InlineKeyboardButton("ğŸŒ éæ´²", callback_data="continent_Africa"), InlineKeyboardButton("ğŸŒ å¤§æ´‹æ´²", callback_data="continent_Oceania")],
            [InlineKeyboardButton("â†©ï¸ è·³è¿‡", callback_data="continent_skip")]]
        query.message.edit_text("è¯·é€‰æ‹©ä¸€ä¸ªå¤§æ´²:", reply_markup=InlineKeyboardMarkup(keyboard)); return QUERY_STATE_CONTINENT_CHOICE


def continent_choice_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); continent = query.data.split('_', 1)[1]; original_query = context.user_data['original_query']
    command = context.user_data['command']

    if continent == 'skip':
        context.user_data['query'] = original_query
        query.message.edit_text(f"å¥½çš„ï¼Œå°†ç›´æ¥æœç´¢: `{escape_markdown_v2(original_query)}`", parse_mode=ParseMode.MARKDOWN_V2)
    else:
        country_list = CONTINENT_COUNTRIES.get(continent)
        if not country_list: query.message.edit_text("âŒ é”™è¯¯ï¼šæ— æ•ˆçš„å¤§æ´²é€‰é¡¹ã€‚"); return ConversationHandler.END
        country_fofa_string = " || ".join([f'country="{code}"' for code in country_list]); final_query = f"({original_query}) && ({country_fofa_string})"
        context.user_data['query'] = final_query
        query.message.edit_text(f"æŸ¥è¯¢å·²æ„å»º:\n`{escape_markdown_v2(final_query)}`\n\næ­£åœ¨å¤„ç†\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)

    if command == '/kkfofa':
        return proceed_with_kkfofa_query(update, context, message_to_edit=query.message)
    elif command == '/allfofa':
        return start_allfofa_search(update, context, message_to_edit=query.message)

def proceed_with_kkfofa_query(update: Update, context: CallbackContext, message_to_edit):
    query_text = context.user_data['query']
    cached_item = find_cached_query(query_text)
    if cached_item:
        dt_utc = datetime.fromisoformat(cached_item['timestamp']); dt_local = dt_utc.astimezone(tz.tzlocal()); time_str = dt_local.strftime('%Y-%m-%d %H:%M')
        message_text = (f"âœ… *å‘ç°ç¼“å­˜*\n\næŸ¥è¯¢: `{escape_markdown_v2(query_text)}`\nç¼“å­˜äº: *{escape_markdown_v2(time_str)}*\n\n")
        keyboard = []; is_expired = (datetime.now(tz.tzutc()) - dt_utc).total_seconds() > CACHE_EXPIRATION_SECONDS
        if is_expired or not is_admin(update.effective_user.id):
             message_text += "âš ï¸ *æ­¤ç¼“å­˜å·²è¿‡æœŸæˆ–æ‚¨æ˜¯è®¿å®¢ï¼Œæ— æ³•å¢é‡æ›´æ–°\\.*" if is_expired else ""
             keyboard.append([InlineKeyboardButton("â¬‡ï¸ ä¸‹è½½æ—§ç¼“å­˜", callback_data='cache_download'), InlineKeyboardButton("ğŸ” å…¨æ–°æœç´¢", callback_data='cache_newsearch')])
        else: 
            message_text += "è¯·é€‰æ‹©æ“ä½œï¼š"; keyboard.append([InlineKeyboardButton("ğŸ”„ å¢é‡æ›´æ–°", callback_data='cache_incremental')]); keyboard.append([InlineKeyboardButton("â¬‡ï¸ ä¸‹è½½ç¼“å­˜", callback_data='cache_download'), InlineKeyboardButton("ğŸ” å…¨æ–°æœç´¢", callback_data='cache_newsearch')])
        keyboard.append([InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data='cache_cancel')])
        message_to_edit.edit_text(message_text, reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
        return QUERY_STATE_CACHE_CHOICE
    return start_new_kkfofa_search(update, context, message_to_edit=message_to_edit)

def cache_choice_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); choice = query.data.split('_')[1]
    if choice == 'download':
        cached_item = find_cached_query(context.user_data['query'])
        if cached_item:
            query.message.edit_text("â¬‡ï¸ æ­£åœ¨ä»æœ¬åœ°ç¼“å­˜å‘é€æ–‡ä»¶..."); file_path = cached_item['cache']['file_path']
            send_file_safely(context, update.effective_chat.id, file_path, filename=os.path.basename(file_path))
            upload_and_send_links(context, update.effective_chat.id, file_path)
            query.message.delete()
        else: query.message.edit_text("âŒ æ‰¾ä¸åˆ°æœ¬åœ°ç¼“å­˜è®°å½•ã€‚")
        return ConversationHandler.END
    elif choice == 'newsearch': return start_new_kkfofa_search(update, context, message_to_edit=query.message)
    elif choice == 'incremental': query.edit_message_text("â³ å‡†å¤‡å¢é‡æ›´æ–°..."); start_download_job(context, run_incremental_update_query, context.user_data); query.message.delete(); return ConversationHandler.END
    elif choice == 'cancel': query.message.edit_text("æ“ä½œå·²å–æ¶ˆã€‚"); return ConversationHandler.END

def start_new_kkfofa_search(update: Update, context: CallbackContext, message_to_edit=None):
    query_text = context.user_data['query']; key_index = context.user_data.get('key_index'); add_or_update_query(query_text)
    msg_text = f"ğŸ”„ æ­£åœ¨å¯¹ `{escape_markdown_v2(query_text)}` æ‰§è¡Œå…¨æ–°æŸ¥è¯¢\\.\\.\\."
    msg = message_to_edit if message_to_edit else update.effective_message.reply_text(msg_text, parse_mode=ParseMode.MARKDOWN_V2)
    if message_to_edit: msg.edit_text(msg_text, parse_mode=ParseMode.MARKDOWN_V2)
    
    guest_key = context.user_data.get('guest_key')
    if guest_key:
        data, error = fetch_fofa_data(guest_key, query_text, page_size=1, fields="host")
        used_key_info = "æ‚¨çš„Key"
    else:
        data, _, used_key_index, _, _, error = execute_query_with_fallback(
            lambda key, key_level, proxy_session: fetch_fofa_data(key, query_text, page_size=1, fields="host", proxy_session=proxy_session),
            preferred_key_index=key_index
        )
        used_key_info = f"Key \\[\\#{used_key_index}\\]"
    if error: msg.edit_text(f"âŒ æŸ¥è¯¢å‡ºé”™: {error}"); return ConversationHandler.END
    
    total_size = data.get('size', 0)
    if total_size == 0: msg.edit_text("ğŸ¤·â€â™€ï¸ æœªæ‰¾åˆ°ç»“æœã€‚"); return ConversationHandler.END
    context.user_data.update({'total_size': total_size, 'chat_id': update.effective_chat.id, 'is_batch_mode': False})
    
    success_message = f"âœ… ä½¿ç”¨ {used_key_info} æ‰¾åˆ° {total_size} æ¡ç»“æœ\\."
    
    if total_size <= 10000:
        msg.edit_text(f"{success_message}\nå¼€å§‹ä¸‹è½½\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
        start_download_job(context, run_full_download_query, context.user_data)
        return ConversationHandler.END
    else:
        keyboard = [
            [InlineKeyboardButton("ğŸ’ å…¨éƒ¨ä¸‹è½½ (å‰1ä¸‡)", callback_data='mode_full'), InlineKeyboardButton("ğŸŒ åˆ†ç‰‡ä¸‹è½½ (çªç ´ä¸Šé™)", callback_data='mode_sharding')],
            [InlineKeyboardButton("ğŸŒ€ æ·±åº¦è¿½æº¯ä¸‹è½½", callback_data='mode_traceback'), InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data='mode_cancel')]
        ]
        msg.edit_text(f"{success_message}\næ£€æµ‹åˆ°å¤§é‡ç»“æœ ({total_size}æ¡)ã€‚ç”±äºå•æ¬¡æŸ¥è¯¢ä¸Šé™ (10,000)ï¼Œæ‚¨å¯ä»¥ï¼š\n\n1ï¸âƒ£ **å‰1ä¸‡**ï¼šä»…ä¸‹è½½æœ€è¿‘çš„1ä¸‡æ¡ã€‚\n2ï¸âƒ£ **åˆ†ç‰‡ä¸‹è½½**ï¼šæŒ‰å›½å®¶è‡ªåŠ¨æ‹†åˆ†ï¼Œå°½å¯èƒ½é€šè¿‡ç§¯å°‘æˆå¤šçªç ´1ä¸‡æ¡é™åˆ¶ (æ¶ˆè€—æ›´å¤šè¯·æ±‚)ã€‚\n3ï¸âƒ£ **æ·±åº¦è¿½æº¯**ï¼šæŒ‰æ—¶é—´å›æº¯ (éœ€é«˜ç­‰çº§Key)ã€‚", reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
        return QUERY_STATE_KKFOFA_MODE 

def query_mode_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); mode = query.data.split('_')[1]
    if mode == 'cancel': query.message.edit_text("æ“ä½œå·²å–æ¶ˆ."); return ConversationHandler.END
    
    if mode == 'sharding':
        if context.user_data.get('is_batch_mode'):
             query.message.edit_text("âš ï¸ æŠ±æ­‰ï¼Œåˆ†ç‰‡ä¸‹è½½ç›®å‰ä»…æ”¯æŒåŸºç¡€ Host å¯¼å‡ºï¼Œä¸æ”¯æŒè‡ªå®šä¹‰æ‰¹é‡å­—æ®µã€‚")
             return ConversationHandler.END
        start_download_job(context, run_sharded_download_job, context.user_data)
        query.message.delete()
        return ConversationHandler.END

    if mode == 'traceback':
        keyboard = [[InlineKeyboardButton("â™¾ï¸ å…¨éƒ¨è·å–", callback_data='limit_none')], [InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data='limit_cancel')]]
        query.message.edit_text("è¯·è¾“å…¥æ·±åº¦è¿½æº¯è·å–çš„ç»“æœæ•°é‡ä¸Šé™ (ä¾‹å¦‚: 50000)ï¼Œæˆ–é€‰æ‹©å…¨éƒ¨è·å–ã€‚", reply_markup=InlineKeyboardMarkup(keyboard))
        return BATCH_STATE_GET_LIMIT if context.user_data.get('is_batch_mode') else QUERY_STATE_GET_TRACEBACK_LIMIT
    job_func = run_batch_download_query if context.user_data.get('is_batch_mode') else run_full_download_query
    if mode == 'full' and job_func:
        query.message.edit_text(f"â³ å¼€å§‹ä¸‹è½½..."); start_download_job(context, job_func, context.user_data); query.message.delete()
    return ConversationHandler.END

def get_traceback_limit(update: Update, context: CallbackContext):
    limit = None
    if update.callback_query:
        query = update.callback_query; query.answer()
        if query.data == 'limit_cancel': query.message.edit_text("æ“ä½œå·²å–æ¶ˆ."); return ConversationHandler.END
    elif update.message:
        try:
            limit = int(update.message.text.strip()); assert limit > 0
        except (ValueError, AssertionError):
            update.message.reply_text("âŒ æ— æ•ˆçš„æ•°å­—ï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ­£æ•´æ•°ã€‚")
            return BATCH_STATE_GET_LIMIT if context.user_data.get('is_batch_mode') else QUERY_STATE_GET_TRACEBACK_LIMIT
    context.user_data['limit'] = limit
    job_func = run_batch_traceback_query if context.user_data.get('is_batch_mode') else run_traceback_download_query
    msg_target = update.callback_query.message if update.callback_query else update.message
    msg_target.reply_text(f"â³ å¼€å§‹æ·±åº¦è¿½æº¯ (ä¸Šé™: {limit or 'æ— '})...")
    start_download_job(context, job_func, context.user_data)
    if update.callback_query: msg_target.delete()
    return ConversationHandler.END

# --- /host å’Œ /lowhost å‘½ä»¤ ---
def _create_dict_from_fofa_result(result_list, fields_list):
    return {fields_list[i]: result_list[i] for i in range(len(fields_list))}
def get_common_host_info(results, fields_list):
    if not results: return {}
    first_entry = _create_dict_from_fofa_result(results[0], fields_list)
    info = {
        "IP": first_entry.get('ip', 'N/A'),
        "åœ°ç†ä½ç½®": f"{first_entry.get('country_name', '')} {first_entry.get('region', '')} {first_entry.get('city', '')}".strip(),
        "ASN": f"{first_entry.get('asn', 'N/A')} ({first_entry.get('org', 'N/A')})",
        "æ“ä½œç³»ç»Ÿ": first_entry.get('os', 'N/A'),
    }
    port_index = fields_list.index('port') if 'port' in fields_list else -1
    if port_index != -1:
        all_ports = sorted(list(set(res[port_index] for res in results if len(res) > port_index)))
        info["å¼€æ”¾ç«¯å£"] = all_ports
    return info
def create_host_summary(host_arg, results, fields_list):
    info = get_common_host_info(results, fields_list)
    summary = [f"ğŸ“Œ *ä¸»æœºæ¦‚è§ˆ: `{escape_markdown_v2(host_arg)}`*"]
    for key, value in info.items():
        if value and value != 'N/A':
            if isinstance(value, list):
                summary.append(f"*{escape_markdown_v2(key)}:* `{escape_markdown_v2(', '.join(map(str, value)))}`")
            else:
                summary.append(f"*{escape_markdown_v2(key)}:* `{escape_markdown_v2(value)}`")
    summary.append("\nğŸ“„ *è¯¦ç»†æŠ¥å‘Šå·²ä½œä¸ºæ–‡ä»¶å‘é€\\.*")
    return "\n".join(summary)
def format_full_host_report(host_arg, results, fields_list):
    info = get_common_host_info(results, fields_list)
    report = [f"ğŸ“Œ *ä¸»æœºèšåˆæŠ¥å‘Š: `{escape_markdown_v2(host_arg)}`*\n"]
    for key, value in info.items():
        if value and value != 'N/A':
            if isinstance(value, list):
                report.append(f"*{escape_markdown_v2(key)}:* `{escape_markdown_v2(', '.join(map(str, value)))}`")
            else:
                report.append(f"*{escape_markdown_v2(key)}:* `{escape_markdown_v2(value)}`")
    report.append("\n\-\-\- *æœåŠ¡è¯¦æƒ…* \-\-\-\n")
    for res_list in results:
        d = _create_dict_from_fofa_result(res_list, fields_list)
        port_info = [f"ğŸŒ *Port `{d.get('port')}` \\({escape_markdown_v2(d.get('protocol', 'N/A'))}\\)*"]
        if d.get('title'): port_info.append(f"  - *æ ‡é¢˜:* `{escape_markdown_v2(d.get('title'))}`")
        if d.get('server'): port_info.append(f"  - *æœåŠ¡:* `{escape_markdown_v2(d.get('server'))}`")
        if d.get('icp'): port_info.append(f"  - *ICP:* `{escape_markdown_v2(d.get('icp'))}`")
        if d.get('jarm'): port_info.append(f"  - *JARM:* `{escape_markdown_v2(d.get('jarm'))}`")
        cert_str = d.get('cert', '{}')
        try:
            cert_info = json.loads(cert_str) if isinstance(cert_str, str) and cert_str.startswith('{') else {}
            if cert_info.get('issuer', {}).get('CN'): port_info.append(f"  - *è¯ä¹¦é¢å‘è€…:* `{escape_markdown_v2(cert_info['issuer']['CN'])}`")
            if cert_info.get('subject', {}).get('CN'): port_info.append(f"  - *è¯ä¹¦ä½¿ç”¨è€…:* `{escape_markdown_v2(cert_info['subject']['CN'])}`")
        except json.JSONDecodeError:
            pass
        if d.get('header'): port_info.append(f"  - *Header:* ```\n{d.get('header')}\n```")
        if d.get('banner'): port_info.append(f"  - *Banner:* ```\n{d.get('banner')}\n```")
        report.append("\n".join(port_info))
    return "\n".join(report)
def host_command_logic(update: Update, context: CallbackContext):
    if not context.args:
        update.message.reply_text(f"ç”¨æ³•: `/host <ip_or_domain>`\n\nç¤ºä¾‹:\n`/host 1\\.1\\.1\\.1`", parse_mode=ParseMode.MARKDOWN_V2)
        return
    host_arg = context.args[0]
    processing_message = update.message.reply_text(f"â³ æ­£åœ¨æŸ¥è¯¢ä¸»æœº `{escape_markdown_v2(host_arg)}`\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    query = f'ip="{host_arg}"' if re.match(r"^\d{1,3}(\.\d{1,3}){3}$", host_arg) else f'domain="{host_arg}"'
    data, final_fields_list, error = None, [], None
    for level in range(3, -1, -1): 
        fields_to_try = get_fields_by_level(level)
        fields_str = ",".join(fields_to_try)
        try:
            processing_message.edit_text(f"â³ æ­£åœ¨å°è¯•ä»¥ *ç­‰çº§ {level}* å­—æ®µæŸ¥è¯¢\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
        except (BadRequest, RetryAfter, TimedOut):
            time.sleep(1)
        temp_data, _, _, _, _, temp_error = execute_query_with_fallback(
            lambda key, key_level, proxy_session: fetch_fofa_data(key, query, page_size=100, fields=fields_str, proxy_session=proxy_session)
        )
        if not temp_error:
            data = temp_data
            final_fields_list = fields_to_try
            error = None
            break
        if "[820001]" not in str(temp_error):
            error = temp_error
            break
        else:
            error = temp_error
            continue
    if error:
        processing_message.edit_text(f"æŸ¥è¯¢å¤±è´¥ ğŸ˜\n*åŸå› :* `{escape_markdown_v2(error)}`", parse_mode=ParseMode.MARKDOWN_V2)
        return
    raw_results = data.get('results', [])
    if not raw_results:
        processing_message.edit_text(f"ğŸ¤·â€â™€ï¸ æœªæ‰¾åˆ°å…³äº `{escape_markdown_v2(host_arg)}` çš„ä»»ä½•ä¿¡æ¯\\.", parse_mode=ParseMode.MARKDOWN_V2)
        return
    
    unique_services = {}
    ip_idx = final_fields_list.index('ip') if 'ip' in final_fields_list else -1
    port_idx = final_fields_list.index('port') if 'port' in final_fields_list else -1
    protocol_idx = final_fields_list.index('protocol') if 'protocol' in final_fields_list else -1
    
    if port_idx != -1 and protocol_idx != -1:
        for res in raw_results:
            key = (res[ip_idx] if ip_idx != -1 else host_arg, res[port_idx], res[protocol_idx])
            if key not in unique_services:
                unique_services[key] = res
        results = list(unique_services.values())
    else:
        results = raw_results

    full_report = format_full_host_report(host_arg, results, final_fields_list)
    if len(full_report) > 3800:
        summary_report = create_host_summary(host_arg, results, final_fields_list)
        processing_message.edit_text(summary_report, parse_mode=ParseMode.MARKDOWN_V2, disable_web_page_preview=True)
        report_filename = f"host_details_{host_arg.replace('.', '_')}.txt"
        try:
            plain_text_report = re.sub(r'([*_`\[\]\\])', '', full_report)
            with open(report_filename, 'w', encoding='utf-8') as f: f.write(plain_text_report)
            send_file_safely(context, update.effective_chat.id, report_filename, caption="ğŸ“„ å®Œæ•´çš„è¯¦ç»†æŠ¥å‘Šå·²é™„ä¸Šã€‚")
            upload_and_send_links(context, update.effective_chat.id, report_filename)
        finally:
            if os.path.exists(report_filename): os.remove(report_filename)
    else:
        processing_message.edit_text(full_report, parse_mode=ParseMode.MARKDOWN_V2, disable_web_page_preview=True)
@admin_only
def host_command(update: Update, context: CallbackContext):
    host_command_logic(update, context)
def format_host_summary(data):
    parts = [f"ğŸ“Œ *ä¸»æœºèšåˆæ‘˜è¦: `{escape_markdown_v2(data.get('host', 'N/A'))}`*"]
    if data.get('ip'): parts.append(f"*IP:* `{escape_markdown_v2(data.get('ip'))}`")
    location = f"{data.get('country_name', '')} {data.get('region', '')} {data.get('city', '')}".strip()
    if location: parts.append(f"*ä½ç½®:* `{escape_markdown_v2(location)}`")
    if data.get('asn'): parts.append(f"*ASN:* `{data.get('asn')} \\({escape_markdown_v2(data.get('org', 'N/A'))}\\)`")
    
    if data.get('ports'):
        port_list = data.get('ports', [])
        if port_list and isinstance(port_list[0], dict):
            port_numbers = sorted([p.get('port') for p in port_list if p.get('port')])
            parts.append(f"*å¼€æ”¾ç«¯å£:* `{escape_markdown_v2(', '.join(map(str, port_numbers)))}`")
        else:
            parts.append(f"*å¼€æ”¾ç«¯å£:* `{escape_markdown_v2(', '.join(map(str, port_list)))}`")

    if data.get('protocols'): parts.append(f"*åè®®:* `{escape_markdown_v2(', '.join(data.get('protocols', [])))}`")
    if data.get('category'): parts.append(f"*èµ„äº§ç±»å‹:* `{escape_markdown_v2(', '.join(data.get('category', [])))}`")
    if data.get('products'):
        product_names = [p.get('name', 'N/A') for p in data.get('products', [])]
        parts.append(f"*äº§å“/ç»„ä»¶:* `{escape_markdown_v2(', '.join(product_names))}`")
    return "\n".join(parts)
def format_host_details(data):
    summary = format_host_summary(data)
    details = ["\n\-\-\- *ç«¯å£è¯¦æƒ…* \-\-\-"]
    for port_info in data.get('port_details', []):
        port_str = f"\nğŸŒ *Port `{port_info.get('port')}` \\({escape_markdown_v2(port_info.get('protocol', 'N/A'))}\\)*"
        if port_info.get('product'): port_str += f"\n  - *äº§å“:* `{escape_markdown_v2(port_info.get('product'))}`"
        if port_info.get('title'): port_str += f"\n  - *æ ‡é¢˜:* `{escape_markdown_v2(port_info.get('title'))}`"
        if port_info.get('jarm'): port_str += f"\n  - *JARM:* `{escape_markdown_v2(port_info.get('jarm'))}`"
        if port_info.get('banner'): port_str += f"\n  - *Banner:* ```\n{port_info.get('banner')}\n```"
        details.append(port_str)
    full_report = summary + "\n".join(details)
    return full_report
def lowhost_command(update: Update, context: CallbackContext) -> None:
    if not context.args:
        update.message.reply_text("ç”¨æ³•: `/lowhost <ip_or_domain> [detail]`\n\nç¤ºä¾‹:\n`/lowhost 1\\.1\\.1\\.1`\n`/lowhost example\\.com detail`", parse_mode=ParseMode.MARKDOWN_V2)
        return
    host = context.args[0]
    detail = len(context.args) > 1 and context.args[1].lower() == 'detail'
    processing_message = update.message.reply_text(f"æ­£åœ¨æŸ¥è¯¢ä¸»æœº `{escape_markdown_v2(host)}` çš„èšåˆä¿¡æ¯\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    data, _, _, _, _, error = execute_query_with_fallback(
        lambda key, key_level, proxy_session: fetch_fofa_host_info(key, host, detail, proxy_session=proxy_session)
    )
    if error:
        processing_message.edit_text(f"æŸ¥è¯¢å¤±è´¥ ğŸ˜\n*åŸå› :* `{escape_markdown_v2(error)}`", parse_mode=ParseMode.MARKDOWN_V2)
        return
    if not data:
        processing_message.edit_text(f"ğŸ¤·â€â™€ï¸ æœªæ‰¾åˆ°å…³äº `{escape_markdown_v2(host)}` çš„ä»»ä½•ä¿¡æ¯\\.", parse_mode=ParseMode.MARKDOWN_V2)
        return
    if detail:
        formatted_text = format_host_details(data)
    else:
        formatted_text = format_host_summary(data)
    if len(formatted_text) > 3800:
        processing_message.edit_text("æŠ¥å‘Šè¿‡é•¿ï¼Œå°†ä½œä¸ºæ–‡ä»¶å‘é€ã€‚")
        report_filename = f"lowhost_details_{host.replace('.', '_')}.txt"
        try:
            plain_text_report = re.sub(r'([*_`\[\]\\])', '', formatted_text)
            with open(report_filename, 'w', encoding='utf-8') as f: f.write(plain_text_report)
            send_file_safely(context, update.effective_chat.id, report_filename, caption="ğŸ“„ å®Œæ•´çš„èšåˆæŠ¥å‘Šå·²é™„ä¸Šã€‚")
            upload_and_send_links(context, update.effective_chat.id, report_filename)
        finally:
            if os.path.exists(report_filename): os.remove(report_filename)
    else:
        processing_message.edit_text(formatted_text, parse_mode=ParseMode.MARKDOWN_V2)

# --- /stats å‘½ä»¤ ---
@admin_only
def stats_command(update: Update, context: CallbackContext):
    if not context.args:
        update.message.reply_text("è¯·è¾“å…¥è¦è¿›è¡Œèšåˆç»Ÿè®¡çš„FOFAæŸ¥è¯¢è¯­æ³•:")
        return STATS_STATE_GET_QUERY
    return get_fofa_stats_query(update, context)
def get_fofa_stats_query(update: Update, context: CallbackContext):
    query_text = " ".join(context.args) if context.args else update.message.text
    msg = update.message.reply_text(f"â³ æ­£åœ¨å¯¹ `{escape_markdown_v2(query_text)}` è¿›è¡Œèšåˆç»Ÿè®¡\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    
    data, _, _, _, _, error = execute_query_with_fallback(
        lambda key, key_level, proxy_session: fetch_fofa_stats(key, query_text, proxy_session=proxy_session)
    )
    
    if error:
        msg.edit_text(f"âŒ ç»Ÿè®¡å¤±è´¥: {escape_markdown_v2(error)}", parse_mode=ParseMode.MARKDOWN_V2)
        return ConversationHandler.END

    # æ™ºèƒ½é€‚é…å±‚ï¼šå¤„ç†åµŒå¥—å’Œæ‰å¹³ä¸¤ç§APIå“åº”æ ¼å¼
    stats_source = data.get("aggs", data)

    report = [f"ğŸ“Š *èšåˆç»Ÿè®¡æŠ¥å‘Š for `{escape_markdown_v2(query_text)}`*\n"]
    
    # å®Œæ•´ç‰ˆ display_mapï¼ŒåŒ…å«å…¨éƒ¨12ä¸ªå¯èšåˆå­—æ®µ
    display_map = {
        "countries": "ğŸŒ Top 5 å›½å®¶/åœ°åŒº",
        "org": "ğŸ¢ Top 5 ç»„ç»‡ (ORG)",
        "asn": "ğŸ“› Top 5 ASN",
        "server": "ğŸ–¥ï¸ Top 5 æœåŠ¡/ç»„ä»¶",
        "protocol": "ğŸ”Œ Top 5 åè®®",
        "port": "ğŸšª Top 5 ç«¯å£",
        "icp": "ğŸ“œ Top 5 ICPå¤‡æ¡ˆ",
        "title": "ğŸ“° Top 5 ç½‘ç«™æ ‡é¢˜",
        "fid": "ğŸ”‘ Top 5 FID æŒ‡çº¹",
        "domain": "ğŸŒ Top 5 åŸŸå",          # <-- æ–°å¢
        "os": "ğŸ’» Top 5 æ“ä½œç³»ç»Ÿ",        # <-- æ–°å¢
        "asset_type": "ğŸ“¦ Top 5 èµ„äº§ç±»å‹" # <-- æ–°å¢
    }
    
    data_found = False
    for key, title in display_map.items():
        items = stats_source.get(key)
        
        if items and isinstance(items, list):
            data_found = True
            report.append(f"*{escape_markdown_v2(title)}*:")
            for item in items[:5]:
                name = escape_markdown_v2(item.get('name', 'N/A'))
                count = item.get('count', 0)
                report.append(f"  `{name}`: *{count:,}*")
            report.append("")

    if not data_found:
        report.append("_æœªæ‰¾åˆ°å¯ä¾›èšåˆçš„æ•°æ®ã€‚_")

    try:
        msg.edit_text("\n".join(report), parse_mode=ParseMode.MARKDOWN_V2)
    except BadRequest as e:
        if "message is too long" in str(e).lower():
            msg.edit_text("âœ… ç»Ÿè®¡å®Œæˆï¼æŠ¥å‘Šè¿‡é•¿ï¼Œå°†ä½œä¸ºæ–‡ä»¶å‘é€ã€‚")
            report_filename = f"stats_report_{int(time.time())}.txt"
            plain_text_report = re.sub(r'([*_`\[\]\\])', '', "\n".join(report))
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write(plain_text_report)
            send_file_safely(context, update.effective_chat.id, report_filename)
            os.remove(report_filename)
        else:
            msg.edit_text(f"âŒ å‘é€æŠ¥å‘Šæ—¶å‡ºé”™: {escape_markdown_v2(str(e))}", parse_mode=ParseMode.MARKDOWN_V2)

    return ConversationHandler.END

def inline_fofa_handler(update: Update, context: CallbackContext) -> None:
    """å¤„ç†å†…è”æŸ¥è¯¢è¯·æ±‚"""
    query_text = update.inline_query.query
    results = []

    try:
        # å¦‚æœç”¨æˆ·åªè¾“å…¥äº†@botnameï¼Œæ²¡æœ‰é™„å¸¦æŸ¥è¯¢è¯­å¥
        if not query_text:
            results.append(
                InlineQueryResultArticle(
                    id=str(uuid.uuid4()),
                    title="å¼€å§‹è¾“å…¥FOFAæŸ¥è¯¢è¯­æ³•...",
                    description='ä¾‹å¦‚: domain="example.com"',
                    input_message_content=InputTextMessageContent(
                        "ğŸ’¡ **FOFA å†…è”æŸ¥è¯¢ç”¨æ³•** ğŸ’¡\n\n"
                        "åœ¨ä»»ä½•èŠå¤©æ¡†ä¸­è¾“å…¥ `@ä½ çš„æœºå™¨äººç”¨æˆ·å` ç„¶åè·Ÿä¸ŠFOFAæŸ¥è¯¢è¯­æ³•ï¼Œå³å¯å¿«é€Ÿæœç´¢ã€‚\n\n"
                        "ä¾‹å¦‚ï¼š`@ä½ çš„æœºå™¨äººç”¨æˆ·å domain=\"qq.com\"`",
                        parse_mode=ParseMode.MARKDOWN
                    )
                )
            )
            update.inline_query.answer(results, cache_time=300) # åˆå§‹æ¶ˆæ¯å¯ä»¥ç¼“å­˜ä¹…ä¸€ç‚¹
            return

        # --- ç”¨æˆ·è¾“å…¥äº†æŸ¥è¯¢è¯­å¥ï¼Œå¼€å§‹è°ƒç”¨FOFA API ---
        def inline_query_logic(key, key_level, proxy_session):
            return fetch_fofa_data(key, query_text, page_size=10, fields="host,title", proxy_session=proxy_session)

        data, _, _, _, _, error = execute_query_with_fallback(inline_query_logic)

        # å¦‚æœæŸ¥è¯¢å‡ºé”™
        if error:
            results.append(
                InlineQueryResultArticle(
                    id='error',
                    title="æŸ¥è¯¢å‡ºé”™",
                    description=str(error),
                    input_message_content=InputTextMessageContent(f"FOFA æŸ¥è¯¢å¤±è´¥: {error}")
                )
            )
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœ
        elif not data or not data.get('results'):
            results.append(
                InlineQueryResultArticle(
                    id='no_results',
                    title="æœªæ‰¾åˆ°ç»“æœ",
                    description=f"æŸ¥è¯¢: {query_text}",
                    input_message_content=InputTextMessageContent(f"å¯¹äºæŸ¥è¯¢ '{query_text}'ï¼ŒFOFA æœªè¿”å›ä»»ä½•ç»“æœã€‚")
                )
            )
        # æˆåŠŸæ‰¾åˆ°ç»“æœ
        else:
            for result in data['results']:
                host = result[0] if result and len(result) > 0 else "N/A"
                title = result[1] if result and len(result) > 1 else "æ— æ ‡é¢˜"
                
                results.append(
                    InlineQueryResultArticle(
                        id=str(uuid.uuid4()),
                        title=host,
                        description=title,
                        input_message_content=InputTextMessageContent(host)
                    )
                )
    
    except Exception as e:
        # æ•è·ä»»ä½•æ„å¤–çš„å´©æºƒï¼Œå¹¶è¿”å›é”™è¯¯ä¿¡æ¯
        logger.error(f"å†…è”æŸ¥è¯¢æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        results = [
            InlineQueryResultArticle(
                id='critical_error',
                title="æœºå™¨äººå†…éƒ¨é”™è¯¯",
                description="å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚",
                input_message_content=InputTextMessageContent("æœºå™¨äººå†…éƒ¨é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")
            )
        ]
    
    # ç¡®ä¿æ€»èƒ½å“åº”Telegramï¼Œé¿å…ç•Œé¢å¡ä½
    update.inline_query.answer(results, cache_time=10) # å®é™…æŸ¥è¯¢ç»“æœç¼“å­˜æ—¶é—´çŸ­ä¸€ç‚¹


# --- /batchfind å‘½ä»¤ ---
BATCH_FEATURES = { "protocol": "åè®®", "domain": "åŸŸå", "os": "æ“ä½œç³»ç»Ÿ", "server": "æœåŠ¡/ç»„ä»¶", "icp": "ICPå¤‡æ¡ˆå·", "title": "æ ‡é¢˜", "jarm": "JARMæŒ‡çº¹", "cert.issuer.org": "è¯ä¹¦é¢å‘ç»„ç»‡", "cert.issuer.cn": "è¯ä¹¦é¢å‘CN", "cert.subject.org": "è¯ä¹¦ä¸»ä½“ç»„ç»‡", "cert.subject.cn": "è¯ä¹¦ä¸»ä½“CN" }
@admin_only
def batchfind_command(update: Update, context: CallbackContext):
    update.message.reply_text("è¯·ä¸Šä¼ ä¸€ä¸ªåŒ…å« IP:Port åˆ—è¡¨çš„ .txt æ–‡ä»¶ã€‚")
    return BATCHFIND_STATE_GET_FILE
def get_batch_file_handler(update: Update, context: CallbackContext):
    doc = update.message.document
    file = doc.get_file()
    file_path = os.path.join(FOFA_CACHE_DIR, doc.file_name)
    file.download(custom_path=file_path)
    context.user_data['batch_file_path'] = file_path
    context.user_data['selected_features'] = set()
    keyboard = []
    features_list = list(BATCH_FEATURES.items())
    for i in range(0, len(features_list), 2):
        row = [InlineKeyboardButton(f"â˜ {features_list[i][1]}", callback_data=f"batchfeature_{features_list[i][0]}")]
        if i + 1 < len(features_list):
            row.append(InlineKeyboardButton(f"â˜ {features_list[i+1][1]}", callback_data=f"batchfeature_{features_list[i+1][0]}"))
        keyboard.append(row)
    keyboard.append([InlineKeyboardButton("âœ… å…¨éƒ¨é€‰æ‹©", callback_data="batchfeature_all"), InlineKeyboardButton("â¡ï¸ å¼€å§‹åˆ†æ", callback_data="batchfeature_done")])
    update.message.reply_text("è¯·é€‰æ‹©æ‚¨éœ€è¦åˆ†æçš„ç‰¹å¾:", reply_markup=InlineKeyboardMarkup(keyboard))
    return BATCHFIND_STATE_SELECT_FEATURES
def select_batch_features_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); feature = query.data.split('_', 1)[1]
    selected = context.user_data['selected_features']
    if feature == 'done':
        if not selected: query.answer("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾ï¼", show_alert=True); return BATCHFIND_STATE_SELECT_FEATURES
        query.message.edit_text("âœ… ç‰¹å¾é€‰æ‹©å®Œæ¯•ï¼Œä»»åŠ¡å·²æäº¤åˆ°åå°åˆ†æã€‚")
        job_context = {'chat_id': query.message.chat_id, 'file_path': context.user_data['batch_file_path'], 'features': list(selected)}
        context.job_queue.run_once(run_batch_find_job, 1, context=job_context, name=f"batchfind_{query.message.chat_id}")
        return ConversationHandler.END
    if feature == 'all':
        if len(selected) == len(BATCH_FEATURES): selected.clear()
        else: selected.update(BATCH_FEATURES.keys())
    elif feature in selected: selected.remove(feature)
    else: selected.add(feature)
    keyboard = []
    features_list = list(BATCH_FEATURES.items())
    for i in range(0, len(features_list), 2):
        row = []
        key1 = features_list[i][0]; row.append(InlineKeyboardButton(f"{'â˜‘' if key1 in selected else 'â˜'} {features_list[i][1]}", callback_data=f"batchfeature_{key1}"))
        if i + 1 < len(features_list):
            key2 = features_list[i+1][0]; row.append(InlineKeyboardButton(f"{'â˜‘' if key2 in selected else 'â˜'} {features_list[i+1][1]}", callback_data=f"batchfeature_{key2}"))
        keyboard.append(row)
    all_text = "âœ… å–æ¶ˆå…¨é€‰" if len(selected) == len(BATCH_FEATURES) else "âœ… å…¨éƒ¨é€‰æ‹©"
    keyboard.append([InlineKeyboardButton(all_text, callback_data="batchfeature_all"), InlineKeyboardButton("â¡ï¸ å¼€å§‹åˆ†æ", callback_data="batchfeature_done")])
    query.message.edit_reply_markup(reply_markup=InlineKeyboardMarkup(keyboard))
    return BATCHFIND_STATE_SELECT_FEATURES
def run_batch_find_job(context: CallbackContext):
    job_data = context.job.context; chat_id, file_path, features = job_data['chat_id'], job_data['file_path'], job_data['features']
    bot = context.bot; msg = bot.send_message(chat_id, "â³ å¼€å§‹æ‰¹é‡åˆ†æä»»åŠ¡...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f: targets = [line.strip() for line in f if line.strip()]
    except Exception as e: msg.edit_text(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}"); return
    if not targets: msg.edit_text("âŒ æ–‡ä»¶ä¸ºç©ºã€‚"); return
    total_targets = len(targets); processed_count = 0; detailed_results_for_excel = []
    for target in targets:
        processed_count += 1
        if processed_count % 10 == 0:
            try: msg.edit_text(f"åˆ†æè¿›åº¦: {create_progress_bar(processed_count/total_targets*100)} ({processed_count}/{total_targets})")
            except (BadRequest, RetryAfter, TimedOut): pass
        query = f'ip="{target}"' if ':' not in target else f'host="{target}"'
        data, _, _, _, _, error = execute_query_with_fallback(
            lambda key, key_level, proxy_session: fetch_fofa_data(key, query, page_size=1, fields=",".join(features), proxy_session=proxy_session)
        )
        if not error and data.get('results'):
            result = data['results'][0]
            row_data = {'Target': target}
            row_data.update({BATCH_FEATURES.get(f, f): result[i] for i, f in enumerate(features)})
            detailed_results_for_excel.append(row_data)
    if detailed_results_for_excel:
        try:
            df = pd.DataFrame(detailed_results_for_excel)
            excel_filename = generate_filename_from_query(os.path.basename(file_path), prefix="analysis", ext=".xlsx")
            df.to_excel(excel_filename, index=False, engine='openpyxl')
            msg.edit_text("âœ… åˆ†æå®Œæˆï¼æ­£åœ¨å‘é€ExcelæŠ¥å‘Š...")
            send_file_safely(context, chat_id, excel_filename, caption="ğŸ“„ è¯¦ç»†ç‰¹å¾åˆ†æExcelæŠ¥å‘Š")
            upload_and_send_links(context, chat_id, excel_filename)
            os.remove(excel_filename)
        except Exception as e: msg.edit_text(f"âŒ ç”ŸæˆExcelå¤±è´¥: {e}")
    else: msg.edit_text("ğŸ¤·â€â™€ï¸ åˆ†æå®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„FOFAæ•°æ®ã€‚")
    if os.path.exists(file_path): os.remove(file_path)

# --- /batch (äº¤äº’å¼) ---
def build_batch_fields_keyboard(user_data):
    selected_fields = user_data.get('selected_fields', set())
    page = user_data.get('page', 0)
    flat_fields = []
    for category, fields in FIELD_CATEGORIES.items():
        for field in fields:
            flat_fields.append((field, category))
    items_per_page = 12
    start_index = page * items_per_page
    end_index = start_index + items_per_page
    page_items = flat_fields[start_index:end_index]
    keyboard = []
    for i in range(0, len(page_items), 2):
        row = []
        field1, cat1 = page_items[i]
        prefix1 = "â˜‘ï¸" if field1 in selected_fields else "â˜"
        row.append(InlineKeyboardButton(f"{prefix1} {field1}", callback_data=f"batchfield_toggle_{field1}"))
        if i + 1 < len(page_items):
            field2, cat2 = page_items[i+1]
            prefix2 = "â˜‘ï¸" if field2 in selected_fields else "â˜"
            row.append(InlineKeyboardButton(f"{prefix2} {field2}", callback_data=f"batchfield_toggle_{field2}"))
        keyboard.append(row)
    nav_row = []
    if page > 0:
        nav_row.append(InlineKeyboardButton("â¬…ï¸ ä¸Šä¸€é¡µ", callback_data="batchfield_prev"))
    if end_index < len(flat_fields):
        nav_row.append(InlineKeyboardButton("ä¸‹ä¸€é¡µ â¡ï¸", callback_data="batchfield_next"))
    keyboard.append(nav_row)
    keyboard.append([InlineKeyboardButton("âœ… å®Œæˆé€‰æ‹©å¹¶å¼€å§‹", callback_data="batchfield_done")])
    return InlineKeyboardMarkup(keyboard)
@admin_only
def batch_command(update: Update, context: CallbackContext):
    if not context.args:
        update.message.reply_text("ç”¨æ³•: `/batch <fofa_query>`")
        return ConversationHandler.END
    query_text = " ".join(context.args)
    context.user_data['query'] = query_text
    context.user_data['selected_fields'] = set(FREE_FIELDS[:5])
    context.user_data['page'] = 0
    keyboard = build_batch_fields_keyboard(context.user_data)
    update.message.reply_text(f"æŸ¥è¯¢: `{escape_markdown_v2(query_text)}`\nè¯·é€‰æ‹©è¦å¯¼å‡ºçš„å­—æ®µ:", reply_markup=keyboard, parse_mode=ParseMode.MARKDOWN_V2)
    return BATCH_STATE_SELECT_FIELDS
def batch_select_fields_callback(update: Update, context: CallbackContext):
    query = update.callback_query
    query.answer()
    action = query.data.split('_', 1)[1]
    if action == "next":
        context.user_data['page'] += 1
    elif action == "prev":
        context.user_data['page'] -= 1
    elif action.startswith("toggle_"):
        field = action.replace("toggle_", "")
        if field in context.user_data['selected_fields']:
            context.user_data['selected_fields'].remove(field)
        else:
            context.user_data['selected_fields'].add(field)
    elif action == "done":
        selected_fields = context.user_data.get('selected_fields')
        if not selected_fields:
            query.answer("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå­—æ®µï¼", show_alert=True)
            return BATCH_STATE_SELECT_FIELDS
        query_text = context.user_data['query']
        fields_str = ",".join(list(selected_fields))
        msg = query.message.edit_text("æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢ä»¥é¢„ä¼°æ•°æ®é‡...")
        data, _, used_key_index, key_level, _, error = execute_query_with_fallback(
            lambda key, key_level, proxy_session: fetch_fofa_data(key, query_text, page_size=1, fields="host", proxy_session=proxy_session)
        )
        if error: msg.edit_text(f"âŒ æŸ¥è¯¢å‡ºé”™: {error}"); return ConversationHandler.END
        total_size = data.get('size', 0)
        if total_size == 0: msg.edit_text("ğŸ¤·â€â™€ï¸ æœªæ‰¾åˆ°ç»“æœã€‚"); return ConversationHandler.END
        allowed_fields = get_fields_by_level(key_level)
        unauthorized_fields = [f for f in selected_fields if f not in allowed_fields]
        if unauthorized_fields:
            msg.edit_text(f"âš ï¸ è­¦å‘Š: æ‚¨é€‰æ‹©çš„å­—æ®µ `{', '.join(unauthorized_fields)}` è¶…å‡ºå½“å‰å¯ç”¨æœ€é«˜çº§Key (ç­‰çº§{key_level}) çš„æƒé™ã€‚è¯·é‡æ–°é€‰æ‹©æˆ–å‡çº§Keyã€‚")
            return BATCH_STATE_SELECT_FIELDS
        context.user_data.update({'chat_id': update.effective_chat.id, 'fields': fields_str, 'total_size': total_size, 'is_batch_mode': True })
        success_message = f"âœ… ä½¿ç”¨ Key \\[\\#{used_key_index}\\] \\(ç­‰çº§{key_level}\\) æ‰¾åˆ° {total_size} æ¡ç»“æœ\\."
        if total_size <= 10000:
            msg.edit_text(f"{success_message}\nå¼€å§‹è‡ªå®šä¹‰å­—æ®µæ‰¹é‡å¯¼å‡º\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2); start_download_job(context, run_batch_download_query, context.user_data)
            return ConversationHandler.END
        else:
            keyboard = [[InlineKeyboardButton("ğŸ’ å¯¼å‡ºå‰1ä¸‡æ¡", callback_data='mode_full'), InlineKeyboardButton("ğŸŒ€ æ·±åº¦è¿½æº¯å¯¼å‡º", callback_data='mode_traceback')], [InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data='mode_cancel')]]
            msg.edit_text(f"{success_message}\nè¯·é€‰æ‹©å¯¼å‡ºæ¨¡å¼:", reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2); return BATCH_STATE_MODE_CHOICE
    keyboard = build_batch_fields_keyboard(context.user_data)
    query.message.edit_reply_markup(reply_markup=keyboard)
    return BATCH_STATE_SELECT_FIELDS

# --- /batchcheckapi å‘½ä»¤ ---
@admin_only
def batch_check_api_command(update: Update, context: CallbackContext) -> int:
    update.message.reply_text("è¯·ä¸Šä¼ ä¸€ä¸ªåŒ…å« API Keys çš„ .txt æ–‡ä»¶ (æ¯è¡Œä¸€ä¸ª Key)ã€‚")
    return BATCHCHECKAPI_STATE_GET_FILE
def receive_api_file(update: Update, context: CallbackContext) -> int:
    doc = update.message.document
    if not doc.file_name.endswith('.txt'):
        update.message.reply_text("âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œè¯·ä¸Šä¼  .txt æ–‡ä»¶ã€‚")
        return ConversationHandler.END
    file = doc.get_file()
    temp_path = os.path.join(FOFA_CACHE_DIR, f"api_check_{doc.file_id}.txt")
    file.download(custom_path=temp_path)
    try:
        with open(temp_path, 'r', encoding='utf-8') as f:
            keys_to_check = [line.strip() for line in f if line.strip()]
    except Exception as e:
        update.message.reply_text(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        if os.path.exists(temp_path): os.remove(temp_path)
        return ConversationHandler.END
    if not keys_to_check:
        update.message.reply_text("ğŸ¤·â€â™€ï¸ æ–‡ä»¶ä¸ºç©ºæˆ–ä¸åŒ…å«ä»»ä½•æœ‰æ•ˆçš„ Keyã€‚")
        if os.path.exists(temp_path): os.remove(temp_path)
        return ConversationHandler.END
    msg = update.message.reply_text(f"â³ å¼€å§‹æ‰¹é‡éªŒè¯ {len(keys_to_check)} ä¸ª API Key...")
    valid_keys, invalid_keys = [], []
    total = len(keys_to_check)
    for i, key in enumerate(keys_to_check):
        data, error = verify_fofa_api(key)
        if not error:
            is_vip = data.get('isvip', False)
            api_level = data.get('vip_level', 0)
            level = 0
            if is_vip:
                if api_level == 2: level = 1
                elif api_level == 3: level = 2
                elif api_level >= 4: level = 3
                else: level = 1
            level_name = {0: "å…è´¹", 1: "ä¸ªäºº", 2: "å•†ä¸š", 3: "ä¼ä¸š"}.get(level, "æœªçŸ¥")
            valid_keys.append(f"`...{key[-4:]}` \\- âœ… *æœ‰æ•ˆ* \\({escape_markdown_v2(data.get('username', 'N/A'))}, {level_name}ä¼šå‘˜\\)")
        else:
            invalid_keys.append(f"`...{key[-4:]}` \\- âŒ *æ— æ•ˆ* \\(åŸå› : {escape_markdown_v2(error)}\\)")
        if (i + 1) % 10 == 0 or (i + 1) == total:
            try:
                progress_text = f"â³ éªŒè¯è¿›åº¦: {create_progress_bar((i+1)/total*100)} ({i+1}/{total})"
                msg.edit_text(progress_text)
            except (BadRequest, RetryAfter, TimedOut):
                time.sleep(2)
    
    report = [f"ğŸ“‹ *æ‰¹é‡API KeyéªŒè¯æŠ¥å‘Š*"]
    report.append(f"\næ€»è®¡: {total} \\| æœ‰æ•ˆ: {len(valid_keys)} \\| æ— æ•ˆ: {len(invalid_keys)}\n")
    if valid_keys:
        report.append("\-\-\- *æœ‰æ•ˆ Keys* \-\-\-")
        report.extend(valid_keys)
    if invalid_keys:
        report.append("\n\-\-\- *æ— æ•ˆ Keys* \-\-\-")
        report.extend(invalid_keys)
    
    report_text = "\n".join(report)
    if len(report_text) > 3800:
        summary = f"âœ… éªŒè¯å®Œæˆï¼\næ€»è®¡: {total} \\| æœ‰æ•ˆ: {len(valid_keys)} \\| æ— æ•ˆ: {len(invalid_keys)}\n\næŠ¥å‘Šè¿‡é•¿ï¼Œå·²ä½œä¸ºæ–‡ä»¶å‘é€\\."
        msg.edit_text(summary)
        report_filename = f"api_check_report_{int(time.time())}.txt"
        try:
            plain_text_report = re.sub(r'([*_`\[\]\\])', '', report_text)
            with open(report_filename, 'w', encoding='utf-8') as f: f.write(plain_text_report)
            send_file_safely(context, update.effective_chat.id, report_filename)
        finally:
            if os.path.exists(report_filename): os.remove(report_filename)
    else:
        msg.edit_text(report_text, parse_mode=ParseMode.MARKDOWN_V2)

    if os.path.exists(temp_path): os.remove(temp_path)
    return ConversationHandler.END

# --- å…¶ä»–ç®¡ç†å‘½ä»¤ ---
@admin_only
def check_command(update: Update, context: CallbackContext):
    msg = update.message.reply_text("â³ æ­£åœ¨æ‰§è¡Œç³»ç»Ÿè‡ªæ£€...")
    report = ["*ğŸ“‹ ç³»ç»Ÿè‡ªæ£€æŠ¥å‘Š*"]
    try:
        global CONFIG; CONFIG = load_json_file(CONFIG_FILE, DEFAULT_CONFIG)
        report.append("âœ… *é…ç½®æ–‡ä»¶*: `config\\.json` åŠ è½½æ­£å¸¸")
    except Exception as e:
        report.append(f"âŒ *é…ç½®æ–‡ä»¶*: åŠ è½½å¤±è´¥ \\- {escape_markdown_v2(str(e))}")
        msg.edit_text("\n".join(report), parse_mode=ParseMode.MARKDOWN_V2); return
    report.append("\n*ğŸ”‘ API Keys:*")
    if not CONFIG.get('apis'): report.append("  \\- âš ï¸ æœªé…ç½®ä»»ä½• API Key")
    else:
        for i, key in enumerate(CONFIG['apis']):
            level = KEY_LEVELS.get(key, -1)
            level_name = {-1: "âŒ æ— æ•ˆ", 0: "âœ… å…è´¹", 1: "âœ… ä¸ªäºº", 2: "âœ… å•†ä¸š", 3: "âœ… ä¼ä¸š"}.get(level, "æœªçŸ¥")
            report.append(f"  `\\#{i+1}` \\(`...{key[-4:]}`\\): {level_name}")
    report.append("\n*ğŸŒ ä»£ç†:*")
    proxies_to_check = CONFIG.get("proxies", [])
    if not proxies_to_check and CONFIG.get("proxy"): proxies_to_check.append(CONFIG.get("proxy"))
    if not proxies_to_check: report.append("  \\- â„¹ï¸ æœªé…ç½®ä»£ç†")
    else:
        for p in proxies_to_check:
            try:
                requests.get("https://fofa.info", proxies={"http": p, "https": p}, timeout=10, verify=False)
                report.append(f"  \\- `{escape_markdown_v2(p)}`: âœ… è¿æ¥æˆåŠŸ")
            except Exception as e: report.append(f"  \\- `{escape_markdown_v2(p)}`: âŒ è¿æ¥å¤±è´¥ \\- `{escape_markdown_v2(str(e))}`")
    msg.edit_text("\n".join(report), parse_mode=ParseMode.MARKDOWN_V2)
@admin_only
def stop_all_tasks(update: Update, context: CallbackContext):
    chat_id = update.effective_chat.id
    context.bot_data[f'stop_job_{chat_id}'] = True
    update.message.reply_text("ğŸ›‘ å·²å‘é€åœæ­¢ä¿¡å·ï¼Œå½“å‰ä¸‹è½½ä»»åŠ¡å°†åœ¨å®Œæˆæœ¬é¡µååœæ­¢ã€‚")
@admin_only
def backup_config_command(update: Update, context: CallbackContext):
    if update.callback_query:
        update.callback_query.answer()
    
    chat_id = update.effective_chat.id
    if os.path.exists(CONFIG_FILE):
        context.bot.send_message(chat_id, "ğŸ“¤ æ­£åœ¨å‘é€é…ç½®æ–‡ä»¶å¤‡ä»½...")
        send_file_safely(context, chat_id, CONFIG_FILE)
        upload_and_send_links(context, chat_id, CONFIG_FILE)
    else:
        context.bot.send_message(chat_id, "âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ã€‚")
@admin_only
def restore_config_command(update: Update, context: CallbackContext):
    update.message.reply_text("è¯·å‘é€æ‚¨çš„ `config.json` å¤‡ä»½æ–‡ä»¶ã€‚")
    return RESTORE_STATE_GET_FILE
def receive_config_file(update: Update, context: CallbackContext):
    doc = update.message.document
    if doc.file_name != 'config.json':
        update.message.reply_text("âŒ æ–‡ä»¶åé”™è¯¯ï¼Œè¯·ç¡®ä¿æ‚¨ä¸Šä¼ çš„æ˜¯ `config.json`ã€‚")
        return ConversationHandler.END
    file = doc.get_file()
    file.download(custom_path=CONFIG_FILE)
    global CONFIG; CONFIG = load_json_file(CONFIG_FILE, DEFAULT_CONFIG)
    update.message.reply_text("âœ… é…ç½®æ–‡ä»¶å·²æ¢å¤ã€‚æœºå™¨äººå°†è‡ªåŠ¨é‡å¯ä»¥åº”ç”¨æ›´æ”¹ã€‚")
    shutdown_command(update, context, restart=True)
    return ConversationHandler.END
@admin_only
def history_command(update: Update, context: CallbackContext):
    if not HISTORY['queries']: update.message.reply_text("æŸ¥è¯¢å†å²ä¸ºç©ºã€‚"); return
    history_text = "*ğŸ•°ï¸ æœ€è¿‘æŸ¥è¯¢å†å²*\n\n"
    for i, item in enumerate(HISTORY['queries'][:15]):
        dt_utc = datetime.fromisoformat(item['timestamp']); dt_local = dt_utc.astimezone(tz.tzlocal()); time_str = dt_local.strftime('%Y-%m-%d %H:%M')
        history_text += f"`{i+1}\\.` `{escape_markdown_v2(item['query_text'])}`\n   _{escape_markdown_v2(time_str)}_\n"
    update.message.reply_text(history_text, parse_mode=ParseMode.MARKDOWN_V2)
@admin_only
def import_command(update: Update, context: CallbackContext):
    update.message.reply_text("è¯·å‘é€æ‚¨è¦å¯¼å…¥çš„æ—§ç¼“å­˜æ–‡ä»¶ (txtæ ¼å¼)ã€‚")
    return IMPORT_STATE_GET_FILE
def get_import_query(update: Update, context: CallbackContext):
    doc = update.message.document
    if not doc.file_name.endswith('.txt'): update.message.reply_text("âŒ è¯·ä¸Šä¼  .txt æ–‡ä»¶ã€‚"); return ConversationHandler.END
    file = doc.get_file()
    temp_path = os.path.join(FOFA_CACHE_DIR, f"import_{doc.file_id}.txt")
    file.download(custom_path=temp_path)
    try:
        with open(temp_path, 'r', encoding='utf-8') as f: result_count = sum(1 for _ in f)
    except Exception as e: update.message.reply_text(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}"); os.remove(temp_path); return ConversationHandler.END
    query_text = update.message.text
    if not query_text: update.message.reply_text("è¯·è¾“å…¥ä¸æ­¤æ–‡ä»¶å…³è”çš„åŸå§‹FOFAæŸ¥è¯¢è¯­æ³•:"); return IMPORT_STATE_GET_FILE
    final_filename = generate_filename_from_query(query_text)
    final_path = os.path.join(FOFA_CACHE_DIR, final_filename)
    shutil.move(temp_path, final_path)
    cache_data = {'file_path': final_path, 'result_count': result_count}
    add_or_update_query(query_text, cache_data)
    update.message.reply_text(f"âœ… æˆåŠŸå¯¼å…¥ç¼“å­˜ï¼\næŸ¥è¯¢: `{escape_markdown_v2(query_text)}`\nå…± {result_count} æ¡è®°å½•\\.", parse_mode=ParseMode.MARKDOWN_V2)
    return ConversationHandler.END
@admin_only
def get_log_command(update: Update, context: CallbackContext):
    if os.path.exists(LOG_FILE):
        send_file_safely(context, update.effective_chat.id, LOG_FILE)
        upload_and_send_links(context, update.effective_chat.id, LOG_FILE)
    else: update.message.reply_text("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶ã€‚")

@admin_only
def shutdown_command(update: Update, context: CallbackContext, restart=False):
    message = "ğŸ¤– æœºå™¨äººæ­£åœ¨é‡å¯..." if restart else "ğŸ¤– æœºå™¨äººæ­£åœ¨å…³é—­..."
    update.message.reply_text(message)
    logger.info(f"Shutdown/Restart initiated by user {update.effective_user.id}")
    
    # v10.9 FIX: Use OS signals for a truly robust and deadlock-free shutdown.
    # This sends a SIGINT signal (like Ctrl+C) to the bot's own process,
    # which updater.idle() is designed to catch gracefully.
    threading.Thread(target=lambda: (time.sleep(1), os.kill(os.getpid(), signal.SIGINT))).start()

@admin_only
def update_script_command(update: Update, context: CallbackContext):
    update_url = CONFIG.get("update_url")
    if not update_url:
        update.message.reply_text("âŒ æœªåœ¨è®¾ç½®ä¸­é…ç½®æ›´æ–°URLã€‚è¯·ä½¿ç”¨ /settings \\-\\> è„šæœ¬æ›´æ–° \\-\\> è®¾ç½®URLã€‚")
        return
    msg = update.message.reply_text("â³ æ­£åœ¨ä»è¿œç¨‹URLä¸‹è½½æ–°è„šæœ¬...")
    try:
        response = requests.get(update_url, timeout=30, proxies=get_proxies())
        response.raise_for_status()
        script_content = response.text
        with open(__file__, 'w', encoding='utf-8') as f:
            f.write(script_content)
        msg.edit_text("âœ… è„šæœ¬æ›´æ–°æˆåŠŸï¼æœºå™¨äººå°†è‡ªåŠ¨é‡å¯ä»¥åº”ç”¨æ–°ç‰ˆæœ¬ã€‚")
        shutdown_command(update, context, restart=True)
    except Exception as e:
        msg.edit_text(f"âŒ æ›´æ–°å¤±è´¥: {escape_markdown_v2(str(e))}", parse_mode=ParseMode.MARKDOWN_V2)

# --- è®¾ç½®èœå• ---
@admin_only
def settings_command(update: Update, context: CallbackContext):
    keyboard = [
        [InlineKeyboardButton("ğŸ”‘ API ç®¡ç†", callback_data='settings_api'), InlineKeyboardButton("âœ¨ é¢„è®¾ç®¡ç†", callback_data='settings_preset')],
        [InlineKeyboardButton("ğŸŒ ä»£ç†æ± ç®¡ç†", callback_data='settings_proxypool'), InlineKeyboardButton("ğŸ“¤ ä¸Šä¼ æ¥å£è®¾ç½®", callback_data='settings_upload')],
        [InlineKeyboardButton("ğŸ‘¨â€ğŸ’¼ ç®¡ç†å‘˜è®¾ç½®", callback_data='settings_admin')],
        [InlineKeyboardButton("ğŸ’¾ å¤‡ä»½ä¸æ¢å¤", callback_data='settings_backup'), InlineKeyboardButton("ğŸ”„ è„šæœ¬æ›´æ–°", callback_data='settings_update')],
        [InlineKeyboardButton("âŒ å…³é—­èœå•", callback_data='settings_close')]
    ]
    message_text = "âš™ï¸ *è®¾ç½®èœå•*"; reply_markup = InlineKeyboardMarkup(keyboard)
    if update.callback_query: update.callback_query.message.edit_text(message_text, reply_markup=reply_markup, parse_mode=ParseMode.MARKDOWN_V2)
    else: update.message.reply_text(message_text, reply_markup=reply_markup, parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_MAIN
def settings_callback_handler(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); menu = query.data.split('_', 1)[1]
    if menu == 'api': return show_api_menu(update, context, force_check=False)
    if menu == 'proxypool': return show_proxypool_menu(update, context)
    if menu == 'backup': return show_backup_restore_menu(update, context)
    if menu == 'preset': return show_preset_menu(update, context)
    if menu == 'update': return show_update_menu(update, context)
    if menu == 'upload': return show_upload_api_menu(update, context)
    if menu == 'admin': return show_admin_menu(update, context)
    if menu == 'close': query.message.edit_text("èœå•å·²å…³é—­."); return ConversationHandler.END
    return SETTINGS_STATE_ACTION
def settings_action_handler(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); action = query.data.split('_', 1)[1]
    if action == 'add_api': query.message.edit_text("è¯·è¾“å…¥æ–°çš„FOFA API Key:"); return SETTINGS_STATE_GET_KEY
    if action == 'remove_api': query.message.edit_text("è¯·è¾“å…¥è¦ç§»é™¤çš„API Keyçš„ç¼–å·:"); return SETTINGS_STATE_REMOVE_API
    if action == 'check_api': return show_api_menu(update, context, force_check=True)
    if action == 'back': return settings_command(update, context)
def show_api_menu(update: Update, context: CallbackContext, force_check=False):
    query = update.callback_query
    if force_check: 
        query.message.edit_text("â³ æ­£åœ¨é‡æ–°æ£€æŸ¥æ‰€æœ‰API KeyçŠ¶æ€...")
        check_and_classify_keys()
    api_list_text = ["*ğŸ”‘ å½“å‰ API Keys:*"]
    if not CONFIG['apis']: api_list_text.append("  \\- _ç©º_")
    else:
        for i, key in enumerate(CONFIG['apis']):
            level = KEY_LEVELS.get(key, -1)
            level_name = {-1: "âŒ æ— æ•ˆ", 0: "âœ… å…è´¹", 1: "âœ… ä¸ªäºº", 2: "âœ… å•†ä¸š", 3: "âœ… ä¼ä¸š"}.get(level, "æœªçŸ¥")
            api_list_text.append(f"  `\\#{i+1}` `...{key[-4:]}` \\- {level_name}")
    keyboard = [
        [InlineKeyboardButton("â• æ·»åŠ ", callback_data='action_add_api'), InlineKeyboardButton("â– ç§»é™¤", callback_data='action_remove_api')],
        [InlineKeyboardButton("ğŸ”„ çŠ¶æ€æ£€æŸ¥", callback_data='action_check_api'), InlineKeyboardButton("ğŸ”™ è¿”å›", callback_data='action_back')]
    ]
    query.message.edit_text("\n".join(api_list_text), reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_ACTION
def get_key(update: Update, context: CallbackContext):
    new_key = update.message.text.strip()
    if new_key in CONFIG['apis']:
        update.message.reply_text("âš ï¸ æ­¤ Key å·²å­˜åœ¨ã€‚")
        return settings_command(update, context)

    msg = update.message.reply_text("â³ æ­£åœ¨éªŒè¯æ–°çš„ API Key...")
    data, error = verify_fofa_api(new_key)
    if error:
        msg.edit_text(f"âŒ Key éªŒè¯å¤±è´¥: {error}\nè¯·é‡æ–°è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„Keyï¼Œæˆ–ä½¿ç”¨ /cancel å–æ¶ˆã€‚")
        return SETTINGS_STATE_GET_KEY  
    
    CONFIG['apis'].append(new_key)
    save_config()
    check_and_classify_keys() 
    msg.edit_text(f"âœ… API Key ({data.get('username', 'N/A')}) å·²æˆåŠŸæ·»åŠ ã€‚")
    
    # ä½¿ç”¨ä¸€ä¸ªæ–°çš„ update å¯¹è±¡æ¥è°ƒç”¨ settings_commandï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ä¸ªæœ‰æ•ˆçš„ update å¯¹è±¡
    # æ¥å‘é€æ–°æ¶ˆæ¯ï¼Œè€Œæˆ‘ä»¬ç¼–è¾‘äº†æ—§æ¶ˆæ¯ã€‚
    fake_update = type('FakeUpdate', (), {'message': update.message, 'callback_query': None})
    return settings_command(fake_update, context)

def remove_api(update: Update, context: CallbackContext):
    input_text = update.message.text.strip()
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰æ•°å­—ï¼Œæ”¯æŒé€—å·ã€ç©ºæ ¼ç­‰åˆ†éš”ç¬¦
    indices_to_remove_str = re.findall(r'\d+', input_text)
    
    if not indices_to_remove_str:
        update.message.reply_text("âŒ è¯·è¾“å…¥ä¸€ä¸ªæˆ–å¤šä¸ªæœ‰æ•ˆçš„æ•°å­—ç¼–å·ã€‚")
        return settings_command(update, context)

    indices_to_remove = set()
    invalid_indices = []
    for index_str in indices_to_remove_str:
        try:
            index = int(index_str) - 1
            if 0 <= index < len(CONFIG['apis']):
                indices_to_remove.add(index)
            else:
                invalid_indices.append(index_str)
        except ValueError:
            invalid_indices.append(index_str)

    if invalid_indices:
        update.message.reply_text(f"âš ï¸ æ— æ•ˆçš„ç¼–å·: {', '.join(invalid_indices)}ã€‚")

    if not indices_to_remove:
        return settings_command(update, context)

    # å¯¹ç´¢å¼•è¿›è¡Œé™åºæ’åºï¼Œä»¥é˜²æ­¢åœ¨åˆ é™¤æ—¶å‡ºç°ç´¢å¼•é”™è¯¯
    sorted_indices = sorted(list(indices_to_remove), reverse=True)
    
    removed_keys_display = []
    for index in sorted_indices:
        removed_key = CONFIG['apis'].pop(index)
        # v10.9.6 FIX: æ‰‹åŠ¨è½¬ä¹‰Markdownå­—ç¬¦ä»¥ç”¨äºç¡®è®¤æ¶ˆæ¯ã€‚
        removed_keys_display.append(f"`...{removed_key[-4:]}` \\(åŸç¼–å· \\#{index + 1}\\)")

    save_config()
    check_and_classify_keys()
    
    update.message.reply_text(f"âœ… å·²æˆåŠŸç§»é™¤ä»¥ä¸‹ Key:\n{', '.join(reversed(removed_keys_display))}", parse_mode=ParseMode.MARKDOWN_V2)
    
    fake_update = type('FakeUpdate', (), {'message': update.message, 'callback_query': None})
    return settings_command(fake_update, context)
def show_preset_menu(update: Update, context: CallbackContext):
    query = update.callback_query; presets = CONFIG.get("presets", [])
    text = ["*âœ¨ é¢„è®¾æŸ¥è¯¢ç®¡ç†*"]
    if not presets: text.append("  \\- _ç©º_")
    else:
        for i, p in enumerate(presets): text.append(f"`{i+1}\\.` *{escape_markdown_v2(p['name'])}*: `{escape_markdown_v2(p['query'])}`")
    keyboard = [
        [InlineKeyboardButton("â• æ·»åŠ ", callback_data='preset_add'), InlineKeyboardButton("â– ç§»é™¤", callback_data='preset_remove')],
        [InlineKeyboardButton("ğŸ”™ è¿”å›", callback_data='preset_back')]
    ]
    query.message.edit_text("\n".join(text), reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_PRESET_MENU
def preset_menu_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); action = query.data.split('_')[1]
    if action == 'add': query.message.edit_text("è¯·è¾“å…¥é¢„è®¾çš„åç§°:"); return SETTINGS_STATE_GET_PRESET_NAME
    if action == 'remove': query.message.edit_text("è¯·è¾“å…¥è¦ç§»é™¤çš„é¢„è®¾çš„ç¼–å·:"); return SETTINGS_STATE_REMOVE_PRESET
    if action == 'back': return settings_command(update, context)
def get_preset_name(update: Update, context: CallbackContext):
    context.user_data['preset_name'] = update.message.text.strip()
    update.message.reply_text("è¯·è¾“å…¥æ­¤é¢„è®¾çš„FOFAæŸ¥è¯¢è¯­æ³•:")
    return SETTINGS_STATE_GET_PRESET_QUERY
def get_preset_query(update: Update, context: CallbackContext):
    preset_query = update.message.text.strip(); preset_name = context.user_data['preset_name']
    CONFIG.setdefault("presets", []).append({"name": preset_name, "query": preset_query}); save_config()
    update.message.reply_text("âœ… é¢„è®¾å·²æ·»åŠ ã€‚")
    return settings_command(update, context)
def remove_preset(update: Update, context: CallbackContext):
    try:
        index = int(update.message.text.strip()) - 1
        if 0 <= index < len(CONFIG['presets']):
            CONFIG['presets'].pop(index); save_config()
            update.message.reply_text("âœ… é¢„è®¾å·²ç§»é™¤ã€‚")
        else: update.message.reply_text("âŒ æ— æ•ˆçš„ç¼–å·ã€‚")
    except ValueError: update.message.reply_text("âŒ è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—ç¼–å·ã€‚")
    return settings_command(update, context)
def show_update_menu(update: Update, context: CallbackContext):
    query = update.callback_query
    url = CONFIG.get("update_url") or "æœªè®¾ç½®"
    text = f"ğŸ”„ *è„šæœ¬æ›´æ–°è®¾ç½®*\n\nå½“å‰æ›´æ–°URL: `{escape_markdown_v2(url)}`"
    keyboard = [[InlineKeyboardButton("âœï¸ è®¾ç½®URL", callback_data='update_set_url'), InlineKeyboardButton("ğŸ”™ è¿”å›", callback_data='update_back')]]
    query.message.edit_text(text, reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_ACTION
def get_update_url(update: Update, context: CallbackContext):
    url = update.message.text.strip()
    if url.lower().startswith('http'): CONFIG['update_url'] = url; save_config(); update.message.reply_text("âœ… æ›´æ–°URLå·²è®¾ç½®ã€‚")
    else: update.message.reply_text("âŒ æ— æ•ˆçš„URLæ ¼å¼ã€‚")
    return settings_command(update, context)
def show_backup_restore_menu(update: Update, context: CallbackContext):
    query = update.callback_query
    text = "ğŸ’¾ *å¤‡ä»½ä¸æ¢å¤*\n\n\\- *å¤‡ä»½*: å‘é€å½“å‰çš„ `config\\.json` æ–‡ä»¶ç»™æ‚¨ã€‚\n\\- *æ¢å¤*: æ‚¨éœ€è¦å‘æœºå™¨äººå‘é€ä¸€ä¸ª `config\\.json` æ–‡ä»¶æ¥è¦†ç›–å½“å‰é…ç½®ã€‚"
    keyboard = [[InlineKeyboardButton("ğŸ“¤ å¤‡ä»½", callback_data='backup_now'), InlineKeyboardButton("ğŸ“¥ æ¢å¤", callback_data='restore_now')], [InlineKeyboardButton("ğŸ”™ è¿”å›", callback_data='backup_back')]]
    query.message.edit_text(text, reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_ACTION
def show_proxypool_menu(update: Update, context: CallbackContext):
    query = update.callback_query
    proxies = CONFIG.get("proxies", [])
    text = ["*ğŸŒ ä»£ç†æ± ç®¡ç†*"]
    if not proxies: text.append("  \\- _ç©º_")
    else:
        for i, p in enumerate(proxies): text.append(f"`{i+1}\\.` `{escape_markdown_v2(p)}`")
    keyboard = [
        [InlineKeyboardButton("â• æ·»åŠ ", callback_data='proxypool_add'), InlineKeyboardButton("â– ç§»é™¤", callback_data='proxypool_remove')],
        [InlineKeyboardButton("ğŸ”™ è¿”å›", callback_data='proxypool_back')]
    ]
    query.message.edit_text("\n".join(text), reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_PROXYPOOL_MENU
def proxypool_menu_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); action = query.data.split('_')[1]
    if action == 'add': query.message.edit_text("è¯·è¾“å…¥è¦æ·»åŠ çš„ä»£ç† (æ ¼å¼: `http://user:pass@host:port`):"); return SETTINGS_STATE_GET_PROXY_ADD
    if action == 'remove': query.message.edit_text("è¯·è¾“å…¥è¦ç§»é™¤çš„ä»£ç†çš„ç¼–å·:"); return SETTINGS_STATE_GET_PROXY_REMOVE
    if action == 'back': return settings_command(update, context)
def get_proxy_to_add(update: Update, context: CallbackContext):
    proxy = update.message.text.strip()
    if proxy not in CONFIG['proxies']: CONFIG['proxies'].append(proxy); save_config(); update.message.reply_text("âœ… ä»£ç†å·²æ·»åŠ ã€‚")
    else: update.message.reply_text("âš ï¸ æ­¤ä»£ç†å·²å­˜åœ¨ã€‚")
    return settings_command(update, context)
def get_proxy_to_remove(update: Update, context: CallbackContext):
    try:
        index = int(update.message.text.strip()) - 1
        if 0 <= index < len(CONFIG['proxies']):
            CONFIG['proxies'].pop(index); save_config()
            update.message.reply_text("âœ… ä»£ç†å·²ç§»é™¤ã€‚")
        else: update.message.reply_text("âŒ æ— æ•ˆçš„ç¼–å·ã€‚")
    except ValueError: update.message.reply_text("âŒ è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—ç¼–å·ã€‚")
    return settings_command(update, context)
def show_upload_api_menu(update: Update, context: CallbackContext):
    query = update.callback_query
    url = CONFIG.get("upload_api_url") or "æœªè®¾ç½®"
    token_status = "å·²è®¾ç½®" if CONFIG.get("upload_api_token") else "æœªè®¾ç½®"
    text = (f"ğŸ“¤ *ä¸Šä¼ æ¥å£è®¾ç½®*\n\n"
            f"æ­¤åŠŸèƒ½å¯å°†æœºå™¨äººç”Ÿæˆçš„æ‰€æœ‰æ–‡ä»¶è‡ªåŠ¨ä¸Šä¼ åˆ°æ‚¨æŒ‡å®šçš„æœåŠ¡å™¨ï¼Œå¹¶è¿”å›ä¸‹è½½å‘½ä»¤ã€‚\n\n"
            f"*API URL:* `{escape_markdown_v2(url)}`\n"
            f"*API Token:* `{token_status}`")
    kbd = [
        [InlineKeyboardButton("âœï¸ è®¾ç½® URL", callback_data='upload_set_url'), InlineKeyboardButton("ğŸ”‘ è®¾ç½® Token", callback_data='upload_set_token')],
        [InlineKeyboardButton("ğŸ”™ è¿”å›", callback_data='upload_back')]
    ]
    query.message.edit_text(text, reply_markup=InlineKeyboardMarkup(kbd), parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_UPLOAD_API_MENU
def upload_api_menu_callback(update: Update, context: CallbackContext):
    query = update.callback_query; query.answer(); action = query.data.split('_', 1)[1]
    if action == 'back': return settings_command(update, context)
    if action == 'set_url': query.message.edit_text("è¯·è¾“å…¥æ‚¨çš„ä¸Šä¼ æ¥å£ URL:"); return SETTINGS_STATE_GET_UPLOAD_URL
    if action == 'set_token': query.message.edit_text("è¯·è¾“å…¥æ‚¨çš„ä¸Šä¼ æ¥å£ Token:"); return SETTINGS_STATE_GET_UPLOAD_TOKEN
    return SETTINGS_STATE_UPLOAD_API_MENU
def get_upload_url(update: Update, context: CallbackContext):
    url = update.message.text.strip()
    if url.lower().startswith('http'):
        CONFIG['upload_api_url'] = url; save_config()
        update.message.reply_text("âœ… ä¸Šä¼  URL å·²æ›´æ–°ã€‚")
    else: update.message.reply_text("âŒ æ— æ•ˆçš„ URL æ ¼å¼ã€‚")
    return settings_command(update, context)
def get_upload_token(update: Update, context: CallbackContext):
    token = update.message.text.strip()
    CONFIG['upload_api_token'] = token; save_config()
    update.message.reply_text("âœ… ä¸Šä¼  Token å·²æ›´æ–°ã€‚")
    return settings_command(update, context)

# --- Admin Management ---
def show_admin_menu(update: Update, context: CallbackContext):
    query = update.callback_query
    admins = CONFIG.get('admins', [])
    text = ["*ğŸ‘¨â€ğŸ’¼ ç®¡ç†å‘˜åˆ—è¡¨*"]
    if not admins:
        text.append("  \\- _ç©º_")
    else:
        for i, admin_id in enumerate(admins):
            user_label = "â­ è¶…çº§ç®¡ç†å‘˜" if i == 0 else f"  `\\#{i+1}`"
            text.append(f"{user_label} \\- `{admin_id}`")
    
    keyboard = []
    if is_super_admin(query.from_user.id):
        keyboard.append([
            InlineKeyboardButton("â• æ·»åŠ ç®¡ç†å‘˜", callback_data='admin_add'),
            InlineKeyboardButton("â– ç§»é™¤ç®¡ç†å‘˜", callback_data='admin_remove')
        ])
    keyboard.append([InlineKeyboardButton("ğŸ”™ è¿”å›", callback_data='admin_back')])
    
    query.message.edit_text("\n".join(text), reply_markup=InlineKeyboardMarkup(keyboard), parse_mode=ParseMode.MARKDOWN_V2)
    return SETTINGS_STATE_ADMIN_MENU

def admin_menu_callback(update: Update, context: CallbackContext):
    query = update.callback_query
    query.answer()
    action = query.data.split('_')[1]

    if not is_super_admin(query.from_user.id):
        query.answer("â›”ï¸ åªæœ‰è¶…çº§ç®¡ç†å‘˜æ‰èƒ½æ‰§è¡Œæ­¤æ“ä½œã€‚", show_alert=True)
        return SETTINGS_STATE_ADMIN_MENU


    if action == 'add':
        query.message.edit_text("è¯·è¾“å…¥æ–°ç®¡ç†å‘˜çš„ Telegram User ID:")
        return SETTINGS_STATE_GET_ADMIN_ID_TO_ADD

    if action == 'remove':
        query.message.edit_text("è¯·è¾“å…¥è¦ç§»é™¤çš„ç®¡ç†å‘˜çš„ç¼–å· (ä¾‹å¦‚: 2):")
        return SETTINGS_STATE_GET_ADMIN_ID_TO_REMOVE

    if action == 'back':
        return settings_command(update, context)

def get_admin_id_to_add(update: Update, context: CallbackContext):
    try:
        new_id = int(update.message.text.strip())
        admins = CONFIG.get('admins', [])
        if new_id in admins:
            update.message.reply_text("âš ï¸ æ­¤ç”¨æˆ·å·²ç»æ˜¯ç®¡ç†å‘˜ã€‚")
        else:
            CONFIG['admins'].append(new_id)
            save_config()
            update.message.reply_text("âœ… ç®¡ç†å‘˜å·²æ·»åŠ ã€‚")
    except ValueError:
        update.message.reply_text("âŒ æ— æ•ˆçš„ User IDï¼Œè¯·è¾“å…¥çº¯æ•°å­—ã€‚")
    
    return settings_command(update, context)

def get_admin_id_to_remove(update: Update, context: CallbackContext):
    try:
        index = int(update.message.text.strip())
        admins = CONFIG.get('admins', [])
        if index == 1:
            update.message.reply_text("âŒ ä¸èƒ½ç§»é™¤è¶…çº§ç®¡ç†å‘˜ã€‚")
        elif 1 < index <= len(admins):
            removed_admin = CONFIG['admins'].pop(index - 1)
            save_config()
            update.message.reply_text(f"âœ… å·²ç§»é™¤ç®¡ç†å‘˜ `{removed_admin}`ã€‚")
        else:
            update.message.reply_text("âŒ æ— æ•ˆçš„ç¼–å·ã€‚")
    except ValueError:
        update.message.reply_text("âŒ è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—ç¼–å·ã€‚")
    
    return settings_command(update, context)

# --- /allfofa Command Logic ---
def start_allfofa_search(update: Update, context: CallbackContext, message_to_edit=None):
    query_text = context.user_data['query']
    msg = message_to_edit if message_to_edit else update.effective_message.reply_text(f"ğŸšš æ­£åœ¨ä¸ºæŸ¥è¯¢ `{escape_markdown_v2(query_text)}` å‡†å¤‡æµ·é‡æ•°æ®è·å–ä»»åŠ¡\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    
    # v10.9.5 FIX: Set min_level=1 for /allfofa pre-check to ensure a VIP key is used.
    data, used_key, _, _, used_proxy, error = execute_query_with_fallback(
        lambda key, key_level, proxy_session: fetch_fofa_next_data(key, query_text, page_size=10000, proxy_session=proxy_session),
        min_level=1
    )

    if error:
        msg.edit_text(f"âŒ æŸ¥è¯¢é¢„æ£€å¤±è´¥: {escape_markdown_v2(error)}", parse_mode=ParseMode.MARKDOWN_V2)
        return ConversationHandler.END
        
    total_size = data.get('size', 0)
    if total_size == 0:
        msg.edit_text("ğŸ¤·â€â™€ï¸ æœªæ‰¾åˆ°ä»»ä½•ç»“æœã€‚")
        return ConversationHandler.END

    initial_results = data.get('results', [])
    initial_next_id = data.get('next')

    context.user_data['query'] = query_text
    context.user_data['total_size'] = total_size
    context.user_data['chat_id'] = update.effective_chat.id
    context.user_data['start_key'] = used_key
    context.user_data['initial_results'] = initial_results
    context.user_data['initial_next_id'] = initial_next_id
    # v10.9.4 FIX: Lock the proxy session for the background job.
    context.user_data['proxy_session'] = used_proxy

    keyboard = [
        [InlineKeyboardButton(f"â™¾ï¸ å…¨éƒ¨è·å– ({total_size}æ¡)", callback_data='allfofa_limit_none')],
        [InlineKeyboardButton("âŒ å–æ¶ˆ", callback_data='allfofa_limit_cancel')]
    ]
    msg.edit_text(
        f"âœ… æŸ¥è¯¢é¢„æ£€æˆåŠŸï¼Œå…±å‘ç° {total_size} æ¡ç»“æœã€‚\n\n"
        "è¯·è¾“å…¥æ‚¨å¸Œæœ›è·å–çš„æ•°é‡ä¸Šé™ (ä¾‹å¦‚: 50000)ï¼Œæˆ–é€‰æ‹©å…¨éƒ¨è·å–ã€‚",
        reply_markup=InlineKeyboardMarkup(keyboard)
    )
    return QUERY_STATE_ALLFOFA_GET_LIMIT

def allfofa_get_limit(update: Update, context: CallbackContext):
    limit = None
    query = update.callback_query
    
    if query:
        query.answer()
        if query.data == 'allfofa_limit_cancel':
            query.message.edit_text("æ“ä½œå·²å–æ¶ˆ.")
            return ConversationHandler.END
        msg_target = query.message
    else:
        try:
            limit = int(update.message.text.strip())
            assert limit > 0
        except (ValueError, AssertionError):
            update.message.reply_text("âŒ æ— æ•ˆçš„æ•°å­—ï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ­£æ•´æ•°ã€‚")
            return QUERY_STATE_ALLFOFA_GET_LIMIT
        msg_target = update.message

    context.user_data['limit'] = limit
    msg_target.reply_text(f"âœ… ä»»åŠ¡å·²æäº¤ï¼\nå°†ä½¿ç”¨ `next` æ¥å£è·å–æ•°æ® (ä¸Šé™: {limit or 'æ— '})...")
    start_download_job(context, run_allfofa_download_job, context.user_data)
    if query:
        msg_target.delete()
    return ConversationHandler.END

def run_allfofa_download_job(context: CallbackContext):
    """
    æ™ºèƒ½å‰¥ç¦»ä¸‹è½½å™¨ (Smart Peeling + Time Slicing)
    æ ¸å¿ƒç­–ç•¥: 
    1. å¾ªç¯æ£€æµ‹å½“å‰Queryçš„æ•°æ®é‡ã€‚
    2. >10000: å– Top1 å›½å®¶ï¼Œæ‹†åˆ†ä¸º Slice (è¯¥å›½å®¶) å’Œ Remaining (éè¯¥å›½å®¶)ã€‚
       å¯¹ Slice ä½¿ç”¨ Time Traceback æš´åŠ›ä¸‹è½½ã€‚
       å¯¹ Remaining è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯ã€‚
    3. <10000: ç›´æ¥æ™®é€šç¿»é¡µä¸‹è½½ã€‚
    """
    job_data = context.job.context
    bot, chat_id = context.bot, job_data['chat_id']
    limit = job_data.get('limit')
    
    # åŸå§‹æŸ¥è¯¢
    original_query = job_data['query']
    
    # ä½¿ç”¨é”å®šçš„ Key å’Œ Proxy Session (ä» allfofa command åˆå§‹åŒ–ä¼ è¿‡æ¥çš„)
    current_key = job_data.get('start_key') 
    proxy_session = job_data.get('proxy_session')

    if not current_key:
        bot.send_message(chat_id, "âŒ å†…éƒ¨é”™è¯¯ï¼šä»»åŠ¡ä¸Šä¸‹æ–‡ä¸¢å¤± Key ä¿¡æ¯ã€‚")
        return

    # è¾“å‡ºæ–‡ä»¶åç®¡ç†
    output_filename = generate_filename_from_query(original_query, prefix="smart_all")
    cache_path = os.path.join(FOFA_CACHE_DIR, output_filename)
    
    # ç”¨äºæ˜¾ç¤ºçš„è¿›åº¦æ›´æ–°
    msg = bot.send_message(chat_id, "ğŸš€ æ™ºèƒ½å‰¥ç¦»å¼•æ“å·²å¯åŠ¨...\næ­£åœ¨åˆ†ææ•°æ®åˆ†å¸ƒ...")
    stop_flag = f'stop_job_{chat_id}'
    
    current_query_scope = original_query
    collected_results = set() # ä¸ºäº†æœ€åå»é‡ (æµ·é‡æ•°æ®å†…å­˜æ˜¯ä¸ªé—®é¢˜ï¼Œä½†å¯¹äºset stré€šå¸¸è¿˜èƒ½æ¥å—ï¼Œå¦‚æœç™¾ä¸‡çº§è€ƒè™‘è½ç›˜å»é‡)
    
    loop_count = 0
    start_time = time.time()
    last_ui_update = 0

    try:
        while True:
            loop_count += 1
            if context.bot_data.get(stop_flag):
                msg.edit_text("ğŸŒ€ ä»»åŠ¡å·²æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¸­æ­¢...")
                break
                
            if limit and len(collected_results) >= limit:
                break

            # 1. ä¼°ç®—å½“å‰ Scope å¤§å°
            data_size_chk, error = fetch_fofa_data(current_key, current_query_scope, page_size=1, fields="host", proxy_session=proxy_session)
            if error: 
                msg.edit_text(f"âŒ ä¾¦æŸ¥å¤±è´¥: {error}")
                break
            
            scope_size = data_size_chk.get('size', 0)
            
            # --- é˜¶æ®µ A: å°æ•°æ®é‡ç›´æ¥åå™¬ ---
            if scope_size <= 10000: # å°äº1ä¸‡ï¼Œä¸€é”…ç«¯
                if loop_count == 1: 
                    msg.edit_text(f"ğŸ” æ•°æ®é‡ ({scope_size}) å°äºå•æ¬¡é™åˆ¶ï¼Œç›´æ¥ä¸‹è½½...")
                
                # æ™®é€šç¿»é¡µè·å– (Normal Page Iteration)
                pages = (scope_size + 9999) // 10000
                for p in range(1, pages + 1):
                    # è·å–
                    d, e = fetch_fofa_data(current_key, current_query_scope, page=p, page_size=10000, fields="host", proxy_session=proxy_session)
                    if not e and d.get('results'):
                        collected_results.update([r for r in d.get('results') if isinstance(r, str) and ':' in r])
                    
                    # è¿›åº¦UI
                    if time.time() - last_ui_update > 3:
                        msg.edit_text(f"ğŸ“¥ ç›´æ¥ä¸‹è½½ä¸­... (å·²æ”¶å½•: {len(collected_results)})")
                        last_ui_update = time.time()
                        
                break # å½“å‰å‰©ä½™çš„æ‰€æœ‰éƒ½åœ¨è¿™ä¸€è½®è¢«æ‹¿èµ°äº†ï¼Œå¤§å¾ªç¯ç»“æŸ

            # --- é˜¶æ®µ B: å¤§æ•°æ®é‡ç©ºé—´å‰¥ç¦» (Country Slicing) ---
            # è·å– Top1 å›½å®¶
            stats_data, e = fetch_fofa_stats(current_key, current_query_scope, proxy_session=proxy_session)
            if e: 
                msg.edit_text(f"âŒ èšåˆåˆ†æå¤±è´¥: {e}")
                break
            
            aggs = stats_data.get("aggs", stats_data)
            countries = aggs.get("countries", [])
            
            if not countries:
                # æç«¯æƒ…å†µï¼šæŸ¥åˆ°äº†Sizeä½†æ²¡æœ‰Statså›½å®¶ï¼Ÿå¯èƒ½æ˜¯IPç±»å‹ã€‚
                # å¼ºåˆ¶è¿›å…¥æ—¶é—´åˆ‡ç‰‡æ¨¡å¼ (Blind Traceback)
                top_country_code = None
            else:
                top_country_code = countries[0].get('name') # e.g., "US" or "CN"
            
            # æ„é€ åˆ‡ç‰‡æŸ¥è¯¢
            if top_country_code:
                slice_query = f'({current_query_scope}) && country="{top_country_code}"'
                # å‰©ä½™éƒ¨åˆ† = å½“å‰Scope && ä¸ç­‰äº Top1
                next_round_query = f'({current_query_scope}) && country!="{top_country_code}"'
                slice_desc = f"å›½å®¶={top_country_code}"
            else:
                # å¦‚æœæ²¡æ³•æŒ‰å›½å®¶åˆ†ï¼Œé‚£å°±æ•´ä¸ªå½“åšä¸€å—è‚‰ï¼Œå°è¯•ç¡¬åˆ‡ (fallback to Time Trace on whole query)
                slice_query = current_query_scope
                next_round_query = None # æ²¡æœ‰ä¸‹ä¸€è½®äº†ï¼Œè¿™æ˜¯æœ€åä¸€æ
                slice_desc = "å…¨éƒ¨å‰©ä½™æ•°æ®"

            # å¯¹ Slice ä½¿ç”¨æ·±åº¦è¿½æº¯ä¸‹è½½ (Time Peeling)
            # ç”¨æˆ·æ ¸å¿ƒç­–ç•¥ï¼šå¤ç”¨æ·±åº¦è¿½æº¯ï¼Œåˆ©ç”¨æ—¶é—´è½´æŠŠè¿™ä¸ªå·¨å¤§çš„ slice æ‰’ä¸‹æ¥
            trace_count_added = 0
            iterator = iter_fofa_traceback(current_key, slice_query, limit=limit, proxy_session=proxy_session)
            
            for batch in iterator:
                if context.bot_data.get(stop_flag): break
                
                # æ‰¹é‡æ·»åŠ 
                valid_items = [item[0] for item in batch if item and isinstance(item, list) and len(item)>0]
                new_items_count = 0
                for item in valid_items:
                    if item not in collected_results:
                        collected_results.add(item)
                        new_items_count += 1
                        
                trace_count_added += new_items_count
                
                if time.time() - last_ui_update > 3:
                    try:
                        prog_bar = create_progress_bar(min(len(collected_results) / (limit or (len(collected_results)+100000)) * 100, 100))
                        msg.edit_text(
                            f"âœ‚ï¸ *æ­£åœ¨å‰¥ç¦»æ•°æ®å—:* `{slice_desc}`\n"
                            f"ğŸ“‰ ç­–ç•¥: æ—¶é—´è½´é™ç»´æ‰“å‡» (Time Trace)\n"
                            f"{prog_bar} æ€»æ•°: {len(collected_results)}\n"
                            f"(æœ¬è½®æ–°å¢: {trace_count_added})",
                            parse_mode=ParseMode.MARKDOWN_V2
                        )
                    except Exception: pass
                    last_ui_update = time.time()
                
                if limit and len(collected_results) >= limit: break
            
            if not next_round_query or context.bot_data.get(stop_flag):
                break
                
            # å‡†å¤‡è¿›å…¥ä¸‹ä¸€è½®ï¼Œå¤„ç†è¢«æ’é™¤äº† Top1 åçš„å‰©ä½™ä¸–ç•Œ
            current_query_scope = next_round_query
            # é˜²æ­¢æ— é™æ­»å¾ªç¯ä¿æŠ¤ (ä¾‹å¦‚ Stats è¿”å›ç©ºä½†Size > 0)
            if loop_count > 50:
                msg.edit_text("âš ï¸ è­¦å‘Šï¼šæ™ºèƒ½å‰¥ç¦»å¾ªç¯æ¬¡æ•°è¿‡å¤šï¼Œè‡ªåŠ¨åœæ­¢ä»¥é˜²æ­»é”ã€‚")
                break

    except Exception as e:
        logger.error(f"Smart download fatal error: {e}", exc_info=True)
        msg.edit_text(f"âŒ ä»»åŠ¡å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return
    
    # ç»“æœäº¤ä»˜
    final_limit_msg = ""
    if limit and len(collected_results) >= limit: final_limit_msg = f" (å·²è¾¾ä¸Šé™ {limit})"
    
    if collected_results:
        # æ’åºå¹¶å†™å…¥æ–‡ä»¶
        sorted_results = sorted(list(collected_results))
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(sorted_results))
            
        final_caption = f"âœ… *æµ·é‡ä¸‹è½½å®Œæˆ*\n\nğŸ¯ åŸå§‹æŸ¥è¯¢: `{escape_markdown_v2(original_query)}`\nğŸ”¢ æœ€ç»ˆè·å–: *{len(collected_results)}* æ¡{escape_markdown_v2(final_limit_msg)}\nâ± è€—æ—¶: {int(time.time()-start_time)}s"
        send_file_safely(context, chat_id, cache_path, caption=final_caption, parse_mode=ParseMode.MARKDOWN_V2)
        upload_and_send_links(context, chat_id, cache_path)
        
        # æœ¬åœ°è®°å½•æ›´æ–°
        cache_entry = {'file_path': cache_path, 'result_count': len(collected_results)}
        add_or_update_query(original_query, cache_entry)
        
        offer_post_download_actions(context, chat_id, original_query)
        msg.delete() # åˆ æ‰è¿›åº¦æ¡
        
    else:
        msg.edit_text("ğŸ¤·â€â™€ï¸ ä»»åŠ¡ç»“æŸï¼Œæœªæ”¶é›†åˆ°æœ‰æ•ˆæ•°æ®ã€‚")
    
    context.bot_data.pop(stop_flag, None)

# --- èœå•æŸ¥è¯¢å¤„ç†å™¨ (v10.9.6) ---
def prompt_for_query(update: Update, context: CallbackContext) -> int:
    """è¦æ±‚ç”¨æˆ·ä¸ºèœå•å‘½ä»¤è¾“å…¥æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚"""
    button_text = update.message.text
    command_map = { "å¸¸è§„æœç´¢": "/kkfofa", "æµ·é‡æœç´¢": "/allfofa", "æ‰¹é‡å¯¼å‡º": "/batch" }
    command = command_map.get(button_text)
    if not command: return ConversationHandler.END
    context.user_data['menu_command'] = command
    update.message.reply_text(f"è¯·è¾“å…¥ `{command}` çš„æŸ¥è¯¢è¯­å¥:")
    return STATE_AWAITING_QUERY

def prompt_for_host(update: Update, context: CallbackContext) -> int:
    """è¦æ±‚ç”¨æˆ·ä¸ºä¸»æœºå‘½ä»¤è¾“å…¥ä¸»æœºå­—ç¬¦ä¸²ã€‚"""
    context.user_data['menu_command'] = '/host'
    update.message.reply_text("è¯·è¾“å…¥è¦æŸ¥è¯¢çš„ä¸»æœº (IPæˆ–åŸŸå):")
    return STATE_AWAITING_HOST

def run_query_from_menu(update: Update, context: CallbackContext):
    """ä½¿ç”¨ç”¨æˆ·æä¾›çš„æ–‡æœ¬è¿è¡ŒæŸ¥è¯¢å‘½ä»¤ã€‚"""
    command = context.user_data.pop('menu_command', None)
    query_text = update.message.text
    context.args = query_text.split()

    if command == '/batch':
        return batch_command(update, context)
    elif command in ['/kkfofa', '/allfofa']:
        return query_entry_point(update, context)
    return ConversationHandler.END

def run_host_from_menu(update: Update, context: CallbackContext):
    """ä½¿ç”¨ç”¨æˆ·æä¾›çš„æ–‡æœ¬è¿è¡Œä¸»æœºå‘½ä»¤ã€‚"""
    context.user_data.pop('menu_command', None)
    host_text = update.message.text
    context.args = host_text.split()
    
    # host_command å¸¦æœ‰ admin_only è£…é¥°å™¨
    host_command(update, context)
    return ConversationHandler.END


# --- ä¸»å‡½æ•°ä¸è°ƒåº¦å™¨ ---
def interactive_setup():
    """Handles the initial interactive setup for the bot."""
    global CONFIG
    print("--- é¦–æ¬¡è¿è¡Œæˆ–é…ç½®ä¸å®Œæ•´ï¼Œè¿›å…¥äº¤äº’å¼è®¾ç½® ---")
    bot_token = input("è¯·è¾“å…¥æ‚¨çš„ Telegram Bot Token (ç•™ç©ºåˆ™é€€å‡º): ").strip()
    if not bot_token:
        return False
    
    admin_id_str = ""
    while not admin_id_str.isdigit():
        admin_id_str = input("è¯·è¾“å…¥æ‚¨çš„ Telegram User ID (ä½œä¸ºç¬¬ä¸€ä¸ªç®¡ç†å‘˜): ").strip()
        if not admin_id_str.isdigit():
            print("é”™è¯¯: User ID å¿…é¡»æ˜¯çº¯æ•°å­—ã€‚")

    admin_id = int(admin_id_str)
    
    CONFIG["bot_token"] = bot_token
    if not CONFIG.get("admins"): # Only set admins if list is empty
        CONFIG["admins"] = [admin_id]

    fofa_keys = []
    if not CONFIG.get("apis"): # Only ask for keys if none are present
        print("è¯·è¾“å…¥æ‚¨çš„ FOFA API Key (è¾“å…¥ç©ºè¡Œç»“æŸ):")
        while True:
            key = input(f"  - Key #{len(fofa_keys) + 1}: ").strip()
            if not key: break
            fofa_keys.append(key)
        CONFIG["apis"] = fofa_keys

    save_config()
    print("âœ… é…ç½®å·²ä¿å­˜åˆ° config.jsonã€‚")
    CONFIG = load_json_file(CONFIG_FILE, DEFAULT_CONFIG)
    return True

def main() -> None:
    global CONFIG
    os.makedirs(FOFA_CACHE_DIR, exist_ok=True)

    if not os.path.exists(CONFIG_FILE) or CONFIG.get("bot_token") == "YOUR_BOT_TOKEN_HERE":
        if not interactive_setup():
            sys.exit(0)

    while True:
        try:
            bot_token = CONFIG.get("bot_token")
            if not bot_token or bot_token == "YOUR_BOT_TOKEN_HERE":
                logger.critical("é”™è¯¯: 'bot_token' æœªåœ¨ config.json ä¸­è®¾ç½®!")
                if not interactive_setup():
                    break
                continue

            check_and_classify_keys()
            updater = Updater(token=bot_token, use_context=True, request_kwargs={'read_timeout': 20, 'connect_timeout': 20})
            break  # Break loop if updater is created successfully
        except InvalidToken:
            logger.error("!!!!!! æ— æ•ˆçš„ Bot Token !!!!!!")
            print("å½“å‰é…ç½®çš„ Telegram Bot Token æ— æ•ˆã€‚")
            if not interactive_setup():
                sys.exit(0)
        except Exception as e:
            logger.critical(f"å¯åŠ¨æ—¶å‘ç”Ÿæ— æ³•æ¢å¤çš„é”™è¯¯: {e}")
            sys.exit(1)

    dispatcher = updater.dispatcher
    dispatcher.bot_data['updater'] = updater
    commands = [
        BotCommand("start", "ğŸš€ å¯åŠ¨æœºå™¨äºº"), BotCommand("help", "â“ å‘½ä»¤æ‰‹å†Œ"),
        BotCommand("kkfofa", "ğŸ” èµ„äº§æœç´¢ (å¸¸è§„)"), BotCommand("allfofa", "ğŸšš èµ„äº§æœç´¢ (æµ·é‡)"),
        BotCommand("host", "ğŸ“¦ ä¸»æœºè¯¦æŸ¥ (æ™ºèƒ½)"), BotCommand("lowhost", "ğŸ”¬ ä¸»æœºé€ŸæŸ¥ (èšåˆ)"),
        BotCommand("stats", "ğŸ“Š å…¨å±€èšåˆç»Ÿè®¡"), BotCommand("batchfind", "ğŸ“‚ æ‰¹é‡æ™ºèƒ½åˆ†æ (Excel)"),
        BotCommand("batch", "ğŸ“¤ æ‰¹é‡è‡ªå®šä¹‰å¯¼å‡º (äº¤äº’å¼)"), BotCommand("batchcheckapi", "ğŸ”‘ æ‰¹é‡éªŒè¯API Key"),
        BotCommand("check", "ğŸ©º ç³»ç»Ÿè‡ªæ£€"), BotCommand("settings", "âš™ï¸ è®¾ç½®èœå•"),
        BotCommand("history", "ğŸ•°ï¸ æŸ¥è¯¢å†å²"), BotCommand("import", "ğŸ–‡ï¸ å¯¼å…¥æ—§ç¼“å­˜"),
        BotCommand("backup", "ğŸ“¤ å¤‡ä»½é…ç½®"), BotCommand("restore", "ğŸ“¥ æ¢å¤é…ç½®"),
        BotCommand("update", "ğŸ”„ åœ¨çº¿æ›´æ–°è„šæœ¬"), BotCommand("getlog", "ğŸ“„ è·å–æ—¥å¿—"),
        BotCommand("shutdown", "ğŸ”Œ å…³é—­æœºå™¨äºº"), BotCommand("stop", "ğŸ›‘ åœæ­¢ä»»åŠ¡"),
        BotCommand("cancel", "âŒ å–æ¶ˆæ“ä½œ")
    ]
    try: updater.bot.set_my_commands(commands)
    except Exception as e: logger.warning(f"è®¾ç½®æœºå™¨äººå‘½ä»¤å¤±è´¥: {e}")
    settings_conv = ConversationHandler(
        entry_points=[CommandHandler("settings", settings_command)],
        states={
            SETTINGS_STATE_MAIN: [CallbackQueryHandler(settings_callback_handler, pattern=r"^settings_")],
            SETTINGS_STATE_ACTION: [
                CallbackQueryHandler(settings_action_handler, pattern=r"^action_"),
                CallbackQueryHandler(show_update_menu, pattern=r"^settings_update"),
                CallbackQueryHandler(show_backup_restore_menu, pattern=r"^settings_backup"),
                CallbackQueryHandler(backup_config_command, pattern=r"^backup_now"),
                CallbackQueryHandler(lambda u,c: restore_config_command(u.callback_query.message, c), pattern=r"^restore_now"),
                CallbackQueryHandler(get_update_url, pattern=r"^update_set_url"),
                CallbackQueryHandler(settings_command, pattern=r"^(update_back|backup_back)"),
            ],
            SETTINGS_STATE_ADMIN_MENU: [CallbackQueryHandler(admin_menu_callback, pattern=r"^admin_")],
            SETTINGS_STATE_GET_ADMIN_ID_TO_ADD: [MessageHandler(Filters.text & ~Filters.command, get_admin_id_to_add)],
            SETTINGS_STATE_GET_ADMIN_ID_TO_REMOVE: [MessageHandler(Filters.text & ~Filters.command, get_admin_id_to_remove)],
            SETTINGS_STATE_GET_KEY: [MessageHandler(Filters.text & ~Filters.command, get_key)],
            SETTINGS_STATE_REMOVE_API: [MessageHandler(Filters.text & ~Filters.command, remove_api)],
            SETTINGS_STATE_PRESET_MENU: [CallbackQueryHandler(preset_menu_callback, pattern=r"^preset_")],
            SETTINGS_STATE_GET_PRESET_NAME: [MessageHandler(Filters.text & ~Filters.command, get_preset_name)],
            SETTINGS_STATE_GET_PRESET_QUERY: [MessageHandler(Filters.text & ~Filters.command, get_preset_query)],
            SETTINGS_STATE_REMOVE_PRESET: [MessageHandler(Filters.text & ~Filters.command, remove_preset)],
            SETTINGS_STATE_GET_UPDATE_URL: [MessageHandler(Filters.text & ~Filters.command, get_update_url)],
            SETTINGS_STATE_PROXYPOOL_MENU: [CallbackQueryHandler(proxypool_menu_callback, pattern=r"^proxypool_")],
            SETTINGS_STATE_GET_PROXY_ADD: [MessageHandler(Filters.text & ~Filters.command, get_proxy_to_add)],
            SETTINGS_STATE_GET_PROXY_REMOVE: [MessageHandler(Filters.text & ~Filters.command, get_proxy_to_remove)],
            SETTINGS_STATE_UPLOAD_API_MENU: [CallbackQueryHandler(upload_api_menu_callback, pattern=r"^upload_")],
            SETTINGS_STATE_GET_UPLOAD_URL: [MessageHandler(Filters.text & ~Filters.command, get_upload_url)],
            SETTINGS_STATE_GET_UPLOAD_TOKEN: [MessageHandler(Filters.text & ~Filters.command, get_upload_token)],
        },
        fallbacks=[CommandHandler("cancel", cancel)], conversation_timeout=300,
    )
    query_conv = ConversationHandler(
        entry_points=[ CommandHandler("kkfofa", query_entry_point), CommandHandler("allfofa", query_entry_point), CallbackQueryHandler(query_entry_point, pattern=r"^run_preset_") ],
        states={
            QUERY_STATE_GET_GUEST_KEY: [MessageHandler(Filters.text & ~Filters.command, get_guest_key)],
            QUERY_STATE_ASK_CONTINENT: [CallbackQueryHandler(ask_continent_callback, pattern=r"^continent_")], 
            QUERY_STATE_CONTINENT_CHOICE: [CallbackQueryHandler(continent_choice_callback, pattern=r"^continent_")], 
            QUERY_STATE_CACHE_CHOICE: [CallbackQueryHandler(cache_choice_callback, pattern=r"^cache_")],
            QUERY_STATE_KKFOFA_MODE: [CallbackQueryHandler(query_mode_callback, pattern=r"^mode_")],
            QUERY_STATE_GET_TRACEBACK_LIMIT: [MessageHandler(Filters.text & ~Filters.command, get_traceback_limit), CallbackQueryHandler(get_traceback_limit, pattern=r"^limit_")],
            QUERY_STATE_ALLFOFA_GET_LIMIT: [MessageHandler(Filters.text & ~Filters.command, allfofa_get_limit), CallbackQueryHandler(allfofa_get_limit, pattern=r"^allfofa_limit_")]
        },
        fallbacks=[CommandHandler("cancel", cancel)], conversation_timeout=300,
    )
    batch_conv = ConversationHandler(
        entry_points=[CommandHandler("batch", batch_command)], 
        states={
            BATCH_STATE_SELECT_FIELDS: [CallbackQueryHandler(batch_select_fields_callback, pattern=r"^batchfield_")],
            BATCH_STATE_MODE_CHOICE: [CallbackQueryHandler(query_mode_callback, pattern=r"^mode_")],
            BATCH_STATE_GET_LIMIT: [MessageHandler(Filters.text & ~Filters.command, get_traceback_limit), CallbackQueryHandler(get_traceback_limit, pattern=r"^limit_")]
        },
        fallbacks=[CommandHandler('cancel', cancel)], conversation_timeout=600,
    )
    import_conv = ConversationHandler(entry_points=[CommandHandler("import", import_command)], states={IMPORT_STATE_GET_FILE: [MessageHandler(Filters.document.mime_type("text/plain"), get_import_query)]}, fallbacks=[CommandHandler("cancel", cancel)], conversation_timeout=300)
    stats_conv = ConversationHandler(entry_points=[CommandHandler("stats", stats_command)], states={STATS_STATE_GET_QUERY: [MessageHandler(Filters.text & ~Filters.command, get_fofa_stats_query)]}, fallbacks=[CommandHandler("cancel", cancel)], conversation_timeout=300)
    batchfind_conv = ConversationHandler(entry_points=[CommandHandler("batchfind", batchfind_command)], states={BATCHFIND_STATE_GET_FILE: [MessageHandler(Filters.document.mime_type("text/plain"), get_batch_file_handler)], BATCHFIND_STATE_SELECT_FEATURES: [CallbackQueryHandler(select_batch_features_callback, pattern=r"^batchfeature_")]}, fallbacks=[CommandHandler("cancel", cancel)], conversation_timeout=300)
    restore_conv = ConversationHandler(entry_points=[CommandHandler("restore", restore_config_command)], states={RESTORE_STATE_GET_FILE: [MessageHandler(Filters.document, receive_config_file)]}, fallbacks=[CommandHandler("cancel", cancel)], conversation_timeout=300)
    scan_conv = ConversationHandler(entry_points=[CallbackQueryHandler(start_scan_callback, pattern=r'^start_scan_')], states={SCAN_STATE_GET_CONCURRENCY: [MessageHandler(Filters.text & ~Filters.command, get_concurrency_callback)], SCAN_STATE_GET_TIMEOUT: [MessageHandler(Filters.text & ~Filters.command, get_timeout_callback)]}, fallbacks=[CommandHandler('cancel', cancel)], conversation_timeout=120)
    batch_check_api_conv = ConversationHandler(entry_points=[CommandHandler("batchcheckapi", batch_check_api_command)], states={BATCHCHECKAPI_STATE_GET_FILE: [MessageHandler(Filters.document.mime_type("text/plain"), receive_api_file)]}, fallbacks=[CommandHandler("cancel", cancel)], conversation_timeout=300)
    
    dispatcher.add_handler(CommandHandler("start", start_command)); dispatcher.add_handler(CommandHandler("help", help_command)); dispatcher.add_handler(CommandHandler("host", host_command)); dispatcher.add_handler(CommandHandler("lowhost", lowhost_command)); dispatcher.add_handler(CommandHandler("check", check_command)); dispatcher.add_handler(CommandHandler("stop", stop_all_tasks)); dispatcher.add_handler(CommandHandler("backup", backup_config_command)); dispatcher.add_handler(CommandHandler("history", history_command)); dispatcher.add_handler(CommandHandler("getlog", get_log_command)); dispatcher.add_handler(CommandHandler("shutdown", shutdown_command)); dispatcher.add_handler(CommandHandler("update", update_script_command)); dispatcher.add_handler(CommandHandler("monitor", monitor_command)) # æ³¨å†Œç›‘æ§å‘½ä»¤
    dispatcher.add_handler(InlineQueryHandler(inline_fofa_handler)); 
    
    # --- æ¢å¤ç›‘æ§ä»»åŠ¡ ---
    if MONITOR_TASKS:
        count = 0
        for task_id, task in MONITOR_TASKS.items():
            if task.get('status') == 'active':
                # è®¡ç®—åˆå§‹å»¶è¿Ÿï¼šåˆ†æ•£å¯åŠ¨ï¼Œé¿å…æ´ªå³° (0 - 60s)
                delay = random.randint(5, 60)
                updater.job_queue.run_once(run_monitor_execution_job, delay, context={"task_id": task_id, "is_restore": True}, name=f"monitor_{task_id}")
                count += 1
        logger.info(f"å·²æ¢å¤ {count} ä¸ªç›‘æ§ä»»åŠ¡ã€‚")

    # --- ä¸»èœå•æŒ‰é’®å¤„ç†å™¨ (v10.9.6) ---
    menu_conv = ConversationHandler(
        entry_points=[
            MessageHandler(Filters.regex('^å¸¸è§„æœç´¢$'), prompt_for_query),
            MessageHandler(Filters.regex('^æµ·é‡æœç´¢$'), prompt_for_query),
            MessageHandler(Filters.regex('^æ‰¹é‡å¯¼å‡º$'), prompt_for_query),
            MessageHandler(Filters.regex('^ä¸»æœºè¯¦æŸ¥$'), prompt_for_host),
        ],
        states={
            STATE_AWAITING_QUERY: [MessageHandler(Filters.text & ~Filters.command, run_query_from_menu)],
            STATE_AWAITING_HOST: [MessageHandler(Filters.text & ~Filters.command, run_host_from_menu)],
        },
        fallbacks=[CommandHandler('cancel', cancel)],
        conversation_timeout=300
    )
    dispatcher.add_handler(menu_conv)
    dispatcher.add_handler(MessageHandler(Filters.regex(r'^è®¾ç½®$'), settings_command))
    dispatcher.add_handler(MessageHandler(Filters.regex(r'^å¸®åŠ©æ‰‹å†Œ$'), help_command))

    dispatcher.add_handler(settings_conv); dispatcher.add_handler(query_conv); dispatcher.add_handler(batch_conv); dispatcher.add_handler(import_conv); dispatcher.add_handler(stats_conv); dispatcher.add_handler(batchfind_conv); dispatcher.add_handler(restore_conv); dispatcher.add_handler(scan_conv); dispatcher.add_handler(batch_check_api_conv)
    
    logger.info(f"ğŸš€ Fofa Bot v10.9 (ç¨³å®šç‰ˆ) å·²å¯åŠ¨...")
    updater.start_polling()
    updater.idle()
    logger.info("Bot has been shut down gracefully.")

if __name__ == "__main__":
    main()
